{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Action items\r\n",
        "1. spark read delta table from adls, use expression to group by for latestchanges.\r\n",
        "2. call write api from adb to synapse sql dedicated pool to store latestchanges.\r\n",
        "3. auth to sql pool using keyvault"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val df_delta = spark.read.format(\"delta\").load(\"abfss://wplushiramsynapsefs@wplushiramsynapseadlsv2.dfs.core.windows.net/synapse/workspaces/wplushiramsynapse/warehouse/lastchanges\")\r\n",
        "df_delta.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "threedot0",
              "session_id": 89,
              "statement_id": 56,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-09-11T20:31:08.5480507Z",
              "session_start_time": null,
              "execution_start_time": "2021-09-11T20:31:08.662576Z",
              "execution_finish_time": "2021-09-11T20:31:10.464393Z"
            },
            "text/plain": "StatementMeta(threedot0, 89, 56, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_delta: org.apache.spark.sql.DataFrame = [key: string, time: bigint ... 2 more fields]\n+---+----+--------+-------+\n|key|time|newValue|deleted|\n+---+----+--------+-------+\n|  e|   7|      19|  false|\n|  f|   8|      12|  false|\n|  d|  10|      21|  false|\n|  g|   9|    null|   true|\n+---+----+--------+-------+\n\n"
          ]
        }
      ],
      "execution_count": 159,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "val latestonly = df_delta.selectExpr(\"key\", \"struct(time, newValue, deleted) as otherCols\").groupBy(\"key\").agg(max(\"otherCols\").as(\"latest\")).selectExpr(\"key\", \"latest.*\")\r\n",
        "latestonly.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "threedot0",
              "session_id": 89,
              "statement_id": 57,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-09-11T20:31:08.6398691Z",
              "session_start_time": null,
              "execution_start_time": "2021-09-11T20:31:10.5567806Z",
              "execution_finish_time": "2021-09-11T20:31:12.370109Z"
            },
            "text/plain": "StatementMeta(threedot0, 89, 57, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "latestonly: org.apache.spark.sql.DataFrame = [key: string, time: bigint ... 2 more fields]\n+---+----+--------+-------+\n|key|time|newValue|deleted|\n+---+----+--------+-------+\n|  g|   9|    null|   true|\n|  f|   8|      12|  false|\n|  e|   7|      19|  false|\n|  d|  10|      21|  false|\n+---+----+--------+-------+\n\n"
          ]
        }
      ],
      "execution_count": 160,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latestonly.createOrReplaceTempView(\"latestonly\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "threedot0",
              "session_id": 89,
              "statement_id": 58,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-09-11T20:31:08.8509465Z",
              "session_start_time": null,
              "execution_start_time": "2021-09-11T20:31:12.4622719Z",
              "execution_finish_time": "2021-09-11T20:31:13.5500401Z"
            },
            "text/plain": "StatementMeta(threedot0, 89, 58, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 161,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "select * from latestonly"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "threedot0",
              "session_id": 89,
              "statement_id": 59,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-09-11T20:31:08.9810702Z",
              "session_start_time": null,
              "execution_start_time": "2021-09-11T20:31:13.6453871Z",
              "execution_finish_time": "2021-09-11T20:31:14.7214204Z"
            },
            "text/plain": "StatementMeta(threedot0, 89, 59, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 162,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": [
                  {
                    "name": "key",
                    "type": "string",
                    "nullable": true,
                    "metadata": {}
                  },
                  {
                    "name": "time",
                    "type": "long",
                    "nullable": true,
                    "metadata": {}
                  },
                  {
                    "name": "newValue",
                    "type": "string",
                    "nullable": true,
                    "metadata": {}
                  },
                  {
                    "name": "deleted",
                    "type": "boolean",
                    "nullable": true,
                    "metadata": {}
                  }
                ]
              },
              "data": [
                [
                  "g",
                  9,
                  null,
                  true
                ],
                [
                  "f",
                  8,
                  "12",
                  false
                ],
                [
                  "e",
                  7,
                  "19",
                  false
                ],
                [
                  "d",
                  10,
                  "21",
                  false
                ]
              ]
            },
            "text/plain": "<Spark SQL result set with 4 rows and 4 fields>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 162,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://stackoverflow.com/questions/26041479/query-sql-database-with-scala"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Class.forName(\"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\r\n",
        "import java.sql.{Connection, DriverManager, ResultSet, Statement}; \r\n",
        "\r\n",
        "def dt = {\r\n",
        "    val connection_string = \"jdbc:sqlserver://wplushiramsynapse.sql.azuresynapse.net:1433;database=wplussynapsedw;user=LoaderXL;password=XL!sqlPools2021\"\r\n",
        "    val connection = DriverManager.getConnection(connection_string)\r\n",
        "    try {\r\n",
        "        val statement = connection.createStatement()\r\n",
        "        val results = statement.executeQuery(\" if object_id('latestonly') is not null drop table latestonly; select 1\")\r\n",
        "    } finally {\r\n",
        "        connection.close\r\n",
        "    } \r\n",
        "}\r\n",
        "\r\n",
        "dt"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "threedot0",
              "session_id": 89,
              "statement_id": 82,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-09-11T20:43:01.810657Z",
              "session_start_time": null,
              "execution_start_time": "2021-09-11T20:43:01.9128329Z",
              "execution_finish_time": "2021-09-11T20:43:04.3983463Z"
            },
            "text/plain": "StatementMeta(threedot0, 89, 82, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "res254: Class[_] = class com.microsoft.sqlserver.jdbc.SQLServerDriver\nimport java.sql.{Connection, DriverManager, ResultSet, Statement}\ndt: Unit\n"
          ]
        }
      ],
      "execution_count": 185,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latestonly.write.option(Constants.SERVER, \"wplushiramsynapse.sql.azuresynapse.net\").synapsesql(\"wplussynapsedw.dbo.latestonly\", Constants.INTERNAL)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "threedot0",
              "session_id": 89,
              "statement_id": 83,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-09-11T20:43:48.7885019Z",
              "session_start_time": null,
              "execution_start_time": "2021-09-11T20:43:48.9441478Z",
              "execution_finish_time": "2021-09-11T20:43:56.8060635Z"
            },
            "text/plain": "StatementMeta(threedot0, 89, 83, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 186,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## But he wants it in ADB.\r\n",
        "1. https://docs.databricks.com/data/data-sources/azure/synapse-analytics.html#frequently-asked-questions-faq\r\n",
        "2. https://github.com/Azure-Samples/azure-sql-db-databricks/blob/main/notebooks/03b-parallel-switch-in-load-into-partitioned-table-single.ipynb"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_spark",
      "language": "Scala",
      "display_name": "Synapse Spark"
    },
    "language_info": {
      "name": "scala"
    },
    "kernel_info": {
      "name": "synapse_spark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}