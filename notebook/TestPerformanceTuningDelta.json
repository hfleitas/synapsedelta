{
	"name": "TestPerformanceTuningDelta",
	"properties": {
		"folder": {
			"name": "Spark"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "36f4f2ac-d14d-41a2-9d8c-dc25333bd294"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# Start your Spark session.\r\n",
					"spark\r\n",
					"\r\n",
					"# Disable BroadcastHashJoin, so Spark will use standard SortMergeJoin. Currently, Hyperspace indexes utilize SortMergeJoin to speed up query.\r\n",
					"spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\r\n",
					"\r\n",
					"# Verify that BroadcastHashJoin is set correctly \r\n",
					"print(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.types import StructField, StructType, StringType, IntegerType\r\n",
					"\r\n",
					"# Sample department records\r\n",
					"departments = [(10, \"Accounting\", \"New York\"), (20, \"Research\", \"Dallas\"), (30, \"Sales\", \"Chicago\"), (40, \"Operations\", \"Boston\")]\r\n",
					"\r\n",
					"# Sample employee records\r\n",
					"employees = [(7369, \"SMITH\", 20), (7499, \"ALLEN\", 30), (7521, \"WARD\", 30), (7566, \"JONES\", 20), (7698, \"BLAKE\", 30)]\r\n",
					"\r\n",
					"# Create a schema for the dataframe\r\n",
					"dept_schema = StructType([StructField('deptId', IntegerType(), True), StructField('deptName', StringType(), True), StructField('location', StringType(), True)])\r\n",
					"emp_schema = StructType([StructField('empId', IntegerType(), True), StructField('empName', StringType(), True), StructField('deptId', IntegerType(), True)])\r\n",
					"\r\n",
					"departments_df = spark.createDataFrame(departments, dept_schema)\r\n",
					"employees_df = spark.createDataFrame(employees, emp_schema)\r\n",
					"\r\n",
					"#TODO ** customize this location path **\r\n",
					"emp_Location = \"/perftest/employees.parquet\"\r\n",
					"dept_Location = \"/perftest/departments.parquet\"\r\n",
					"\r\n",
					"employees_df.write.format(\"delta\").mode(\"overwrite\").save(emp_Location)\r\n",
					"departments_df.write.format(\"delta\").mode(\"overwrite\").save(dept_Location)"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# emp_Location and dept_Location are the user-defined locations above to save parquet files\r\n",
					"emp_DF = spark.read.format(\"delta\").load(emp_Location)\r\n",
					"dept_DF = spark.read.format(\"delta\").load(dept_Location)\r\n",
					"\r\n",
					"# Verify the data is available and correct\r\n",
					"emp_DF.show()\r\n",
					"dept_DF.show()"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"mssparkutils.fs.ls(\"/\")\r\n",
					"##mssparkutils.fs.rm(\"/<yourpath>\", True)\r\n",
					"##mssparkutils.fs.ls(\"/\")\r\n",
					""
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from hyperspace import *\r\n",
					"\r\n",
					"# Create an instance of Hyperspace\r\n",
					"hyperspace = Hyperspace(spark)"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create index configurations\r\n",
					"\r\n",
					"emp_IndexConfig = IndexConfig(\"empIndex1\", [\"deptId\"], [\"empName\"])\r\n",
					"dept_IndexConfig1 = IndexConfig(\"deptIndex1\", [\"deptId\"], [\"deptName\"])\r\n",
					"dept_IndexConfig2 = IndexConfig(\"deptIndex2\", [\"location\"], [\"deptName\"])"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Run as needed\r\n",
					"If you had index definitions then you may want to delete and vacuum them"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"hyperspace.deleteIndex(\"empIndex1\")\r\n",
					"hyperspace.deleteIndex(\"deptIndex1\")\r\n",
					"hyperspace.deleteIndex(\"deptIndex2\")\r\n",
					""
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"hyperspace.vacuumIndex(\"empIndex1\")\r\n",
					"hyperspace.vacuumIndex(\"deptIndex1\")\r\n",
					"hyperspace.vacuumIndex(\"deptIndex2\")\r\n",
					"hyperspace.indexes().show()"
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Index creation\r\n",
					"The error for delta:\r\n",
					"\r\n",
					"Only creating index over HDFS file based scan nodes is supported. Source plan: FileScan parquet [empId#586,empName#587,deptId#588] Batched: true, DataFilters: [], Format: Parquet, Location: "
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create indexes from configurations\r\n",
					"\r\n",
					"hyperspace.createIndex(emp_DF, emp_IndexConfig)\r\n",
					"hyperspace.createIndex(dept_DF, dept_IndexConfig1)\r\n",
					"hyperspace.createIndex(dept_DF, dept_IndexConfig2)"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"hyperspace.indexes().show()"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Enable Hyperspace\r\n",
					"Hyperspace.enable(spark)\r\n",
					"\r\n",
					"emp_DF = spark.read.parquet(emp_Location)\r\n",
					"dept_DF = spark.read.parquet(dept_Location)\r\n",
					"\r\n",
					"emp_DF.show(5)\r\n",
					"dept_DF.show(5)"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Filter with equality predicate\r\n",
					"\r\n",
					"eqFilter = dept_DF.filter(\"\"\"deptId = 20\"\"\").select(\"\"\"deptName\"\"\")\r\n",
					"eqFilter.show()"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"eqFilter.explain(True)"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"\r\n",
					"emp_DF.createOrReplaceTempView(\"EMP\")\r\n",
					"dept_DF.createOrReplaceTempView(\"DEPT\")\r\n",
					"\r\n",
					"joinQuery = spark.sql(\"SELECT EMP.empName, DEPT.deptName FROM EMP, DEPT WHERE EMP.deptId = DEPT.deptId\")\r\n",
					"\r\n",
					"joinQuery.show()\r\n",
					"##joinQuery.explain(True)"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"joinQuery.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"Employees\")"
				],
				"execution_count": 17
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Z Order\r\n",
					"This works as long as the format is **Delta**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"\r\n",
					"OPTIMIZE Employees"
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"\r\n",
					"select count(*) from Employees where deptName like 'Research%';"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}