{
	"name": "E2E_MLFLOW_Sklearn_ADLS",
	"properties": {
		"folder": {
			"name": "_adhoc"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "threetwo",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "e9f76017-a49f-4cce-a95e-d8c8c63ccec5"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
				"name": "threetwo",
				"type": "Spark",
				"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.2",
				"nodeCount": 5,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Linear Regression (Sklearn)\r\n",
					"This tutorial shows how to use Predict on a Sklearn model.\r\n",
					"Duration 00:02:38\r\n",
					"# Train SKLearn Model"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import fsspec\r\n",
					"import pandas\r\n",
					"\r\n",
					"fsspec_handle = fsspec.open('abfss://wplushiramsynapsefs@wplushiramsynapseadlsv2.dfs.core.windows.net/LengthOfStay_cooked_small.csv')\r\n",
					"\r\n",
					"with fsspec_handle.open() as f:\r\n",
					"    train_df = pandas.read_csv(f)"
				],
				"execution_count": 56
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"train_df.head()"
				],
				"execution_count": 57
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import os\r\n",
					"import shutil\r\n",
					"import mlflow\r\n",
					"import json\r\n",
					"from mlflow.utils import model_utils\r\n",
					"\r\n",
					"import numpy as np\r\n",
					"import pandas as pd\r\n",
					"\r\n",
					"from sklearn.linear_model import LinearRegression"
				],
				"execution_count": 58
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class LinearRegressionModel():\r\n",
					"  _ARGS_FILENAME = 'args.json'\r\n",
					"  FEATURES_KEY = 'features'\r\n",
					"  TARGETS_KEY = 'targets'\r\n",
					"  TARGETS_PRED_KEY = 'targets_pred'\r\n",
					"\r\n",
					"  def __init__(self, fit_intercept, nb_input_features=9, nb_output_features=1):\r\n",
					"    self.fit_intercept = fit_intercept\r\n",
					"    self.nb_input_features = nb_input_features\r\n",
					"    self.nb_output_features = nb_output_features\r\n",
					"\r\n",
					"  def get_args(self):\r\n",
					"    args = {\r\n",
					"        'nb_input_features': self.nb_input_features,\r\n",
					"        'nb_output_features': self.nb_output_features,\r\n",
					"        'fit_intercept': self.fit_intercept\r\n",
					"    }\r\n",
					"    return args\r\n",
					"\r\n",
					"  def create_model(self):\r\n",
					"    self.model = LinearRegression(fit_intercept=self.fit_intercept)\r\n",
					"\r\n",
					"  def train(self, dataset):\r\n",
					"\r\n",
					"    features = np.stack([sample for sample in iter(\r\n",
					"        dataset[LinearRegressionModel.FEATURES_KEY])], axis=0)\r\n",
					"\r\n",
					"    targets = np.stack([sample for sample in iter(\r\n",
					"        dataset[LinearRegressionModel.TARGETS_KEY])], axis=0)\r\n",
					"\r\n",
					"\r\n",
					"    self.model.fit(features, targets)\r\n",
					"\r\n",
					"  def predict(self, dataset):\r\n",
					"    features = np.stack([sample for sample in iter(\r\n",
					"        dataset[LinearRegressionModel.FEATURES_KEY])], axis=0)\r\n",
					"    targets_pred = self.model.predict(features)\r\n",
					"    return targets_pred\r\n",
					"\r\n",
					"  def save(self, path):\r\n",
					"    if os.path.exists(path):\r\n",
					"      shutil.rmtree(path)\r\n",
					"\r\n",
					"    # save the sklearn model with mlflow\r\n",
					"    mlflow.sklearn.save_model(self.model, path)\r\n",
					"\r\n",
					"    # save args\r\n",
					"    self._save_args(path)\r\n",
					"\r\n",
					"  def _save_args(self, path):\r\n",
					"    args_filename = os.path.join(path, LinearRegressionModel._ARGS_FILENAME)\r\n",
					"    with open(args_filename, 'w') as f:\r\n",
					"      args = self.get_args()\r\n",
					"      json.dump(args, f)"
				],
				"execution_count": 59
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def train(train_df, output_model_path):\r\n",
					"  print(f\"Start to train LinearRegressionModel.\")\r\n",
					"\r\n",
					"  # Initialize input dataset\r\n",
					"  dataset = train_df.to_numpy()\r\n",
					"  datasets = {}\r\n",
					"  datasets['targets'] = dataset[:, -1]\r\n",
					"  datasets['features'] = dataset[:, :9]\r\n",
					"\r\n",
					"  # Initialize model class obj\r\n",
					"  model_class = LinearRegressionModel(fit_intercept=10)\r\n",
					"  with mlflow.start_run(nested=True) as run:\r\n",
					"    model_class.create_model()\r\n",
					"    model_class.train(datasets)\r\n",
					"    model_class.save(output_model_path)\r\n",
					"    print(model_class.predict(datasets))"
				],
				"execution_count": 60
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"train(train_df, './artifacts/output')"
				],
				"execution_count": 61
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Upload to ADLS"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import fsspec\r\n",
					"import pandas \r\n",
					"from fsspec.core import split_protocol\r\n",
					"\r\n",
					"STORAGE_PATH = 'abfs://wplushiramsynapsefs/predict/models/mlflow/sklearn/e2e_linear_regression/'\r\n",
					"\r\n",
					"protocol, _ = split_protocol(STORAGE_PATH)\r\n",
					"print (protocol)\r\n",
					"\r\n",
					"fs = fsspec.filesystem(protocol)\r\n",
					"fs.put(\r\n",
					"    './artifacts/output',\r\n",
					"    STORAGE_PATH, \r\n",
					"    recursive=True, overwrite=True)"
				],
				"execution_count": 62
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Import SynapseML Predict"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pandas as pd\r\n",
					"\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql.functions import col, pandas_udf,udf,lit\r\n",
					"\r\n",
					"import azure.synapse.ml.predict as pcontext\r\n",
					"import azure.synapse.ml.predict.utils._logger as synapse_predict_logger\r\n",
					"\r\n",
					"print(pcontext.__version__)"
				],
				"execution_count": 63
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Set some input parameters\r\n",
					"Model and Data are both stored on ADLS. Must use full abfss path, not the mount.\r\n",
					"\r\n",
					"Return type is int"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"DATA_FILE = \"abfss://wplushiramsynapsefs@wplushiramsynapseadlsv2.dfs.core.windows.net/LengthOfStay_cooked_small.csv\"\r\n",
					"ADLS_MODEL_URI_SKLEARN = \"abfss://wplushiramsynapsefs@wplushiramsynapseadlsv2.dfs.core.windows.net/predict/models/mlflow/sklearn/e2e_linear_regression/\"\r\n",
					"RETURN_TYPES = \"INT\""
				],
				"execution_count": 64
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Enable SynapseML predict\r\n",
					"Set the spark conf spark.synapse.ml.predict.enabled as true to enable the library."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.conf.set(\"spark.synapse.ml.predict.enabled\",\"true\")"
				],
				"execution_count": 65
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Bind Model"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"model = pcontext.bind_model(RETURN_TYPES, \"mlflow\", \"sklearn_linear_regression\", ADLS_MODEL_URI_SKLEARN).register()"
				],
				"execution_count": 66
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Load Data"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df = spark.read \\\r\n",
					"    .format(\"csv\") \\\r\n",
					"    .option(\"header\", \"true\") \\\r\n",
					"    .csv(DATA_FILE,\r\n",
					"        inferSchema=True)\r\n",
					"df = df.select(df.columns[:9])\r\n",
					"df.createOrReplaceTempView('data')\r\n",
					"df.show(10)\r\n",
					"df"
				],
				"execution_count": 67
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql \r\n",
					"select * from data"
				],
				"execution_count": 68
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"predictions = spark.sql(\r\n",
					"                  \"\"\"\r\n",
					"                      SELECT PREDICT('sklearn_linear_regression', *) AS predict FROM data\r\n",
					"                  \"\"\"\r\n",
					"              ).show()"
				],
				"execution_count": 69
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Special Thanks\r\n",
					"* Ajay Agarwal\r\n",
					"* Tian Wei\r\n",
					"* Nellie Gustafsson"
				]
			}
		]
	}
}