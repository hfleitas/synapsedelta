{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "wplushiramsynapse"
		},
		"LaptopFact_password": {
			"type": "secureString",
			"metadata": "Secure string for 'password' of 'LaptopFact'"
		},
		"SQLOnDemand_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'SQLOnDemand'"
		},
		"wplushiramsynapse-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'wplushiramsynapse-WorkspaceDefaultSqlServer'"
		},
		"LabFilesZip_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://pbiworkshoplabsgvw.blob.core.windows.net/synapse/LabFiles.zip"
		},
		"LaptopFact_properties_typeProperties_host": {
			"type": "string",
			"defaultValue": "C:/hiram-msft/ws-SynapseSQLPools"
		},
		"LaptopFact_properties_typeProperties_userId": {
			"type": "string",
			"defaultValue": "LAPTOP-RRU3V9OS\\dwloader"
		},
		"wplushiramsynapse-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://wplushiramsynapseadlsv2.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Delta-Import')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Target",
						"description": "overwrites staging table in SQL Pools from Serverless",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Delta",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"sink": {
								"type": "SqlPoolSink",
								"preCopyScript": "exec DeltaDropTarget",
								"allowPolyBase": true,
								"polyBaseSettings": {
									"rejectValue": 0,
									"rejectType": "value",
									"useTypeDefault": true
								},
								"tableOption": "autoCreate",
								"sqlWriterUseTableLock": false
							},
							"enableStaging": true,
							"stagingSettings": {
								"linkedServiceName": {
									"referenceName": "wplushiramsynapse-WorkspaceDefaultStorage",
									"type": "LinkedServiceReference"
								},
								"path": "wplushiramsynapsefs"
							},
							"translator": {
								"type": "TabularTranslator",
								"mappings": [
									{
										"source": {
											"name": "key",
											"type": "String"
										},
										"sink": {
											"name": "key",
											"type": "String"
										}
									},
									{
										"source": {
											"name": "value",
											"type": "String"
										},
										"sink": {
											"name": "value",
											"type": "String"
										}
									}
								]
							}
						},
						"inputs": [
							{
								"referenceName": "vDeltaTarget",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "Target",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Delta",
						"description": "Includes CDC",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Delta",
								"type": "NotebookReference"
							},
							"snapshot": true
						}
					},
					{
						"name": "LastChanges",
						"description": "overwrites staging table in SQL Pools from Serverless",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Delta",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"sink": {
								"type": "SqlPoolSink",
								"preCopyScript": "exec DeltaDropLastChanges",
								"allowPolyBase": true,
								"polyBaseSettings": {
									"rejectValue": 0,
									"rejectType": "value",
									"useTypeDefault": true
								},
								"tableOption": "autoCreate",
								"sqlWriterUseTableLock": false
							},
							"enableStaging": true,
							"stagingSettings": {
								"linkedServiceName": {
									"referenceName": "wplushiramsynapse-WorkspaceDefaultStorage",
									"type": "LinkedServiceReference"
								},
								"path": "wplushiramsynapsefs"
							},
							"translator": {
								"type": "TabularTranslator",
								"mappings": [
									{
										"source": {
											"name": "key",
											"type": "String"
										},
										"sink": {
											"name": "key",
											"type": "String"
										}
									},
									{
										"source": {
											"name": "time",
											"type": "Int64"
										},
										"sink": {
											"name": "time",
											"type": "Int64"
										}
									},
									{
										"source": {
											"name": "newValue",
											"type": "String"
										},
										"sink": {
											"name": "newValue",
											"type": "String"
										}
									},
									{
										"source": {
											"name": "deleted",
											"type": "Boolean"
										},
										"sink": {
											"name": "deleted",
											"type": "Boolean"
										}
									}
								]
							}
						},
						"inputs": [
							{
								"referenceName": "vDeltaLastChanges",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "LastChanges",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "ShakeNBake",
						"description": "Merge and Rename",
						"type": "SqlPoolStoredProcedure",
						"dependsOn": [
							{
								"activity": "LastChanges",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Target",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"sqlPool": {
							"referenceName": "wplussynapsedw",
							"type": "SqlPoolReference"
						},
						"typeProperties": {
							"storedProcedureName": "[dbo].[ShakeNBake]"
						}
					}
				],
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/vDeltaTarget')]",
				"[concat(variables('workspaceId'), '/datasets/Target')]",
				"[concat(variables('workspaceId'), '/notebooks/Delta')]",
				"[concat(variables('workspaceId'), '/datasets/vDeltaLastChanges')]",
				"[concat(variables('workspaceId'), '/datasets/LastChanges')]",
				"[concat(variables('workspaceId'), '/sqlPools/wplussynapsedw')]",
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/GetLabFilesFromHTTP')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "HTTPtoBlob",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BinarySource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "BinaryReadSettings"
								}
							},
							"sink": {
								"type": "BinarySink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "HTTPLabFilesZip",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "blobLabFilesZip",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"folder": {
					"name": "M01L05Lab01"
				},
				"annotations": [
					"Workshop"
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/HTTPLabFilesZip')]",
				"[concat(variables('workspaceId'), '/datasets/blobLabFilesZip')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MoveUnzippedFiles')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Wait1",
						"type": "Wait",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"waitTimeInSeconds": {
								"value": "@pipeline().parameters.hiram",
								"type": "Expression"
							}
						}
					}
				],
				"parameters": {
					"hiram": {
						"type": "string",
						"defaultValue": "select 10 as wait"
					}
				},
				"folder": {
					"name": "M01L05Lab01"
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/UnzipLabFiles')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "UnzipLabFiles",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BinarySource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true
								},
								"formatSettings": {
									"type": "BinaryReadSettings",
									"compressionProperties": {
										"type": "ZipDeflateReadSettings"
									}
								}
							},
							"sink": {
								"type": "BinarySink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings",
									"copyBehavior": "PreserveHierarchy"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "blobLabFilesZip",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "bloblUnzipLabFiles",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"folder": {
					"name": "M01L05Lab01"
				},
				"annotations": [
					"workshop"
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/blobLabFilesZip')]",
				"[concat(variables('workspaceId'), '/datasets/bloblUnzipLabFiles')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/wait')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Wait1",
						"type": "Wait",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"waitTimeInSeconds": 1
						}
					}
				],
				"parameters": {
					"test": {
						"type": "string",
						"defaultValue": "SELECT PATINDEX('%schools%', 'W3Schools.com');"
					}
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/HTTPLabFilesZip')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LabFilesZip",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LabFilesZip')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LastChanges')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [],
				"typeProperties": {
					"schema": "dbo",
					"table": "lastchanges"
				},
				"sqlPool": {
					"referenceName": "wplussynapsedw",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/wplussynapsedw')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Target')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [],
				"typeProperties": {
					"schema": "dbo",
					"table": "Target"
				},
				"sqlPool": {
					"referenceName": "wplussynapsedw",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/wplussynapsedw')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/blobLabFilesZip')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "wplushiramsynapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "LabFiles.zip",
						"fileSystem": "wplushiramsynapsefs"
					},
					"compression": {
						"type": "ZipDeflate"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bloblUnzipLabFiles')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "wplushiramsynapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileSystem": "nyctaxistaging"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/vDeltaLastChanges')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SQLOnDemand",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "dbo",
					"table": "vDeltaLastChanges"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SQLOnDemand')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/vDeltaTarget')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SQLOnDemand",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "dbo",
					"table": "vDeltaTarget"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SQLOnDemand')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LabFilesZip')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('LabFilesZip_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LaptopFact')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "FileServer",
				"typeProperties": {
					"host": "[parameters('LaptopFact_properties_typeProperties_host')]",
					"userId": "[parameters('LaptopFact_properties_typeProperties_userId')]",
					"password": {
						"type": "SecureString",
						"value": "[parameters('LaptopFact_password')]"
					}
				},
				"connectVia": {
					"referenceName": "Laptop",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/Laptop')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQLOnDemand')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('SQLOnDemand_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/wplushiramsynapse-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('wplushiramsynapse-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/wplushiramsynapse-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('wplushiramsynapse-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Daily')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "MoveUnzippedFiles",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Day",
						"interval": 1,
						"startTime": "2021-07-22T19:58:00Z",
						"endTime": "2021-07-23T19:58:00Z",
						"timeZone": "UTC",
						"schedule": {
							"hours": [
								7
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/MoveUnzippedFiles')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Hourly')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Started",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "Delta-Import",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Hour",
						"interval": 1,
						"startTime": "2021-09-02T15:00:00",
						"timeZone": "Eastern Standard Time"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Delta-Import')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Last Day of Month')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "wait",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Month",
						"interval": 15,
						"startTime": "2021-07-01T18:53:00Z",
						"timeZone": "UTC",
						"schedule": {
							"minutes": [
								58
							],
							"hours": [
								8
							],
							"monthDays": [
								-1
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/wait')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Laptop')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Self-hosted')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CopyIntoDevs')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.objects O JOIN sys.schemas S ON O.schema_id = S.schema_id WHERE O.NAME = 'devs' AND O.TYPE = 'U' AND S.NAME = 'dbo')\nCREATE TABLE dbo.devs\n\t(\n\t [C1] nvarchar(4000),\n\t [C2] nvarchar(4000)\n\t)\nWITH\n\t(\n\tDISTRIBUTION = ROUND_ROBIN,\n\t CLUSTERED COLUMNSTORE INDEX\n\t -- HEAP\n\t)\nGO\n\n--Uncomment the 4 lines below to create a stored procedure for data pipeline orchestration​\n--CREATE PROC bulk_load_devs\n--AS\n--BEGIN\nCOPY INTO dbo.devs\n(C1 1, C2 2)\nFROM 'https://wplushiramsynapseadlsv2.dfs.core.windows.net/wplushiramsynapsefs/devs.csv'\nWITH\n(\n\tFILE_TYPE = 'CSV'\n\t,MAXERRORS = 0\n\t,ERRORFILE = 'https://wplushiramsynapseadlsv2.dfs.core.windows.net/wplushiramsynapsefs/'\n)\n--END\nGO\n\nSELECT TOP 100 * FROM dbo.devs\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CreateLogins')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "M01L05Lab01"
				},
				"content": {
					"query": "--use master \nCREATE LOGIN DWLoader WITH PASSWORD = 'Str0ng_password';\nCREATE LOGIN dwloaderrc10 WITH PASSWORD = 'Str0ng_password';\n\nCREATE USER DWLoader FOR LOGIN DWLoader;\nCREATE USER dwloaderrc10 FOR LOGIN dwloaderrc10;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CreateUser')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "M01L05Lab01"
				},
				"content": {
					"query": "--use wplussynapsedw\nCREATE USER DWLoader FOR LOGIN DWLoader;\nCREATE USER dwloaderrc10 FOR LOGIN dwloaderrc10;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeltaDropTarget')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"content": {
					"query": "--  1. DeltaDropTarget\nif object_id('DeltaDropTarget') is not null drop proc DeltaDropTarget\ngo \ncreate proc DeltaDropTarget\nas \n    if object_id('target') is not null\n    begin\n        drop table target\n    end \ngo\n\n-- 2. DeltaDropLastChanges\nif object_id('DeltaDropLastChanges') is not null drop proc DeltaDropLastChanges\ngo \ncreate proc DeltaDropLastChanges\nas \n    if object_id('LastChanges') is not null\n    begin\n        drop table LastChanges\n    end \ngo\n\n--3. ShakeNBake\nif object_id('ShakeNBake') is not null drop proc ShakeNBake\ngo \ncreate proc ShakeNBake\nas \n    -- rename object target2hash TO ReportTarget;\n    \n    --ctas to merge\n    if object_id('target2hash') is not null drop table [target2hash];\n    create table [target2hash]\n    with (\n        distribution = hash([key]),\n        clustered columnstore index\n    ) as select * from [target];\n\n    --merge\n    merge into [target2hash] as t\n    using (select * from [lastchanges]) as lc ([key], [time], [newvalue], [deleted])\n    on lc.[key] = t.[key]\n    when matched and lc.[deleted] = 1 \n        then delete\n    when matched\n        then update set t.[key] = lc.[key], t.[value] = lc.[newValue]\n    when not matched and lc.[deleted] = 0 \n        then insert ([key], [value]) values(lc.[key], lc.[newValue]);\n\n    --rename\n    if object_id('ReportTarget') is not null drop table [ReportTarget];\n    rename object target2hash TO ReportTarget;\ngo",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeltaSQLPoolMerge')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"content": {
					"query": "-- Merge statements with a WHEN NOT MATCHED [BY TARGET] clause must target a hash distributed table.\n\nif object_id('target2hash') is not null drop table [target2hash];\n\ncreate table [target2hash]\nwith (\n    distribution = hash([key]),\n    clustered columnstore index\n)\nas\nselect * from [target]\ngo \n\nmerge into [target2hash] as t\nusing (select * from [lastchanges]) as lc ([key], [time], [newvalue], [deleted])\non lc.[key] = t.[key]\nwhen matched and lc.[deleted] = 1 \n    then delete\nwhen matched\n    then update set t.[key] = lc.[key], t.[value] = lc.[newValue]\nwhen not matched and lc.[deleted] = 0 \n    then insert ([key], [value]) values(lc.[key], lc.[newValue]);\n\nselect * from target2hash;\n\n-- remarks: https://docs.microsoft.com/en-us/sql/t-sql/statements/merge-transact-sql?view=azure-sqldw-latest#remarks",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeltaSrvlessViews')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"content": {
					"query": "create credential [https://wplushiramsynapseadlsv2.dfs.core.windows.net/wplushiramsynapsefs] \nwith \n    identity = 'SHARED ACCESS SIGNATURE', \n    secret = 'mySASkey';\ngo \n\nselect * from sys.credentials;\ngo\n\ndrop view if exists vDeltaTarget;\ngo\ncreate view vDeltaTarget\nas\nSELECT [key], [value]\nFROM OPENROWSET(\n    BULK 'abfss://wplushiramsynapsefs@wplushiramsynapseadlsv2.dfs.core.windows.net/synapse/workspaces/wplushiramsynapse/warehouse/target',\n    FORMAT = 'delta') as rows\ngo\n\ndrop view if exists vDeltaLastChanges;\ngo\ncreate view vDeltaLastChanges\nas\nSELECT [key], [time], [newvalue], [deleted]\nFROM OPENROWSET(\n    BULK 'abfss://wplushiramsynapsefs@wplushiramsynapseadlsv2.dfs.core.windows.net/synapse/workspaces/wplushiramsynapse/warehouse/lastchanges',\n    FORMAT = 'delta') as rows \ngo\n\nselect * from vDeltaTarget\nselect * from vDeltaLastChanges\n\n{\n    \"errorCode\": \"2200\",\n    \"message\": \"Failure happened on 'Source' side. ErrorCode=SqlOperationFailed,\n    'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,\n    Message=A database operation failed with the following error: 'Invalid column name 'value'.',\n    Source=,''Type=System.Data.SqlClient.SqlException,Message=Invalid column name 'value'.,\n    Source=.Net SqlClient Data Provider,SqlErrorNumber=207,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=207,State=1,Message=Invalid column name 'value'.,},],'\",\n    \"failureType\": \"UserError\",\n    \"target\": \"LastChanges\",\n    \"details\": []\n}\n\n{\n    \"errorCode\": \"2200\",\n    \"message\": \"Failure happened on 'Source' side. ErrorCode=SqlOperationFailed,\n    'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,\n    Message=A database operation failed with the following error: 'Invalid column name 'value'.',\n    Source=,''Type=System.Data.SqlClient.SqlException,Message=Invalid column name 'value'.,\n    Source=.Net SqlClient Data Provider,SqlErrorNumber=207,Class=16,ErrorCode=-2146232060,State=1,\n    Errors=[{Class=16,Number=207,State=1,Message=Invalid column name 'value'.,},],'\",\n    \"failureType\": \"UserError\",\n    \"target\": \"LastChanges\",\n    \"details\": []\n}",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "hiram",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ExternalStuff')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "create database scoped credential wplushiramsynapsefs_cred\nwith \n     identity = 'wplushiramsynapseadlsv2', \n     secret = '' --my storage key, sas not needed\ngo \n\n-- if behind private endpoint, in order to auth FW need to use Service Idendity. Using the key wont work.\n\ncreate database scoped credential msi_cred\nwith \n     identity = 'Managed Identity' -- same as 'Managed Service Identity'\ngo \n\ndrop external data source wplushiramsynapsefs\ngo \n\ncreate external data source wplushiramsynapsefs\nwith (    \n    location = 'abfs://wplushiramsynapsefs@wplushiramsynapseadlsv2.blob.core.windows.net/',\n    credential = msi_cred,\n    type = HADOOP\n)\n\n-- Ref: abfs recommened for big data workloads & adls gen2.\n-- https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction#key-features-of-data-lake-storage-gen2\n\ndrop external file format skipHeader_CSV\ngo \n\ncreate external file format skipHeader_CSV\nwith (FORMAT_TYPE = DELIMITEDTEXT,\n      FORMAT_OPTIONS(\n          FIELD_TERMINATOR = ',',\n          STRING_DELIMITER = '\"',\n          FIRST_ROW = 2, \n          USE_TYPE_DEFAULT = True)\n)\n\nif object_id('extDevs') is not null drop external table extDevs\ngo \n\nCREATE EXTERNAL TABLE extDevs (\n    [first] varchar(50),\n    [last] varchar(50)\n)  \nWITH (\n    LOCATION = 'devs.csv',\n    DATA_SOURCE = wplushiramsynapsefs,  \n    FILE_FORMAT = skipHeader_CSV\n)\nGO\n\nSELECT TOP 10 * FROM extDevs",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Reboot')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"content": {
					"query": "--reset table\nselect * from [target]\ngo \n\ndelete from [target]; \ngo \n\ninsert [target] values('a','0')\ninsert [target] values('b','1')\ninsert [target] values('c','2')\ninsert [target] values('d','3')\ngo  \n\n\nselect * from [target];\n\ndelete from [target]; \ngo \n\ninsert [target] values('d','3')\ninsert [target] values('c','200')\ngo\n\n--reset lastchanges\ndelete from [lastchanges];\ninsert [lastchanges] values ('a','1',NULL,1)\ninsert [lastchanges] values ('c','5','200',0)\ninsert [lastchanges] values ('b','2',NULL,1)",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ServerlessOpenRowsetDevs')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- devs.csv\n-- first,last\n-- hiram,fleitas\n-- christina,sosa\n-- brent,carpenetti\n\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://wplushiramsynapseadlsv2.dfs.core.windows.net/wplushiramsynapsefs/devs.csv',\n        FORMAT = 'CSV',\n        PARSER_VERSION='2.0'\n    ) AS [result]\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ServerlessVersion')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "select @@servername, @@version",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TrollHunters')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Pixies"
				},
				"content": {
					"query": "--  Troll Hunters: http://trollhunters.wikia.com/wiki/Trollhunters_Wiki , https://en.wikipedia.org/wiki/Trollhunters\n--  :connect localhost\n--  :connect TrollHunters\n--  By: Hiram Fleitas, hiramfleitas@hotmail.com. Special thx to Andy and Dmitri.\ngo\n\n--  +------+\n--  | Lore |\n--  +------+\nif db_id(N'ᕙ༼,இܫஇ,༽ᕗ') is not null --drop/create db \nbegin\n\talter database [ᕙ༼,இܫஇ,༽ᕗ] set single_user with rollback immediate \n\tdrop database [ᕙ༼,இܫஇ,༽ᕗ] end\nelse begin \n\tcreate database [ᕙ༼,இܫஇ,༽ᕗ]\n\ton primary (name='TrollHunters',filename='C:\\Program Files\\Microsoft SQL Server\\MSSQL15.MSSQLSERVER\\MSSQL\\DATA\\TrollHunters.mdf', size=64MB, maxsize=unlimited, filegrowth=64MB) --S:\\SQLDATA\\, C:\\Program Files\\Microsoft SQL Server\\MSSQL13.MSSQLSERVER\\MSSQL\\DATA\\\n\tlog on (name='TrollHunters_log',filename='C:\\Program Files\\Microsoft SQL Server\\MSSQL15.MSSQLSERVER\\MSSQL\\DATA\\TrollHunters_log.ldf', size=8MB, filegrowth=64MB) --L:\\SQLLOGS\\, C:\\Program Files\\Microsoft SQL Server\\MSSQL13.MSSQLSERVER\\MSSQL\\DATA\\\nend\t\ngo\n\nuse [ᕙ༼,இܫஇ,༽ᕗ]\ngo \ncreate table [Character] ( \n\tCharacterId\t\tint \tnot null,\n    FullName\t\tvarchar(50)\t\t\tnot null,\n\tAffiliation\t\tvarchar(60)\t\t\tnull, \n\tCategory\t\tvarchar(10)\t\t\tnull, \n\tAka\t\t\t\tvarchar(300)\t\tnull, --*\n\t[Status]\t\tvarchar(35)\t\t\tnull,\n\tRace\t\t\tvarchar(50)\t\t\tnull,\n\tAge\t\t\t\tint\t\t\t\t\tnull,\n\tHome\t\t\tvarchar(50)\t\t\tnull,\n\tRelatives\t\tvarchar(300)\t\tnull, --*\n\tWeapons\t\t\tvarchar(100)\t\tnull, --*\n\tEyeColor\t\tvarchar(20)\t\t\tnull,\n\tHairColor\t\tvarchar(50)\t\t\tnull,\n\tMinions\t\t\tvarchar(100)\t\tnull,\n\tVoicedBy\t\tvarchar(50)\t\t\tnull)\n\tconstraint pk_Character primary key clustered (CharacterId asc));\ngo\ncreate table [Locations] ( LocationId int not null, LocationName varchar(50), CharacterId int )\n-- , constraint pk_Locations primary key clustered (LocationId asc));\n--create table Aka (AkaId int identity(1,1) not null, Aka varchar(50), CharacterId int foreign key references [Character](CharacterId), ByCharacterId int foreign key references [Character](CharacterId), constraint pk_Aka primary key clustered (AkaId asc));\n--create table Relatives (RelativeId int identity(1,1) not null, CharacterId int foreign key references [Character](CharacterId), RelativeCharacterId int foreign key references [Character](CharacterId), Relation varchar(50), constraint pk_Relative primary key clustered (RelativeId asc));\n--create table Weapons (WeaponId int identity(1,1) not null, CharacterId int foreign key references [Character](CharacterId), WeaponName varchar(50), [Type] varchar(20), Origin varchar(20), constraint pk_Weapons primary key clustered (WeaponId asc));\n--create table WeaponUser (WeaponUserId int identity(1,1) not null, CharacterId int foreign key references [Character](CharacterId), Occurrance varchar(50), Victims varchar(50), constraint pk_WeaponUser primary key clustered (WeaponUserId asc)); \ngo\ncreate table [Episode] ( \n    CharacterId\t\tint           not null,\n    FullName\t\tnvarchar(64)  not null,\n    VoicedBy\t\tnvarchar(64)  not null,\n\tBirthYear\t\tint\t\t\t  not null, \n\tCategory\t\tnvarchar(64)  not null,\n\t[Status]\t\tnvarchar(64)  not null,\n    Age\t\t\t\tint\t\t\t  not null)\n    constraint PK_Episode primary key clustered (CharacterId asc));\ngo\nset identity_insert [Character] on; --drop table [Character] \n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values \n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (1,'Jim Lake Jr.','Good','Hero','Young Atlas, Jimbo, Master Jim, Fleshbag, Jim \"Fake\" Jr., Jimmy Jam, Little Gynt','Alive','Human',16,'Arcadia Oaks','Jim Lake Sr., Barbara Lake, Claire Nuñez','Sword of Daylight, Sword of Eclipse','Blue','Black','Toby Domzalski','Anton Yelchin')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (2,'Claire Maria Nuñez','Good (cursed by overpowering the shadow staff)','Hero','C-bomb, Shadowdancer','Alive','Human',15,'Arcadia','Mr. Nuñez, Mrs. Nuñez, Baby Enrique, Jim Lake Jr.','Shadow Staff','Brown','Black with white and purple streaks','NotEnrique','Lexi Medrano')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (3,'Gunmar the Black','Evil, Gumm-Gumm Army, The Janus Order','Villain','Dark Underlord, Skullcrusher','Alive','Gumm-Gumm',null,'Darklands, Trollmarket','Bular','Decimaar Blade','Blue','Black','Otto Scaarbach, Stricklander, Nomura, Gladys, NotEnrique, Stalklings, Various Changeling Trolls','Clancy Brown')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (4,'Ararghaumont','Gunmar, Heartstone Trollmarket, Trollhunters','Hero','AAARRRGGHH!!!, Wingman','Alive','Krubera Troll',null,'Deep Caverns, Heartstone Trollmarket',null,'Fists and Strength','Green','Dark green',null,'Fred Tatasciore')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (5,'Stricklander','Gunmar, The Janus Order, Bular, Barbara Lake, Jim','Villain','Strikler, Impure, Boss man','Alive','Changeling Troll',null,'Arcadia','Walter Strickler','Knives','Green','Black','NotEnrique, Angor Rot, Antamonstrum, Gladys, Goblins, Fragwa','Jonathan Hyde')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (6,'Angor Rot',null,'Villain',null,'Deceased','Troll',null,null,'his village','Shadow Staff, Creeper Sun Blade, Sword of Daylight','Yellow, White',null,'Pixies','Ike Amadi')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (7,'Zelda Nomura','The Janus Order, Gunmar, Bular, Stricklander','Villain','Impure, Nomura','Alive','Changeling',null,'Arcadia Oaks',null,'Dual-Khopesh','Green','Black',null,'Lauren Tom')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (8,'Draal the Deadly','Good','Hero',null,'Cursed by Gunmar''s Decimer Blade','Troll',null,'Heartstone Trollmarket','Kanjigar','Fists','Yellow and Red',null,null,'Matthew Waterson')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (9,'Morgana','Herself','Villain','Argante, Lady Pale, Eldritch Queen, Baba Yaga','Alive',null,null,'Forests of the Black Sea, Bulgaria',null,'Sorcery',null,'Gray','Angor Rot','Lena Headey')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (10,'Blinkous Galadrigal','Good','Hero','Blinky','Alive','Troll',600,'Heartstone Trollmarket','Dictatious Maximus Galadrigal',null,'Brown','Blue',null,'Kelsey Grammer')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (11,'Bular','Evil','Villain','Son of Gunmar','Deceased','Gumm-Gumm',null,'Darklands','Gunmar','Sword, Horns','Orange',null,'Changeling Trolls, NotEnrique, Nomura, Otto Scaarbach, Gladys, Customs Agent, Stalkling','Ron Perlman')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (12,'Dictatious Maximus Galadrigal','Bad','Villain',null,'Alive','Troll',4999,'Trollmarket, Darklands','Blinky',null,null,null,'Draal the Deadly (possessed by Gunmar), Gumm-Gumm Warriors, Krubera Warriors','Mark Hamill')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (13,'Tobias Domzalski','Good','Hero','Tobes, Toby-Pie, Wingman, T.P., Dumbzalski','Alive','Human',15,'Arcadia','Nana, Mr. Domzalski, Mrs. Domzalski','Warhammer, Glamour Mask','Green','Brown',null,'Charlie Saxton')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (14,'Usurna','Gumm-Gumm','Villain',null,'Alive','Krubera',null,null,null,'Poison Knives',null,null,'Krubera soldiers, Krax','Anjelica Huston')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (15,'Elijah Leslie Pepperjack','Good, Creepslayerz','Hero','Eli Pepperjack','Alive','Human',15,'Arcadia',null,'Mace, shurikens, umbrellas','Green','Black',null,'Cole Sand')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (16,'Steve Palchuk','Neutral, Good, Creepslayerz, The Trollhunters','Ally',null,'Alive','Human',16,'Arcadia',null,'His fists','Brown','Light brown','Various jocks','Steven Yeun')\n\t-- http://trollhunters.wikia.com/wiki/Category:Characters?page=2\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (17,'Vendel','Good','Hero',null,'Deceased','Troll',9000,'Heartstone Trollmarket','Rundle, Kilfred','Staff','White','Grayish brown',null,'Victor Raider-Wexler')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (18,'Kanjigar the Courageous','Good','Hero',null,'Dead','Troll',null,'The Void, The Hero''s Forge', 'Draal','Sword of Daylight','Yellow',null,null,'James Purefoy')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (19,'Merlin','Good',null,null,null,'Wizard',null,'Camelot',null,'Sorcery',null,null,'Trollhunters','David Bradley')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (20,'Jim Lake Sr.',null,null,null,null,'Human',null,null,'Jim Lake Jr., Barbara Lake',null,'Green','Black',null,null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (21,'Otto Scaarbach','Gunmar, The Janus Order','Villain','Otto, Mr. Evilman','Deceased','Changeling',null,null,null,'Polymorph abilities','Blue','Black','Goblins, Lower-ranking Changelings, NotEnrique','Tom Kenny')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (22,'Gladysgro','Gunmar, Bular, The Janus Order, Stricklander','Villain','Gladys','Deceased','Changeling',null,'Arcadia',null,'Dental tools','Blue','Brown',null,'Melanie Paxson')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (23,'Darci Scott','Good','Ally',null,'Alive','Human',15,'Arcadia',null,null,'Brown','Brown','Mary Wang','Yara Shahidi')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (24,'Mary J. Wang','Good','Hero','Maria, Mare','Alive','Human',15,'Arcadia',null,null,'Brown','Black',null,'Lauren Tom')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (25,'Deya the Deliverer','Good','Hero','Deya \"The Deliverer\"','Dead','Troll',null,'The Void, The Hero''s Forge',null,'Sword of Daylight',null,null,null,null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (26,'Unkar the Unfortunate','Good','Hero',null,'Deceased','Troll',null,'The Void, The Hero''s Forge',null,'Sword of Daylight',null,null,null,'Wallace Shawn')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (27,'Barbara Lake','Good','Hero',null,'Alive','Human',null,'Arcadia Oaks','Jim Lake Jr.',null,'Blue','Brown',null,'Amy Landecker')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (28,'Enrique Nuñez','Good',null,'Baby Enrique','Alive','Human',1,'Arcadia Oaks, Darklands','Claire Nuñez, Mr. Nuñez, Mrs. Nuñez',null,null,'Blonde',null,null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (29,'Gnome Chompsky','Good','Hero',null,'Alive','Gnome',null,'Hearstone Trollmarket, Toby''s dollhouse','Space Girldoll','Teeth',null,null,null,null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (30,'Waltolemew Strickler','Good',null,'Baby Strickler','Alive','Human',1,'Darklands',null,null,'Green','Brown',null,null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (31,'Goblins','Gumm-Gumm','Villain',null,'Alive','Trolls',null,'Arcadia','Large groups, Teeth',null,null,'Blue, Green',null,null)\n\t-- http://trollhunters.wikia.com/wiki/Category:Characters?page=3\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (32,'Gatto','Neutral',null,null,'Alive','Volcanic Troll',null,'Argentina',null,'Lava, His Stomach',null,null,null,null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (33,'Señor Uhl',null,'Teacher','Uhl the unforgiving','Alive','Human',38,'Arcadia',null,null,null,'Blonde',null,'Fred Tatasciore')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (34,'Shannon Longhannon','Good','Ally',null,'Alive','Human',15,'Arcadia',null,null,'Blue','Brown',null,'Bebe Wood')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (35,'Wumpa','Quagawumps','Ally',null,'Alive','Quagawump Troll',null,'Swamps of Florida','Blungo, Quagawumps','Spears','Green','Green','Quagawumps',null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (36,'Nyarlagroth','Gumm-Gumm','Bad',null,'Deceased','Huge Black Snake',null,'Darklands',null,'Large horns, Thick scales, Teeth, Tongues',null,null,null,null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (37,'Mrs. Nuñez',null,null,'Mom','Alive','Human',null,'Arcadia','Claire Nuñez, Enrique Nuñez, Mr. Nuñez','Politics, Soy Sausage','Brown','Black',null,'Andrea Navedo')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (38,'Mr. Nuñez',null,null,'Dad','Alive','Human',null,'Arcadia','Claire Nuñez, Enrique Nuñez, Mrs. Nuñez','BBQ','Brown','Black',null,'Tom Kenny')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (39,'Bagdwella','Good',null,null,'Alive','Troll',null,'Heartstone Trollmarket',null,null,'Orange','Red',null,'Fred Tatasciore')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (40,'Coach Lawrence',null,'Teacher','Coach','Alive','Human',56,'Arcadia',null,null,'Blue','Black',null,'Tom Wilson')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (41,'Gnomes','Thieves',null,'Scum of the earth','Alive','Gnome',null,'Hearstone Trollmarket',null,'Speed, Size, Teeth',null,null,null,null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (42,'The Pixies','Bad',null,null,'Alive','Pixies',null,'Container',null,'Size, Speed, Hallucination',null,null,null,null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (43,'Blood Goblins','Gumm-Gumm','Villain',null,'Alive','Trolls',null,'Draklands, Arcadia','Large groups, Teeth',null,null,'White',null,null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (44,'Nana','Good',null,null,'Alive','Human',null,'Arcadia','Toby Domzalski','Lack of sight','Blue','Gray',null,null);\nset identity_insert [Character] off;\ngo \nset identity_insert [Locations] on; --drop table [Locations] \n\tinsert into Locations (LocationId, LocationName, CharacterId) values\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(1,'Arcadia Oaks High', 5)\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(2,'Arcadia Oaks High', 40)\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(3,'Arcadia Oaks High', 33)\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(4,'Arcadia Oaks High', 1)\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(5,'Arcadia Oaks High', 13)\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(6,'Arcadia Oaks High', 2)\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(7,'Arcadia Oaks High', 16)\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(8,'Arcadia Oaks High', 15)\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(9,'Arcadia Oaks High', 24)\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(10,'Arcadia Oaks High', 23)\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(11,'Arcadia Oaks High', 34);\nset identity_insert Locations off;\ngo \ninsert\t[Episode] --drop table [Episode] \nselect\tc.CharacterId, FullName, VoicedBy, year(dateadd(year,-age,getdate())), Category, [Status], Age \nfrom\t[Character] c \njoin\t[Locations] l on c.CharacterId=l.CharacterId \nand\t\tc.Race='Human'\nand\t\tl.LocationName='Arcadia Oaks High'\norder by c.CharacterId;\ngo\n\n--  +-----------------------------+------------------------------------------------+\n--  | Angor spies on Trollhunters | Summon SHARK ! Kernel prevents local-to-local. |\n--  +-----------------------------+------------------------------------------------+\n--  :connect TrollHunters\nselect\t'Angor spies on Jim and friends', getdate(), * \nfrom\tdbo.[Character] c \njoin\t[Locations] l on c.CharacterId=l.CharacterId \nand\t\tc.Race='Human'\nand\t\tl.LocationName='Arcadia Oaks High';\ngo\n\n--  +---------------------------------------------+-------------------------------------------------------------------------------+\n--  | Angor releases Pixies, goes in for the kill | Release Pixies c# console app. 1101100000111011000010000001100011100111001000 |\n--  +---------------------------------------------+-------------------------------------------------------------------------------+\n--  :connect Pixes10101\n-- use [ᕙ༼,இܫஇ,༽ᕗ];\nselect 'Angor releases the Pixies', getdate(), * from dbo.Episode;\ngo\nselect 'Angor goes in for the kill', getdate(), * \nfrom\tdbo.[Character] c \njoin\tLocations l on c.CharacterId=l.CharacterId \nand\t\tc.Race='Human'\nand\t\tl.LocationName='Arcadia Oaks High';\ngo\n\n--  +----------------+\n--  | Toby saves Jim |\n--  +----------------+\n--  Turn on Force Encryption, with Certificate. Restart mssql svc.\n--  no more hallucinations. Jim and Toby kick Angor's butt and save Arcadia.\n--  :connect Pixes10101\ndrop login angor\ngo\n\n--  +----------+\n--  | Lore DDM |\n--  +----------+\n--  :connect localhost\n-- use [ᕙ༼,இܫஇ,༽ᕗ]\n-- go \n-- add users with select\ndeclare @sql nvarchar(max), @count int\nselect @count = count(*) from [Character] c inner join sysusers su on c.FullName = su.name where Race = 'Human';\nwhile @count < 17\nbegin\n\tselect top 1 @sql = 'create user ['+FullName+'] without login; grant select on [Character] to ['+FullName+'];' \n\tfrom [Character] where FullName not in (select name from sysusers) and Race = 'Human';\n\texec sp_executesql @sql;\n\tselect @count = count(*) from [Character] c inner join sysusers su on c.FullName = su.name where Race = 'Human';\nend\t\ngo\nselect name from sysusers su inner join [Character] c on c.FullName = su.name;\ngo\n--drop users \n/*\nuse [ᕙ༼,இܫஇ,༽ᕗ]\ngo \ndeclare @sql nvarchar(max), @count int\nselect @count = count(*) from [Character] c inner join sysusers su on c.FullName = su.name where Race = 'Human';\nwhile @count between 1 and 17 \nbegin\n\tselect top 1 @sql = 'drop user if exists ['+FullName+'] ;' from [Character] where FullName in (select name from sysusers) and Race = 'Human';\n\texec sp_executesql @sql;\n\tselect @count = count(*) from [Character] c inner join sysusers su on c.FullName = su.name where Race = 'Human';\nend\t\ngo*/\n-- Mask the Agent column with DDM.\ngo\n\n--  +------------------+\n--  | Killahead Bridge |\n--  +------------------+\nalter table [Character] alter column FullName\tadd masked with (function = 'Partial(0, \"---\", 0)');\nalter table [Character] alter column Aka\t\tadd masked with (function = 'Partial(0, \"---\", 0)');\nalter table [Character] alter column Race\t\tadd masked with (function = 'Partial(0, \"---\", 0)');\nalter table [Character] alter column Age\t\tadd masked with (function = 'default()'); --random(0,0)\nalter table [Character] alter column Relatives\tadd masked with (function = 'Partial(0, \"---\", 0)');\nalter table [Character] alter column EyeColor\tadd masked with (function = 'Partial(0, \"---\", 0)');\nalter table [Character] alter column HairColor\tadd masked with (function = 'Partial(0, \"---\", 0)');\nalter table [Character] alter column Minions\tadd masked with (function = 'Partial(0, \"---\", 0)');\ngo \n\n--  +-------------------------------------------------------+\n--  | For the glory of Merlin, Daylight is mine to command! |\n--  +-------------------------------------------------------+\ngrant unmask to [Jim Lake Jr.];\ngo\n\n--  +-----------------------------+\n--  | Jim becomes the Trollhunter |\n--  +-----------------------------+\nexecute as user = 'Jim Lake Jr.';\n\tselect 'Seen as Jim' as Person, * from [Character]; \nrevert;\ngo\n-- Mom doesn't know.\nexecute as user = 'Barbara Lake';\n\tselect 'Seen as Barbara' as Person, * from [Character]; \nrevert;\ngo\n-- Mom senses something's up.\nexecute as user = 'Barbara Lake';\n\tselect 'Seen as Barbara' as Person, *\n\tfrom [Character]\n\twhere FullName like '%Jim%' \nrevert;\ngo\n\n--  +---------------+\n--  | Mom finds out | --SAFEGUARD: DENY CREATE TABLE TO READ ONLY USERS\n--  +---------------+\ncreate table Letters (\n\tLetter \tvarchar(1) \tnot null)\n\t\ngo --drop table letters\n\ninsert Letters (Letter) values ('A')\ninsert Letters (Letter) values ('B')\ninsert Letters (Letter) values ('C')\ninsert Letters (Letter) values ('D')\ninsert Letters (Letter) values ('E')\ninsert Letters (Letter) values ('F')\ninsert Letters (Letter) values ('G')\ninsert Letters (Letter) values ('H')\ninsert Letters (Letter) values ('I')\ninsert Letters (Letter) values ('J')\ninsert Letters (Letter) values ('K')\ninsert Letters (Letter) values ('L')\ninsert Letters (Letter) values ('M')\ninsert Letters (Letter) values ('N')\ninsert Letters (Letter) values ('Ñ')\ninsert Letters (Letter) values ('O')\ninsert Letters (Letter) values ('P')\ninsert Letters (Letter) values('Q')\ninsert Letters (Letter) values ('R')\ninsert Letters (Letter) values ('S')\ninsert Letters (Letter) values('T')\ninsert Letters (Letter) values ('U')\ninsert Letters (Letter) values ('V')\ninsert Letters (Letter) values('W')\ninsert Letters (Letter) values ('X')\ninsert Letters (Letter) values ('Y')\ninsert Letters (Letter) values ('Z')\ninsert Letters (Letter) values(' ')\ninsert Letters (Letter) values ('.')\ninsert Letters (Letter) values ('0')\ninsert Letters (Letter) values ('1')\ninsert Letters (Letter) values ('3');\ngo\ngrant select on Letters to [Barbara Lake]; \ngo\nexecute as user = 'Barbara Lake';\n\tselect \t'Seen as Barbara' as Person\n\t,c.FullName\n\t,L01.Letter as L01\t,L02.Letter as L02\t,L03.Letter as L03\t,L04.Letter as L04\n\t,L05.Letter as L05\t,L06.Letter as L06\t,L07.Letter as L07\t,L08.Letter as L08\n\t,L09.Letter as L09\t,L10.Letter as L10\t,L11.Letter as L11\t,L12.Letter as L12\n\t,L13.Letter as L13\t,L14.Letter as L14\t,L15.Letter as L15\t,L16.Letter as L16\n\t,L17.Letter as L17\t,L18.Letter as L18\t,L19.Letter as L19\t,L20.Letter as L20\n\t,L21.Letter as L21\t,L22.Letter as L22\t,L23.Letter as L23\t,L24.Letter as L24\n\t,L25.Letter as L25\t,L26.Letter as L26\t,L27.Letter as L27\t,L28.Letter as L28\n\t,L29.Letter as L29\n\tfrom \t[Character] c\n\tinner join Letters L01 on (L01.Letter = substring(c.FullName,01,1))\n\tinner join Letters L02 on (L02.Letter = substring(c.FullName,02,1))\n\tinner join Letters L03 on (L03.Letter = substring(c.FullName,03,1))\n\tinner join Letters L04 on (L04.Letter = substring(c.FullName,04,1))\n\tinner join Letters L05 on (L05.Letter = substring(c.FullName,05,1))\n\tinner join Letters L06 on (L06.Letter = substring(c.FullName,06,1))\n\tinner join Letters L07 on (L07.Letter = substring(c.FullName,07,1))\n\tinner join Letters L08 on (L08.Letter = substring(c.FullName,08,1))\n\tinner join Letters L09 on (L09.Letter = substring(c.FullName,09,1))\n\tinner join Letters L10 on (L10.Letter = substring(c.FullName,10,1))\n\tinner join Letters L11 on (L11.Letter = substring(c.FullName,11,1))\n\tinner join Letters L12 on (L12.Letter = substring(c.FullName,12,1))\n\tinner join Letters L13 on (L13.Letter = substring(c.FullName,13,1))\n\tinner join Letters L14 on (L14.Letter = substring(c.FullName,14,1))\n\tinner join Letters L15 on (L15.Letter = substring(c.FullName,15,1))\n\tinner join Letters L16 on (L16.Letter = substring(c.FullName,16,1))\n\tinner join Letters L17 on (L17.Letter = substring(c.FullName,17,1))\n\tinner join Letters L18 on (L18.Letter = substring(c.FullName,18,1))\n\tinner join Letters L19 on (L19.Letter = substring(c.FullName,19,1))\n\tinner join Letters L20 on (L20.Letter = substring(c.FullName,20,1))\n\tinner join Letters L21 on (L21.Letter = substring(c.FullName,21,1))\n\tinner join Letters L22 on (L22.Letter = substring(c.FullName,22,1))\n\tinner join Letters L23 on (L23.Letter = substring(c.FullName,23,1))\n\tinner join Letters L24 on (L24.Letter = substring(c.FullName,24,1))\n\tinner join Letters L25 on (L25.Letter = substring(c.FullName,25,1))\n\tinner join Letters L26 on (L26.Letter = substring(c.FullName,26,1))\n\tinner join Letters L27 on (L27.Letter = substring(c.FullName,27,1))\n\tinner join Letters L28 on (L28.Letter = substring(c.FullName,28,1))\n\tinner join Letters L29 on (L29.Letter = substring(c.FullName,29,1))\nrevert;\ngo\n\n--  +--------------------------+\n--  | Killahead Bridge Dropped | --WE FINISHED DDM HACK.\n--  +--------------------------+\nalter table [Character] alter column FullName\tdrop masked ;\nalter table [Character] alter column Aka\t\tdrop masked ;\nalter table [Character] alter column Race\t\tdrop masked ;\nalter table [Character] alter column Age\t\tdrop masked ;\nalter table [Character] alter column Relatives\tdrop masked ;\nalter table [Character] alter column EyeColor\tdrop masked ;\nalter table [Character] alter column HairColor\tdrop masked ;\nalter table [Character] alter column Minions\tdrop masked ;\ngo \n\n--  +----------+\n--  | Lore RLS |\n--  +----------+\n--  :connect localhost\n--  add users with select. lines 172-181.\ndrop security policy if exists PortalPolicy;\ndrop function if exists Portal.fn_PortalAccess;\ndrop schema if exists Portal;\ndrop view if exists [Humans];\ngo\ncreate view [Humans] as \n\tselect\tFullName, [Status], Age, Home, Relatives, EyeColor, HairColor\n\tfrom\t[Character]\n\twhere\t( FullName = user_name() or Relatives like '%'+user_name()+'%' )\n\tor\t\tuser_name()='Jim Lake Jr.';\ngo\n--  Everyone in Arcadia knows eachother.\ndeclare @sql nvarchar(max)=null, @count int = null;\nselect @count = count(*) from sys.database_permissions where permission_name = 'select' and object_name(major_id) = 'Humans' and user_name(grantee_principal_id) in (select FullName from [Character] where Race = 'Human')\nwhile @count < 17\nbegin\n\tselect top 1 @sql = 'grant select on [Humans] to ['+FullName+'];'\n\tfrom [Character] where FullName in (select name from sysusers) and Race = 'Human' \n\tand FullName not in (select user_name(grantee_principal_id) from sys.database_permissions where permission_name = 'select' and object_name(major_id) = 'Humans');\n\texec sp_executesql @sql;\n\tselect @count = count(*) from sys.database_permissions where permission_name = 'select' and object_name(major_id) = 'Humans' and user_name(grantee_principal_id) in (select FullName from [Character] where Race = 'Human')\nend\t\ngo\nselect\tuser_name(grantee_principal_id) as [User], permission_name, object_name(major_id) as [OnObject] \nfrom\tsys.database_permissions \nwhere\tpermission_name = 'select' \nand\t\tobject_name(major_id) = 'Humans' \nand\t\tuser_name(grantee_principal_id) in (select\tFullName \n\t\t\t\t\t\t\t\t\t\t\tfrom\t[Character] \n\t\t\t\t\t\t\t\t\t\t\twhere\tRace = 'Human')\ngo\n--  Mom doesn't know about the Trolls.\nexecute as user = 'Barbara Lake';\n\tselect 'Seen as Barbara' as Person, * from [Humans]; \nrevert;\ngo\n--  By the glory of Merlin, Daylight is mine to command!\nexecute as user = 'Jim Lake Jr.';\n\tselect 'Seen as Jim' as Person, * from [Humans]; \nrevert;\ngo\n--  Only the heartstone can open the portal to Trollmarkert.\ncreate schema Portal; \ngo\ncreate function Portal.fn_PortalAccess (@FullName as sysname, @Relatives as sysname) \nreturns table with schemabinding as\nreturn\tselect\t1 as PortalAccess \n\t\twhere\t( @FullName = user_name() or @Relatives like '%'+user_name()+'%' )\n\t\tor\t\tuser_name() = 'Jim Lake Jr.';\ngo \ncreate security policy PortalPolicy\nadd filter predicate Portal.fn_PortalAccess (FullName, Relatives) on dbo.[Character] with (state = on);\ngo \n--  Trollhunters check if an RLS enchantment exists.\nselect object_name(object_id) as ObjectName, * from sys.security_policies;\nselect object_name(object_id) as ObjectName, * from sys.security_predicates;\ngo\n--  Mom doesn't know about the Trolls.\nexecute as user = 'Barbara Lake';\n\tselect 'Seen as Barbara' as Person, * from [Character]; \nrevert;\ngo\n--  By the glory of Merlin, Daylight is mine to command!\nexecute as user = 'Jim Lake Jr.';\n\tselect 'Seen as Jim' as Person, * from [Character]; \nrevert;\ngo\n--  Gunmar gets the heartstone.\nalter security policy PortalPolicy with (state = off);\ngo\n--  Trollhunters check if an RLS enchantment exists.\nselect object_name(object_id) as ObjectName, * from sys.security_policies;\nselect object_name(object_id) as ObjectName, * from sys.security_predicates;\ngo\n--  Mom learns the trolls when Gunmar attacked Arcadia.\nexecute as user = 'Barbara Lake';\n\tselect 'Seen as Barbara' as Person, * from [Character]; \nrevert; \ngo\n--  Mom helps the trollhunters save Arcadia from the Gum-Gum army.\nexecute as user = 'Barbara Lake';\n\tselect * from sys.partitions where object_id = object_id('dbo.Character');\nrevert;\ngo\nexecute as user = 'Barbara Lake';\n    select 'Seen as Barbara' as Person, 1 / (Age - 16), * from [Character]; --Divide by zero error encountered.\nrevert;\ngo\n--Jim and the trollhunters fight the gum-gum army.\nalter security policy PortalPolicy with (state = on); --Command(s) completed successfully.\ngo\n--  Mom continues to help Jim save Arcadia.\nexecute as user = 'Barbara Lake';\n    select 'Seen as Barbara' as Person, 1 / (Age - 16), * from [Character]; --Patched error in SQL2016 CUs. SQL2017 RTM still occurs.\nrevert;\ngo\nexecute as user = 'Barbara Lake';\n    select 'Seen as Barbara' as Person, * from [Character]\n\twhere 1 = 1 / (Age - 56); --Divide by zero error encountered. \nrevert; --9000, 4999, 600, 56, 38, 16, 15, 1\ngo\nexecute as user = 'Barbara Lake';\n\texec sp_columns [Character]; \n\texec sp_pkeys [Character]; \n\texec sp_fkeys [Character];\nrevert;\ngo\n\n--  +--------------------------------------------------------------------+\n--  | Barbara can brute-force devide by 0 to identify every integer.     |\n--  | Barbara can use the DDM letters table to identify every character. |\n--  | When all seems lost, our hero...\t\t\t\t\t\t\t\t\t |\n--\t| Jim the Trollhunter has the Amulet Of Daylight!\t\t\t\t\t |\n--  +--------------------------------------------------------------------+",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/alterdw')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- use master\n\nalter database wplussynapsedw set READ_COMMITTED_SNAPSHOT on with NO_WAIT\nalter database wplussynapsedw set RESULT_SET_CACHING on with NO_WAIT   ",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/fwrules')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- DW / Dedicated SQL Pool\n-- use master\n\nselect * from master.sys.firewall_rules;\n\nexec sp_delete_firewall_rule 'AllowAllWindowsAzureIps';  ",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Delta')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threedot0",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "Synapse Spark"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threedot0",
						"name": "threedot0",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threedot0",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"case class Data(key: String, value: String)\r\n",
							"\r\n",
							"case class ChangeData(key: String, newValue: String, deleted: Boolean, time: Long) {\r\n",
							"  assert(newValue != null ^ deleted)\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val target = Seq(\r\n",
							"  Data(\"a\", \"0\"),\r\n",
							"  Data(\"b\", \"1\"),\r\n",
							"  Data(\"c\", \"2\"),\r\n",
							"  Data(\"d\", \"3\")\r\n",
							").toDF()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"sql(\"drop table if exists target\")\r\n",
							"target.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"target\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import com.microsoft.spark.sqlanalytics.utils.Constants\r\n",
							"import org.apache.spark.sql.SqlAnalyticsConnector._\r\n",
							"\r\n",
							"val targetDF = spark.sqlContext.sql(\"select * from target\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val changeDataSource = Seq(\r\n",
							"  ChangeData(\"a\", \"10\", deleted = false, time = 0),\r\n",
							"  ChangeData(\"a\", null, deleted = true, time = 1),   // a was updated and then deleted\r\n",
							"  ChangeData(\"b\", null, deleted = true, time = 2),   // b was just deleted once\r\n",
							"  ChangeData(\"c\", null, deleted = true, time = 3),   // c was deleted and then updated twice\r\n",
							"  ChangeData(\"c\", \"20\", deleted = false, time = 4),\r\n",
							"  ChangeData(\"c\", \"200\", deleted = false, time = 5)\r\n",
							").toDF().createOrReplaceTempView(\"changes\")\r\n",
							"\r\n",
							"// val changeDataSource = Seq(\r\n",
							"//   ChangeData(\"a\", \"10\", deleted = false, time = 0),\r\n",
							"//   ChangeData(\"a\", null, deleted = true, time = 1),   // a was updated and then deleted\r\n",
							"//   ChangeData(\"b\", null, deleted = true, time = 2),   // b was just deleted once\r\n",
							"//   ChangeData(\"c\", null, deleted = true, time = 3),   // c was deleted and then updated twice\r\n",
							"//   ChangeData(\"c\", \"20\", deleted = false, time = 4),\r\n",
							"//   ChangeData(\"d\", \"82\", deleted = false, time = 5)\r\n",
							"//   ChangeData(\"e\", \"19\", deleted = false, time = 6)\r\n",
							"//   ChangeData(\"f\", \"12\", deleted = false, time = 7)\r\n",
							"//   ChangeData(\"g\", null, deleted = false, time = 8)\r\n",
							"//   ChangeData(\"d\", \"21\", deleted = false, time = 9)\r\n",
							"// ).toDF().createOrReplaceTempView(\"changes\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql \r\n",
							"MERGE INTO target t\r\n",
							"USING (\r\n",
							"  -- Find the latest change for each key based on the timestamp\r\n",
							"  SELECT key, latest.newValue as newValue, latest.deleted as deleted FROM (    \r\n",
							"    -- Note: For nested structs, max on struct is computed as \r\n",
							"    -- max on first struct field, if equal fall back to second fields, and so on.\r\n",
							"    SELECT key, max(struct(time, newValue, deleted)) as latest FROM changes GROUP BY key\r\n",
							"  )\r\n",
							") s\r\n",
							"ON s.key = t.key\r\n",
							"WHEN MATCHED AND s.deleted = true THEN DELETE\r\n",
							"WHEN MATCHED THEN UPDATE SET key = s.key, value = s.newValue\r\n",
							"WHEN NOT MATCHED AND s.deleted = false THEN INSERT (key, value) VALUES (key, newValue)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val changesDF = spark.sql(\"select * from changes\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val latestChangeForEachKey = changesDF.selectExpr(\"key\", \"struct(time, newValue, deleted) as otherCols\").groupBy(\"key\").agg(max(\"otherCols\").as(\"latest\")).selectExpr(\"key\", \"latest.*\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"latestChangeForEachKey.createOrReplaceTempView(\"lastchanges\")"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"latestChangeForEachKey.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lastchanges\") "
						],
						"outputs": [],
						"execution_count": 15
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Getting Started with Delta Lake')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					}
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Hitchhiker's Guide to Delta Lake (Python)\n",
							"\n",
							"This tutorial has been adapted for more clarity from its original counterpart [here](https://docs.delta.io/latest/quick-start.html). This notebook helps you quickly explore the main features of [Delta Lake](https://github.com/delta-io/delta). It provides code snippets that show how to read from and write to Delta Lake tables from interactive, batch, and streaming queries.\n",
							"\n",
							"Here's what we will cover:\n",
							"* Create a table\n",
							"* Understanding meta-data\n",
							"* Read data\n",
							"* Update table data\n",
							"* Overwrite table data\n",
							"* Conditional update without overwrite\n",
							"* Read older versions of data using Time Travel\n",
							"* Write a stream of data to a table\n",
							"* Read a stream of changes from a table"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Configuration\n",
							"Make sure you modify this as appropriate."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import random\n",
							"\n",
							"session_id = random.randint(0,1000000)\n",
							"delta_table_path = \"/delta/delta-table-{0}\".format(session_id)\n",
							"\n",
							"delta_table_path"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Create a table\n",
							"To create a Delta Lake table, write a DataFrame out in the **delta** format. You can use existing Spark SQL code and change the format from parquet, csv, json, and so on, to delta.\n",
							"\n",
							"These operations create a new Delta Lake table using the schema that was inferred from your DataFrame. For the full set of options available when you create a new Delta Lake table, see Create a table and Write to a table (subsequent cells in this notebook)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"data = spark.range(0,5)\n",
							"data.show()\n",
							"data.write.format(\"delta\").save(delta_table_path)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Understanding Meta-data\n",
							"\n",
							"In Delta Lake, meta-data is no different from data i.e., it is stored next to the data. Therefore, an interesting side-effect here is that you can peek into meta-data using regular Spark APIs. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"[log_line.value for log_line in spark.read.text(delta_table_path + \"/_delta_log/\").collect()]"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read data\n",
							"\n",
							"You read data in your Delta Lake table by specifying the path to the files."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.format(\"delta\").load(delta_table_path)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Update table data\n",
							"\n",
							"Delta Lake supports several operations to modify tables using standard DataFrame APIs. This example runs a batch job to overwrite the data in the table.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"data = spark.range(5,10)\n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"When you now inspect the meta-data, what you will notice is that the original data is over-written. Well, not in a true sense but appropriate entries are added to Delta's transaction log so it can provide an \"illusion\" that the original data was deleted. We can verify this by re-inspecting the meta-data. You will see several entries indicating reference removal to the original data."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"[log_line.value for log_line in spark.read.text(delta_table_path + \"/_delta_log/\").collect()]"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Save as catalog tables\n",
							"\n",
							"Delta Lake can write to managed or external catalog tables."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Write data to a new managed catalog table.\n",
							"data.write.format(\"delta\").saveAsTable(\"ManagedDeltaTable\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"# Define an external catalog table that points to the existing Delta Lake data in storage.\n",
							"spark.sql(\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '{0}'\".format(delta_table_path))"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"# List the 2 new tables.\n",
							"spark.sql(\"SHOW TABLES\").show()\n",
							"\n",
							"# Explore their properties.\n",
							"spark.sql(\"DESCRIBE EXTENDED ManagedDeltaTable\").show(truncate=False)\n",
							"spark.sql(\"DESCRIBE EXTENDED ExternalDeltaTable\").show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Conditional update without overwrite\n",
							"\n",
							"Delta Lake provides programmatic APIs to conditional update, delete, and merge (upsert) data into tables. For more information on these operations, see [Table Deletes, Updates, and Merges](https://docs.delta.io/latest/delta-update.html)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from delta.tables import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"delta_table = DeltaTable.forPath(spark, delta_table_path)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"# Update every even value by adding 100 to it\n",
							"delta_table.update(\n",
							"  condition = expr(\"id % 2 == 0\"),\n",
							"  set = { \"id\": expr(\"id + 100\") })\n",
							"delta_table.toDF().show()"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"# Delete every even value\n",
							"delta_table.delete(\"id % 2 == 0\")\n",
							"delta_table.toDF().show()"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"# Upsert (merge) new data\n",
							"new_data = spark.range(0,20).alias(\"newData\")\n",
							"\n",
							"delta_table.alias(\"oldData\")\\\n",
							"    .merge(new_data.alias(\"newData\"), \"oldData.id = newData.id\")\\\n",
							"    .whenMatchedUpdate(set = { \"id\": lit(\"-1\")})\\\n",
							"    .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") })\\\n",
							"    .execute()\n",
							"\n",
							"delta_table.toDF().show(100)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"source": [
							"## History\n",
							"Delta's most powerful feature is the ability to allow looking into history i.e., the changes that were made to the underlying Delta Table. The cell below shows how simple it is to inspect the history."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().show(20, 1000, False)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read older versions of data using Time Travel\n",
							"\n",
							"You can query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option.\n",
							"\n",
							"Once you run the cell below, you should see the first set of data, from before you overwrote it. Time Travel is an extremely powerful feature that takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again. For more information, see [Query an older snapshot of a table (time travel)](https://docs.delta.io/latest/delta-batch.html#deltatimetravel)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write a stream of data to a table\n",
							"\n",
							"You can also write to a Delta Lake table using Spark's Structured Streaming. The Delta Lake transaction log guarantees exactly-once processing, even when there are other streams or batch queries running concurrently against the table. By default, streams run in append mode, which adds new records to the table.\n",
							"\n",
							"For more information about Delta Lake integration with Structured Streaming, see [Table Streaming Reads and Writes](https://docs.delta.io/latest/delta-streaming.html).\n",
							"\n",
							"In the cells below, here's what we are doing:\n",
							"\n",
							"1. *Cell 28* Setup a simple Spark Structured Streaming job to generate a sequence and make the job write into our Delta Table\n",
							"2. *Cell 30* Show the newly appended data\n",
							"3. *Cell 31* Inspect history\n",
							"4. *Cell 32* Stop the structured streaming job\n",
							"5. *Cell 33* Inspect history <-- You'll notice appends have stopped"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"streaming_df = spark.readStream.format(\"rate\").load()\n",
							"stream = streaming_df\\\n",
							"    .selectExpr(\"value as id\")\\\n",
							"    .writeStream\\\n",
							"    .format(\"delta\")\\\n",
							"    .option(\"checkpointLocation\", \"/tmp/checkpoint-{0}\".format(session_id))\\\n",
							"    .start(delta_table_path)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read a stream of changes from a table\n",
							"\n",
							"While the stream is writing to the Delta Lake table, you can also read from that table as streaming source. For example, you can start another streaming query that prints all the changes made to the Delta Lake table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.toDF().sort(col(\"id\").desc()).show(100)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(20, 1000, False)"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"source": [
							"stream.stop()"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(100, 1000, False)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Compaction\n",
							"\n",
							"If a Delta Table is growing too large, you can compact it by repartitioning into a smaller number of files.\n",
							"\n",
							"The option `dataChange = false` is an optimization that tells Delta Lake to do the repartition without marking the underlying data as \"modified\". This ensures that any other concurrent operations (such as streaming reads/writes) aren't negatively impacted.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"partition_count = 2\n",
							"\n",
							"spark.read\\\n",
							"    .format(\"delta\")\\\n",
							"    .load(delta_table_path)\\\n",
							"    .repartition(partition_count)\\\n",
							"    .write.option(\"dataChange\", \"false\")\\\n",
							"    .format(\"delta\")\\\n",
							"    .mode(\"overwrite\")\\\n",
							"    .save(delta_table_path)    "
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Convert Parquet to Delta\n",
							"You can do an in-place conversion from the Parquet format to Delta."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_path = \"/parquet/parquet-table-{0}\".format(session_id)\n",
							"\n",
							"data = spark.range(0,5)\n",
							"data.write.parquet(parquet_path)\n",
							"\n",
							"# Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"source": [
							"DeltaTable.convertToDelta(spark, \"parquet.`{0}`\".format(parquet_path))\n",
							"\n",
							"# Confirm that the converted data is now in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "markdown",
						"source": [
							"## SQL Support\n",
							"Delta supports table utility commands through SQL.  You can use SQL to:\n",
							"* Get a DeltaTable's history\n",
							"* Vacuum a DeltaTable\n",
							"* Convert a Parquet file to Delta\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DESCRIBE HISTORY delta.`{0}`\".format(delta_table_path)).show()"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"VACUUM delta.`{0}`\".format(delta_table_path)).show()"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_id = random.randint(0,1000)\n",
							"parquet_path = \"/parquet/parquet-table-{0}-{1}\".format(session_id, parquet_path)\n",
							"\n",
							"data = spark.range(0,5)\n",
							"data.write.parquet(parquet_path)\n",
							"\n",
							"# Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)\n",
							"\n",
							"# Use SQL to convert the parquet table to Delta\n",
							"spark.sql(\"CONVERT TO DELTA parquet.`{0}`\".format(parquet_path))\n",
							"\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"outputs": [],
						"execution_count": 26
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/HiSpark')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threedot0",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threedot0",
						"name": "threedot0",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threedot0",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.0",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"print('hi spark')"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import pkg_resources\r\n",
							"for d in pkg_resources.working_set:\r\n",
							"     print(d)"
						],
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/wplussynapsedw')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"restorePointInTime": "0001-01-01T00:00:00",
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Delta2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threedot0",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "Synapse Spark"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threedot0",
						"name": "threedot0",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threedot0",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Delta Demo\r\n",
							"\r\n",
							"1.\tSpark, scala delta, apis, sql magics - merge (see [this notebook](https://github.com/hfleitas/synapsedelta/blob/main/notebook/Delta.json))\r\n",
							"2.\tServerless, credentials, open query (see sql [script](https://github.com/hfleitas/synapsedelta/blob/main/sqlscript/DeltaServerless.json))\r\n",
							"3.\tIntegration pipeline, copy activity with overwrite if exists (see [pipeline](https://github.com/hfleitas/synapsedelta/blob/main/pipeline/Delta-Import.json))\r\n",
							"4.\tSql pools - merge (see sql [script](https://github.com/hfleitas/synapsedelta/blob/main/sqlscript/DeltaSQLPoolMerge.json))"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"spark.version"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"case class Data(key: String, value: String)\r\n",
							"\r\n",
							"case class ChangeData(key: String, newValue: String, deleted: Boolean, time: Long) {\r\n",
							"  assert(newValue != null ^ deleted)\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val target = Seq(\r\n",
							"  Data(\"a\", \"0\"),\r\n",
							"  Data(\"b\", \"1\"),\r\n",
							"  Data(\"c\", \"2\"),\r\n",
							"  Data(\"d\", \"3\")\r\n",
							").toDF()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"sql(\"drop table if exists target\")\r\n",
							"target.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"target\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import com.microsoft.spark.sqlanalytics.utils.Constants\r\n",
							"import org.apache.spark.sql.SqlAnalyticsConnector._\r\n",
							"\r\n",
							"val targetDF = spark.sqlContext.sql(\"select * from target\")\r\n",
							"\r\n",
							"// targetDF.write.option(Constants.SERVER, \"wplushiramsynapse.sql.azuresynapse.net\").synapsesql(\"wplussynapsedw.dbo.target\", Constants.INTERNAL)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from target order by key"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"describe extended target"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val changeDataSource = Seq(\r\n",
							"  ChangeData(\"a\", \"10\", deleted = false, time = 0),\r\n",
							"  ChangeData(\"a\", null, deleted = true, time = 1),   // a was updated and then deleted\r\n",
							"  ChangeData(\"b\", null, deleted = true, time = 2),   // b was just deleted once\r\n",
							"  ChangeData(\"c\", null, deleted = true, time = 3),   // c was deleted and then updated twice\r\n",
							"  ChangeData(\"c\", \"20\", deleted = false, time = 4),\r\n",
							"  ChangeData(\"c\", \"200\", deleted = false, time = 5)\r\n",
							").toDF().createOrReplaceTempView(\"changes\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### SQL Example"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql \r\n",
							"MERGE INTO target t\r\n",
							"USING (\r\n",
							"  -- Find the latest change for each key based on the timestamp\r\n",
							"  SELECT key, latest.newValue as newValue, latest.deleted as deleted FROM (    \r\n",
							"    -- Note: For nested structs, max on struct is computed as \r\n",
							"    -- max on first struct field, if equal fall back to second fields, and so on.\r\n",
							"    SELECT key, max(struct(time, newValue, deleted)) as latest FROM changes GROUP BY key\r\n",
							"  )\r\n",
							") s\r\n",
							"ON s.key = t.key\r\n",
							"WHEN MATCHED AND s.deleted = true THEN DELETE\r\n",
							"WHEN MATCHED THEN UPDATE SET key = s.key, value = s.newValue\r\n",
							"WHEN NOT MATCHED AND s.deleted = false THEN INSERT (key, value) VALUES (key, newValue)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// import io.delta.tables._\r\n",
							"// import org.apache.spark.sql.functions._\r\n",
							"\r\n",
							"// val deltaTable = DeltaTable.forName(\"target\")\r\n",
							"val changesDF = spark.sql(\"select * from changes\")\r\n",
							"changesDF.head()"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val latestChangeForEachKey = changesDF.selectExpr(\"key\", \"struct(time, newValue, deleted) as otherCols\").groupBy(\"key\").agg(max(\"otherCols\").as(\"latest\")).selectExpr(\"key\", \"latest.*\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"latestChangeForEachKey.show() // shows the latest change for each key"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"latestChangeForEachKey.createOrReplaceTempView(\"lastchanges\")"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from lastchanges"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"//instead of delta to read as ext table in dedicated pool\r\n",
							"\r\n",
							"latestChangeForEachKey.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lastchanges\") "
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### SCALA Merge example (Skipped)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// deltaTable.as(\"t\")\r\n",
							"//   .merge(\r\n",
							"//     latestChangeForEachKey.as(\"s\"),\r\n",
							"//     \"s.key = t.key\")\r\n",
							"//   .whenMatched(\"s.deleted = true\")\r\n",
							"//   .delete()\r\n",
							"//   .whenMatched()\r\n",
							"//   .updateExpr(Map(\"key\" -> \"s.key\", \"value\" -> \"s.newValue\"))\r\n",
							"//   .whenNotMatched(\"s.deleted = false\")\r\n",
							"//   .insertExpr(Map(\"key\" -> \"s.key\", \"value\" -> \"s.newValue\"))\r\n",
							"//   .execute()"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// import com.microsoft.spark.sqlanalytics.utils.Constants\r\n",
							"// import org.apache.spark.sql.SqlAnalyticsConnector._\r\n",
							"\r\n",
							"val lastchangesDF = spark.sqlContext.sql(\"select * from lastchanges\")\r\n",
							"\r\n",
							"// lastchangesDF.write.option(Constants.SERVER, \"wplushiramsynapse.sql.azuresynapse.net\").synapsesql(\"wplussynapsedw.dbo.lastchanges\", Constants.INTERNAL)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Resources\r\n",
							"* https://docs.databricks.com/_static/notebooks/merge-in-cdc.html\r\n",
							"* https://techcommunity.microsoft.com/t5/azure-synapse-analytics/query-delta-lake-files-using-t-sql-language-in-azure-synapse/ba-p/2388398\r\n",
							"* https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/query-delta-lake-format\r\n",
							"* https://databricks.com/blog/2019/03/19/efficient-upserts-into-data-lakes-databricks-delta.html\r\n",
							"* https://docs.microsoft.com/en-us/sql/t-sql/statements/merge-transact-sql?view=azure-sqldw-latest\r\n",
							"* https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html\r\n",
							"* https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-develop-ctas\r\n",
							"* https://www.mssqltips.com/sqlservertip/6282/azure-data-factory-multiple-file-load-example-part-2/"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Primer')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "IfPausedThenOnline",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "GetStatus",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@not(equals('Online',string(activity('GetStatus').output.properties.status)))",
								"type": "Expression"
							},
							"ifTrueActivities": [
								{
									"name": "Resume",
									"description": "Start",
									"type": "WebActivity",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"url": {
											"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Sql/servers/',pipeline().parameters.Server,'/databases/',pipeline().parameters.DW,'/resume?api-version=2018-06-01-preview')",
											"type": "Expression"
										},
										"method": "POST",
										"headers": {},
										"body": {
											"value": "@string('')",
											"type": "Expression"
										},
										"authentication": {
											"type": "MSI",
											"resource": "https://management.core.windows.net/"
										}
									}
								},
								{
									"name": "ScaleAPI",
									"type": "WebActivity",
									"dependsOn": [
										{
											"activity": "Resume",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"url": {
											"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Sql/servers/',pipeline().parameters.Server,'/databases/',pipeline().parameters.DW,'?api-version=2018-06-01-preview')",
											"type": "Expression"
										},
										"method": "PUT",
										"headers": {
											"Content-Type": "application/json"
										},
										"body": {
											"value": "@concat('{\"location\":\"eastus2\",\"sku\":{\"name\": \"',pipeline().parameters.SLO,'\"}}')",
											"type": "Expression"
										},
										"authentication": {
											"type": "MSI",
											"resource": "https://management.core.windows.net/"
										}
									}
								}
							]
						}
					},
					{
						"name": "GetStatus",
						"description": "Start",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Sql/servers/',pipeline().parameters.Server,'/databases/',pipeline().parameters.DW,'?api-version=2018-06-01-preview')",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "GET",
							"headers": {},
							"body": {
								"value": "@string('')",
								"type": "Expression"
							},
							"authentication": {
								"type": "MSI",
								"resource": "https://management.core.windows.net/"
							}
						}
					},
					{
						"name": "UntilOnline",
						"type": "Until",
						"dependsOn": [
							{
								"activity": "IfPausedThenOnline",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@equals('Online',string(activity('GetStatusOnline').output.properties.status))\n",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "GetStatusOnline",
									"description": "Start",
									"type": "WebActivity",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"url": {
											"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Sql/servers/',pipeline().parameters.Server,'/databases/',pipeline().parameters.DW,'?api-version=2018-06-01-preview')",
											"type": "Expression"
										},
										"method": "GET",
										"headers": {},
										"body": {
											"value": "@string('')",
											"type": "Expression"
										},
										"authentication": {
											"type": "MSI",
											"resource": "https://management.core.windows.net/"
										}
									}
								}
							],
							"timeout": "7.00:00:00"
						}
					},
					{
						"name": "execTrollhunters",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "UntilOnline",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Delta-Import",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "IfOnlinePause",
						"description": "Sacle down and Pause",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "GetStatusEnd",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@equals('Online',string(activity('GetStatusEnd').output.properties.status))",
								"type": "Expression"
							},
							"ifTrueActivities": [
								{
									"name": "ScaleWebhook",
									"description": "",
									"type": "WebHook",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"url": "https://s1events.azure-automation.net/webhooks?token=EnUcmPAqjmhXx7uKcLIj%2bN%2fNkzod6WPU6CaGc%2bhaPQI%3d",
										"method": "POST",
										"headers": {},
										"body": {
											"WebhookName": "scaledw"
										},
										"timeout": "00:10:00"
									}
								},
								{
									"name": "Pause",
									"description": "Pause",
									"type": "WebActivity",
									"dependsOn": [
										{
											"activity": "ScaleWebhook",
											"dependencyConditions": [
												"Completed"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"url": {
											"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Sql/servers/',pipeline().parameters.Server,'/databases/',pipeline().parameters.DW,'/pause?api-version=2018-06-01-preview')",
											"type": "Expression"
										},
										"connectVia": {
											"referenceName": "AutoResolveIntegrationRuntime",
											"type": "IntegrationRuntimeReference"
										},
										"method": "POST",
										"headers": {},
										"body": {
											"value": "@string('')",
											"type": "Expression"
										},
										"authentication": {
											"type": "MSI",
											"resource": "https://management.core.windows.net/"
										}
									}
								}
							]
						}
					},
					{
						"name": "GetStatusEnd",
						"description": "Start",
						"type": "WebActivity",
						"dependsOn": [
							{
								"activity": "execTrollhunters",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Sql/servers/',pipeline().parameters.Server,'/databases/',pipeline().parameters.DW,'?api-version=2018-06-01-preview')",
								"type": "Expression"
							},
							"method": "GET",
							"headers": {},
							"body": {
								"value": "@string('')",
								"type": "Expression"
							},
							"authentication": {
								"type": "MSI",
								"resource": "https://management.core.windows.net/"
							}
						}
					},
					{
						"name": "Notify",
						"description": "",
						"type": "WebActivity",
						"dependsOn": [
							{
								"activity": "execTrollhunters",
								"dependencyConditions": [
									"Failed"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": "https://prod-23.eastus2.logic.azure.com:443/workflows/a3b71278fcd74578912de49da8f0cc41/triggers/manual/paths/invoke?api-version=2016-10-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=hDohhaBVFqS07vlHEOhum2ZLvhUnvyi1u9oqKBO5fRc",
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "POST",
							"headers": {
								"Content-type": "application/json"
							},
							"body": {
								"value": "@concat('{\n   \"DataFactoryName\": \"',pipeline().DataFactory,'\",\n   \"PipelineName\": \"',pipeline().Pipeline,'\",\n   \"Subject\": \"An error has occurred!\",\n   \"ErrorMessage\": \"The ADF pipeline failed. Please check the activity log.\",\n   \"EmailTo\": \"',pipeline().parameters.Notify,'\"\n}')",
								"type": "Expression"
							}
						}
					}
				],
				"parameters": {
					"SubscriptionID": {
						"type": "string",
						"defaultValue": "eaab21d5-8ecd-4ef0-a0c4-92fac2e22875"
					},
					"ResourceGroup": {
						"type": "string",
						"defaultValue": "dw"
					},
					"Server": {
						"type": "string",
						"defaultValue": "hiramdw"
					},
					"DW": {
						"type": "string",
						"defaultValue": "dw"
					},
					"SLO": {
						"type": "string",
						"defaultValue": "DW1000c"
					},
					"Notify": {
						"type": "string",
						"defaultValue": "hiramfleitas@microsoft.com"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/pipelines/Delta-Import')]"
			]
		}
	]
}