{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "wplushiramsynapse"
		},
		"LaptopFact_password": {
			"type": "secureString",
			"metadata": "Secure string for 'password' of 'LaptopFact'"
		},
		"SQLOnDemand_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'SQLOnDemand'"
		},
		"adbhf_accessToken": {
			"type": "secureString",
			"metadata": "Secure string for 'accessToken' of 'adbhf'"
		},
		"iotmonitoringsa10774_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'iotmonitoringsa10774'"
		},
		"wplushiramsynapse-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'wplushiramsynapse-WorkspaceDefaultSqlServer'"
		},
		"wplushiramsynapseLoaderXL_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'wplushiramsynapseLoaderXL'"
		},
		"AzureKeyVault1_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://synapseAKV.vault.azure.net/"
		},
		"LabFilesZip_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://pbiworkshoplabsgvw.blob.core.windows.net/synapse/LabFiles.zip"
		},
		"LaptopFact_properties_typeProperties_host": {
			"type": "string",
			"defaultValue": "C:/hiram-msft/ws-SynapseSQLPools"
		},
		"LaptopFact_properties_typeProperties_userId": {
			"type": "string",
			"defaultValue": "LAPTOP-RRU3V9OS\\dwloader"
		},
		"PublicDataset_sasUri": {
			"type": "secureString",
			"metadata": "Secure string for 'sasUri' of 'PublicDataset'"
		},
		"freeadx_properties_typeProperties_database": {
			"type": "string",
			"defaultValue": "MyDatabase"
		},
		"nyctaxistaging_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://wplushiramsynapseadlsv2.dfs.core.windows.net/"
		},
		"powerapps_properties_typeProperties_connectionString_secretName": {
			"type": "string",
			"defaultValue": "powerappssqlconnectionstring"
		},
		"sqldbedgeoulvzyml_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "e4e06275-58d1-4081-8f1b-be12462eb701"
		},
		"sqldbedgeoulvzyml_properties_typeProperties_resourceGroupName": {
			"type": "string",
			"defaultValue": "wind"
		},
		"wplushiramsynapse-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://wplushiramsynapseadlsv2.dfs.core.windows.net"
		},
		"Daily_properties_DevOpsCICD_parameters_environment": {
			"type": "string",
			"defaultValue": "dev"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/13456789123456789123456789123456789123456789')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Wait1",
						"type": "Wait",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"waitTimeInSeconds": 1
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "_adhoc"
				},
				"annotations": [],
				"lastPublishTime": "2022-03-23T05:25:01Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ADFOrcIntoDW')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy data1",
						"description": "Would like to use wildcard path for diff partition folders and additional column for filepath month.",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "OrcSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"wildcardFolderPath": "orc_sample",
									"wildcardFileName": "*.*",
									"enablePartitionDiscovery": false
								}
							},
							"sink": {
								"type": "SqlPoolSink",
								"preCopyScript": "truncate table staging.orcsample",
								"allowPolyBase": true,
								"polyBaseSettings": {
									"rejectValue": 0,
									"rejectType": "value",
									"useTypeDefault": true
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "Orc_Sample",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "OrcSample",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "_adhoc"
				},
				"annotations": [],
				"lastPublishTime": "2022-03-23T05:25:07Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Orc_Sample')]",
				"[concat(variables('workspaceId'), '/datasets/OrcSample')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CSV to ADX')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy data1",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "AzureDataExplorerSink"
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": false,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "LengthOfStay_cooked_small",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "freeadx",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "Kusto"
				},
				"annotations": [],
				"lastPublishTime": "2022-03-22T22:23:40Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/LengthOfStay_cooked_small')]",
				"[concat(variables('workspaceId'), '/datasets/freeadx')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CrimeData-ADB')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy_dpx",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Get Key from AKV",
								"dependencyConditions": [
									"Skipped"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [
							{
								"name": "Source",
								"value": "training/crime-data-2016/"
							},
							{
								"name": "Destination",
								"value": "dwtemp/03.02/"
							}
						],
						"typeProperties": {
							"source": {
								"type": "BinarySource",
								"storeSettings": {
									"type": "AzureBlobStorageReadSettings",
									"recursive": true,
									"wildcardFileName": "*"
								},
								"formatSettings": {
									"type": "BinaryReadSettings"
								}
							},
							"sink": {
								"type": "BinarySink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings",
									"copyBehavior": "PreserveHierarchy"
								}
							},
							"enableStaging": false,
							"enableSkipIncompatibleRow": false,
							"skipErrorFile": {
								"fileMissing": true
							},
							"validateDataConsistency": false
						},
						"inputs": [
							{
								"referenceName": "PublicDataset",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "DestinationDataset_nuj",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "LabNotebook",
						"type": "DatabricksNotebook",
						"dependsOn": [
							{
								"activity": "Get Key from AKV",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebookPath": "/Repos/hiramfleitas@microsoft.com/hiramdatabricks/Includes/Databricks-Data-Transformations",
							"baseParameters": {
								"accountName": "wplushiramsynapseadlsv2",
								"accountKey": {
									"value": "@string(activity('Get Key from AKV').output.value)",
									"type": "Expression"
								},
								"containerName": "dwtemp"
							}
						},
						"linkedServiceName": {
							"referenceName": "adbhf",
							"type": "LinkedServiceReference"
						}
					},
					{
						"name": "Get Key from AKV",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": true,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@concat(pipeline().parameters.KeyVaultDNSName,'secrets/accountkey/?api-version=7.0')",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "GET",
							"headers": {},
							"authentication": {
								"type": "MSI",
								"resource": "https://vault.azure.net"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"KeyVaultDNSName": {
						"type": "string",
						"defaultValue": "https://synapseakv.vault.azure.net/"
					}
				},
				"folder": {
					"name": "_adhoc"
				},
				"annotations": [],
				"lastPublishTime": "2022-03-23T05:25:14Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/PublicDataset')]",
				"[concat(variables('workspaceId'), '/datasets/DestinationDataset_nuj')]",
				"[concat(variables('workspaceId'), '/linkedServices/adbhf')]",
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Delta-AutoDemo')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "FromScratch",
						"description": "Reset, make Delta table, and initial load",
						"type": "ExecutePipeline",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Delta-FromScratch",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "Changes",
						"description": "Make Delta changes, stage and merge",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "FromScratch",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Delta-Changes",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "Delta"
				},
				"annotations": [],
				"lastPublishTime": "2022-03-23T04:49:58Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/Delta-FromScratch')]",
				"[concat(variables('workspaceId'), '/pipelines/Delta-Changes')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Delta-Changes')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Target",
						"description": "overwrites staging table in SQL Pools from Serverless",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Delta-Changes",
								"dependencyConditions": [
									"Skipped"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"sink": {
								"type": "SqlDWSink",
								"preCopyScript": "exec DeltaDropTarget",
								"allowPolyBase": true,
								"polyBaseSettings": {
									"rejectValue": 0,
									"rejectType": "value",
									"useTypeDefault": true
								},
								"tableOption": "autoCreate",
								"disableMetricsCollection": false
							},
							"enableStaging": true,
							"stagingSettings": {
								"linkedServiceName": {
									"referenceName": "wplushiramsynapse-WorkspaceDefaultStorage",
									"type": "LinkedServiceReference"
								},
								"path": "wplushiramsynapsefs"
							},
							"translator": {
								"type": "TabularTranslator",
								"mappings": [
									{
										"source": {
											"name": "key",
											"type": "String"
										},
										"sink": {
											"name": "key",
											"type": "String"
										}
									},
									{
										"source": {
											"name": "value",
											"type": "String"
										},
										"sink": {
											"name": "value",
											"type": "String"
										}
									}
								]
							}
						},
						"inputs": [
							{
								"referenceName": "vDeltaTarget",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "LoaderXLTarget",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "Delta-Changes",
						"description": "Includes CDC",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Delta-Changes",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "threetwo",
								"type": "BigDataPoolReference"
							},
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					},
					{
						"name": "LastChanges",
						"description": "overwrites staging table in SQL Pools from Serverless",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Delta-Changes",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "AzureSqlSource",
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"sink": {
								"type": "SqlDWSink",
								"preCopyScript": "exec DeltaDropLastChanges",
								"allowPolyBase": true,
								"polyBaseSettings": {
									"rejectValue": 0,
									"rejectType": "value",
									"useTypeDefault": true
								},
								"tableOption": "autoCreate",
								"disableMetricsCollection": false
							},
							"enableStaging": true,
							"stagingSettings": {
								"linkedServiceName": {
									"referenceName": "wplushiramsynapse-WorkspaceDefaultStorage",
									"type": "LinkedServiceReference"
								},
								"path": "wplushiramsynapsefs"
							},
							"translator": {
								"type": "TabularTranslator",
								"mappings": [
									{
										"source": {
											"name": "key",
											"type": "String"
										},
										"sink": {
											"name": "key",
											"type": "String"
										}
									},
									{
										"source": {
											"name": "time",
											"type": "Int64"
										},
										"sink": {
											"name": "time",
											"type": "Int64"
										}
									},
									{
										"source": {
											"name": "newValue",
											"type": "String"
										},
										"sink": {
											"name": "newValue",
											"type": "String"
										}
									},
									{
										"source": {
											"name": "deleted",
											"type": "Boolean"
										},
										"sink": {
											"name": "deleted",
											"type": "Boolean"
										}
									}
								]
							}
						},
						"inputs": [
							{
								"referenceName": "vDeltaLastChanges",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "LoaderXLLastChanges",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "MergeLatest",
						"description": "",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [
							{
								"activity": "LastChanges",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"storedProcedureName": "[dbo].[MegreLatest]"
						},
						"linkedServiceName": {
							"referenceName": "wplushiramsynapseLoaderXL",
							"type": "LinkedServiceReference"
						}
					},
					{
						"name": "check",
						"type": "Script",
						"dependsOn": [
							{
								"activity": "MergeLatest",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"linkedServiceName": {
							"referenceName": "wplushiramsynapseLoaderXL",
							"type": "LinkedServiceReference"
						},
						"typeProperties": {
							"scripts": [
								{
									"type": "Query",
									"text": "select * from reporttarget"
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"variables": {
					"kvurl": {
						"type": "String",
						"defaultValue": "https://synapseakv.vault.azure.net/"
					}
				},
				"folder": {
					"name": "Delta"
				},
				"annotations": [],
				"lastPublishTime": "2022-03-23T05:25:17Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/vDeltaTarget')]",
				"[concat(variables('workspaceId'), '/datasets/LoaderXLTarget')]",
				"[concat(variables('workspaceId'), '/notebooks/Delta-Changes')]",
				"[concat(variables('workspaceId'), '/bigDataPools/threetwo')]",
				"[concat(variables('workspaceId'), '/datasets/vDeltaLastChanges')]",
				"[concat(variables('workspaceId'), '/datasets/LoaderXLLastChanges')]",
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapseLoaderXL')]",
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Delta-FromScratch')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Reset-delete",
						"description": "clear tables",
						"type": "Script",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"linkedServiceName": {
							"referenceName": "wplushiramsynapseLoaderXL",
							"type": "LinkedServiceReference"
						},
						"typeProperties": {
							"scripts": [
								{
									"type": "Query",
									"text": "delete from target;\ndelete from reporttarget;\ndelete from lastchanges;"
								}
							]
						}
					},
					{
						"name": "DeltaTarget-NewAPI",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Reset-delete",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Delta-FromScratch",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "threetwo",
								"type": "BigDataPoolReference"
							},
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					},
					{
						"name": "FoldTarget",
						"description": "ctas to hash for merge",
						"type": "Script",
						"dependsOn": [
							{
								"activity": "DeltaTarget-NewAPI",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"linkedServiceName": {
							"referenceName": "wplushiramsynapseLoaderXL",
							"type": "LinkedServiceReference"
						},
						"typeProperties": {
							"scripts": [
								{
									"type": "Query",
									"text": "if object_id('reporttarget') is not null drop table [reporttarget];\n\n  --ctas to hash for merge\ncreate table [reporttarget]\n    with (\n        distribution = hash([key]),\n        clustered columnstore index\n    ) as select * from [target];\n"
								}
							]
						}
					},
					{
						"name": "check",
						"type": "Script",
						"dependsOn": [
							{
								"activity": "FoldTarget",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"linkedServiceName": {
							"referenceName": "wplushiramsynapseLoaderXL",
							"type": "LinkedServiceReference"
						},
						"typeProperties": {
							"scripts": [
								{
									"type": "Query",
									"text": "select * from reporttarget"
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "Delta"
				},
				"annotations": [],
				"lastPublishTime": "2022-03-23T04:49:35Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapseLoaderXL')]",
				"[concat(variables('workspaceId'), '/notebooks/Delta-FromScratch')]",
				"[concat(variables('workspaceId'), '/bigDataPools/threetwo')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Delta-ReadLog-ADB')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "ReadDeltaLog",
						"description": "",
						"type": "DatabricksNotebook",
						"dependsOn": [
							{
								"activity": "Get Key from AKV",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Get ConnectionString from AKV",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": true
						},
						"userProperties": [],
						"typeProperties": {
							"notebookPath": "/Repos/hiramfleitas@microsoft.com/hiramdatabricks/Includes/DeltaChanges",
							"baseParameters": {
								"accountName": "wplushiramsynapseadlsv2",
								"accountKey": {
									"value": "@string(activity('Get Key from AKV').output.value)",
									"type": "Expression"
								},
								"containerName": "wplushiramsynapsefs",
								"jdbcConnectionString": {
									"value": "@string(activity('Get ConnectionString from AKV').output.value)",
									"type": "Expression"
								},
								"deltaPath": "synapse/workspaces/wplushiramsynapse/warehouse/lastchanges"
							}
						},
						"linkedServiceName": {
							"referenceName": "adbhf",
							"type": "LinkedServiceReference"
						}
					},
					{
						"name": "Get Key from AKV",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": true,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@concat(pipeline().parameters.KeyVaultDNSName,'secrets/accountkey/?api-version=7.0')",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "GET",
							"headers": {},
							"authentication": {
								"type": "MSI",
								"resource": "https://vault.azure.net"
							}
						}
					},
					{
						"name": "Get ConnectionString from AKV",
						"description": "jdbc",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": true,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@concat(pipeline().parameters.KeyVaultDNSName,'secrets/ConnectionString/?api-version=7.0')",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "GET",
							"headers": {},
							"authentication": {
								"type": "MSI",
								"resource": "https://vault.azure.net"
							}
						}
					},
					{
						"name": "MergeLatest",
						"description": "",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [
							{
								"activity": "ReadDeltaLog",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"storedProcedureName": "[dbo].[MegreLatest]"
						},
						"linkedServiceName": {
							"referenceName": "wplushiramsynapseLoaderXL",
							"type": "LinkedServiceReference"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"KeyVaultDNSName": {
						"type": "string",
						"defaultValue": "https://synapseakv.vault.azure.net/"
					}
				},
				"variables": {
					"KeyVaultDNSName": {
						"type": "String",
						"defaultValue": "https://synapseakv.vault.azure.net/"
					}
				},
				"folder": {
					"name": "Delta"
				},
				"annotations": [],
				"lastPublishTime": "2022-03-23T04:43:03Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/adbhf')]",
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapseLoaderXL')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Delta-ReadLog')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Get Key from AKV",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": true,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@concat(variables('kvurl'),'secrets/accountkey/?api-version=7.0')",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "GET",
							"headers": {},
							"authentication": {
								"type": "MSI",
								"resource": "https://vault.azure.net"
							}
						}
					},
					{
						"name": "Get ConnectionString from AKV",
						"description": "jdbc",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": true,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@concat(variables('kvurl'),'secrets/ConnectionString/?api-version=7.0')",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "GET",
							"headers": {},
							"authentication": {
								"type": "MSI",
								"resource": "https://vault.azure.net"
							}
						}
					},
					{
						"name": "Delta-ReadLog",
						"description": "Includes CDC",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Get ConnectionString from AKV",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "Get Key from AKV",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": true
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Delta-ReadLog",
								"type": "NotebookReference"
							},
							"parameters": {
								"accountName": {
									"value": "wplushiramsynapseadlsv2",
									"type": "string"
								},
								"accountKey": {
									"value": {
										"value": "@string(activity('Get Key from AKV').output.value)",
										"type": "Expression"
									},
									"type": "string"
								},
								"containerName": {
									"value": "wplushiramsynapsefs",
									"type": "string"
								},
								"deltaPath": {
									"value": "synapse/workspaces/wplushiramsynapse/warehouse/lastchanges",
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "threetwo",
								"type": "BigDataPoolReference"
							},
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					},
					{
						"name": "MergeLatest",
						"description": "",
						"type": "SqlServerStoredProcedure",
						"dependsOn": [
							{
								"activity": "Delta-ReadLog",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"storedProcedureName": "[dbo].[MegreLatest]"
						},
						"linkedServiceName": {
							"referenceName": "wplushiramsynapseLoaderXL",
							"type": "LinkedServiceReference"
						}
					},
					{
						"name": "check",
						"type": "Script",
						"dependsOn": [
							{
								"activity": "MergeLatest",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"linkedServiceName": {
							"referenceName": "wplushiramsynapseLoaderXL",
							"type": "LinkedServiceReference"
						},
						"typeProperties": {
							"scripts": [
								{
									"type": "Query",
									"text": "select * from reporttarget"
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"variables": {
					"kvurl": {
						"type": "String",
						"defaultValue": "https://synapseakv.vault.azure.net/"
					}
				},
				"folder": {
					"name": "Delta"
				},
				"annotations": [],
				"lastPublishTime": "2022-03-23T05:25:20Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/notebooks/Delta-ReadLog')]",
				"[concat(variables('workspaceId'), '/bigDataPools/threetwo')]",
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapseLoaderXL')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DevOpsCICD')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "ExposureControl",
				"activities": [
					{
						"name": "FeatureFlag",
						"type": "IfCondition",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@equals(pipeline().parameters.environment, 'disabled')",
								"type": "Expression"
							},
							"ifTrueActivities": [
								{
									"name": "Wait1",
									"type": "Wait",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"waitTimeInSeconds": 1
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"environment": {
						"type": "string",
						"defaultValue": "dev"
					}
				},
				"folder": {
					"name": "_adhoc"
				},
				"annotations": [],
				"lastPublishTime": "2022-03-23T05:25:22Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/E2E_MLFLOW_Sklearn_ADLS')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "E2E_MLFLOW_Sklearn_ADLS",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "E2E_MLFLOW_Sklearn_ADLS",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "_adhoc"
				},
				"annotations": [],
				"lastPublishTime": "2022-03-23T05:25:28Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/E2E_MLFLOW_Sklearn_ADLS')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/GetLabFilesFromHTTP')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "HTTPtoBlob",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BinarySource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "BinaryReadSettings"
								}
							},
							"sink": {
								"type": "BinarySink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "HTTPLabFilesZip",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "blobLabFilesZip",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "M01L05Lab01"
				},
				"annotations": [
					"Workshop"
				],
				"lastPublishTime": "2022-03-22T22:23:44Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/HTTPLabFilesZip')]",
				"[concat(variables('workspaceId'), '/datasets/blobLabFilesZip')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MoveUnzippedFiles')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Wait1",
						"type": "Wait",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"waitTimeInSeconds": {
								"value": "@pipeline().parameters.hiram",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"hiram": {
						"type": "string",
						"defaultValue": "select 10 as wait"
					}
				},
				"folder": {
					"name": "M01L05Lab01"
				},
				"annotations": [],
				"lastPublishTime": "2022-03-22T22:16:33Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Params')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Params",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Params",
								"type": "NotebookReference"
							},
							"parameters": {
								"p1": {
									"value": {
										"value": "@pipeline().parameters.p1",
										"type": "Expression"
									},
									"type": "int"
								}
							},
							"snapshot": true,
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"p1": {
						"type": "int",
						"defaultValue": 3
					}
				},
				"folder": {
					"name": "_adhoc"
				},
				"annotations": [],
				"lastPublishTime": "2022-03-23T05:25:33Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/Params')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Primer')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "IfPausedThenOnline",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "GetStatus",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@not(equals('Online',string(activity('GetStatus').output.properties.status)))",
								"type": "Expression"
							},
							"ifTrueActivities": [
								{
									"name": "Resume",
									"description": "Start",
									"type": "WebActivity",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"url": {
											"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Sql/servers/',pipeline().parameters.Server,'/databases/',pipeline().parameters.DW,'/resume?api-version=2018-06-01-preview')",
											"type": "Expression"
										},
										"method": "POST",
										"headers": {},
										"body": {
											"value": "@string('')",
											"type": "Expression"
										},
										"authentication": {
											"type": "MSI",
											"resource": "https://management.core.windows.net/"
										}
									}
								},
								{
									"name": "ScaleAPI",
									"type": "WebActivity",
									"dependsOn": [
										{
											"activity": "Resume",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"url": {
											"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Sql/servers/',pipeline().parameters.Server,'/databases/',pipeline().parameters.DW,'?api-version=2018-06-01-preview')",
											"type": "Expression"
										},
										"method": "PUT",
										"headers": {
											"Content-Type": "application/json"
										},
										"body": {
											"value": "@concat('{\"location\":\"eastus2\",\"sku\":{\"name\": \"',pipeline().parameters.SLO,'\"}}')",
											"type": "Expression"
										},
										"authentication": {
											"type": "MSI",
											"resource": "https://management.core.windows.net/"
										}
									}
								}
							]
						}
					},
					{
						"name": "GetStatus",
						"description": "Start",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Sql/servers/',pipeline().parameters.Server,'/databases/',pipeline().parameters.DW,'?api-version=2018-06-01-preview')",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "GET",
							"headers": {},
							"body": {
								"value": "@string('')",
								"type": "Expression"
							},
							"authentication": {
								"type": "MSI",
								"resource": "https://management.core.windows.net/"
							}
						}
					},
					{
						"name": "UntilOnline",
						"type": "Until",
						"dependsOn": [
							{
								"activity": "IfPausedThenOnline",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@equals('Online',string(activity('GetStatusOnline').output.properties.status))\n",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "GetStatusOnline",
									"description": "Start",
									"type": "WebActivity",
									"dependsOn": [],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"url": {
											"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Sql/servers/',pipeline().parameters.Server,'/databases/',pipeline().parameters.DW,'?api-version=2018-06-01-preview')",
											"type": "Expression"
										},
										"method": "GET",
										"headers": {},
										"body": {
											"value": "@string('')",
											"type": "Expression"
										},
										"authentication": {
											"type": "MSI",
											"resource": "https://management.core.windows.net/"
										}
									}
								}
							],
							"timeout": "7.00:00:00"
						}
					},
					{
						"name": "execTrollhunters",
						"type": "ExecutePipeline",
						"dependsOn": [
							{
								"activity": "UntilOnline",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"pipeline": {
								"referenceName": "Delta-Changes",
								"type": "PipelineReference"
							},
							"waitOnCompletion": true,
							"parameters": {}
						}
					},
					{
						"name": "IfOnlinePause",
						"description": "Sacle down and Pause",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "GetStatusEnd",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@equals('Online',string(activity('GetStatusEnd').output.properties.status))",
								"type": "Expression"
							},
							"ifTrueActivities": [
								{
									"name": "ScaleWebhook",
									"description": "",
									"type": "WebHook",
									"dependsOn": [],
									"userProperties": [],
									"typeProperties": {
										"url": "https://s1events.azure-automation.net/webhooks?token=EnUcmPAqjmhXx7uKcLIj%2bN%2fNkzod6WPU6CaGc%2bhaPQI%3d",
										"method": "POST",
										"headers": {},
										"body": {
											"WebhookName": "scaledw"
										},
										"timeout": "00:10:00"
									}
								},
								{
									"name": "Pause",
									"description": "Pause",
									"type": "WebActivity",
									"dependsOn": [
										{
											"activity": "ScaleWebhook",
											"dependencyConditions": [
												"Completed"
											]
										}
									],
									"policy": {
										"timeout": "7.00:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"url": {
											"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Sql/servers/',pipeline().parameters.Server,'/databases/',pipeline().parameters.DW,'/pause?api-version=2018-06-01-preview')",
											"type": "Expression"
										},
										"connectVia": {
											"referenceName": "AutoResolveIntegrationRuntime",
											"type": "IntegrationRuntimeReference"
										},
										"method": "POST",
										"headers": {},
										"body": {
											"value": "@string('')",
											"type": "Expression"
										},
										"authentication": {
											"type": "MSI",
											"resource": "https://management.core.windows.net/"
										}
									}
								}
							]
						}
					},
					{
						"name": "GetStatusEnd",
						"description": "Start",
						"type": "WebActivity",
						"dependsOn": [
							{
								"activity": "execTrollhunters",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": {
								"value": "@concat('https://management.azure.com/subscriptions/',pipeline().parameters.SubscriptionID,'/resourceGroups/',pipeline().parameters.ResourceGroup,'/providers/Microsoft.Sql/servers/',pipeline().parameters.Server,'/databases/',pipeline().parameters.DW,'?api-version=2018-06-01-preview')",
								"type": "Expression"
							},
							"method": "GET",
							"headers": {},
							"body": {
								"value": "@string('')",
								"type": "Expression"
							},
							"authentication": {
								"type": "MSI",
								"resource": "https://management.core.windows.net/"
							}
						}
					},
					{
						"name": "Notify",
						"description": "",
						"type": "WebActivity",
						"dependsOn": [
							{
								"activity": "execTrollhunters",
								"dependencyConditions": [
									"Failed"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": "https://prod-23.eastus2.logic.azure.com:443/workflows/a3b71278fcd74578912de49da8f0cc41/triggers/manual/paths/invoke?api-version=2016-10-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=hDohhaBVFqS07vlHEOhum2ZLvhUnvyi1u9oqKBO5fRc",
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"method": "POST",
							"headers": {
								"Content-type": "application/json"
							},
							"body": {
								"value": "@concat('{\n   \"DataFactoryName\": \"',pipeline().DataFactory,'\",\n   \"PipelineName\": \"',pipeline().Pipeline,'\",\n   \"Subject\": \"An error has occurred!\",\n   \"ErrorMessage\": \"The ADF pipeline failed. Please check the activity log.\",\n   \"EmailTo\": \"',pipeline().parameters.Notify,'\"\n}')",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"SubscriptionID": {
						"type": "string",
						"defaultValue": "eaab21d5-8ecd-4ef0-a0c4-92fac2e22875"
					},
					"ResourceGroup": {
						"type": "string",
						"defaultValue": "dw"
					},
					"Server": {
						"type": "string",
						"defaultValue": "hiramdw"
					},
					"DW": {
						"type": "string",
						"defaultValue": "dw"
					},
					"SLO": {
						"type": "string",
						"defaultValue": "DW1000c"
					},
					"Notify": {
						"type": "string",
						"defaultValue": "hiramfleitas@microsoft.com"
					}
				},
				"folder": {
					"name": "_adhoc"
				},
				"annotations": [],
				"lastPublishTime": "2022-03-23T05:25:51Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/pipelines/Delta-Changes')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/UnzipLabFiles')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "UnzipLabFiles",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BinarySource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true
								},
								"formatSettings": {
									"type": "BinaryReadSettings",
									"compressionProperties": {
										"type": "ZipDeflateReadSettings"
									}
								}
							},
							"sink": {
								"type": "BinarySink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings",
									"copyBehavior": "PreserveHierarchy"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "blobLabFilesZip",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "bloblUnzipLabFiles",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "M01L05Lab01"
				},
				"annotations": [
					"workshop"
				],
				"lastPublishTime": "2022-03-22T22:23:01Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/blobLabFilesZip')]",
				"[concat(variables('workspaceId'), '/datasets/bloblUnzipLabFiles')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/copy')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Copy data1",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "LengthOfStay_cooked_small",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "copyLoS2",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "_adhoc"
				},
				"annotations": [],
				"lastPublishTime": "2022-05-10T14:39:56Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/LengthOfStay_cooked_small')]",
				"[concat(variables('workspaceId'), '/datasets/copyLoS2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dominicks_oj_train-RegressionOnnx')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "dominicks_oj_train-RegressionOnnx",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "dominicks_oj_train-RegressionOnnx",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "_adhoc"
				},
				"annotations": [],
				"lastPublishTime": "2022-03-23T05:25:37Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/dominicks_oj_train-RegressionOnnx')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkjob')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "wordcount",
						"type": "SparkJob",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"sparkJob": {
								"referenceName": "wordcount",
								"type": "SparkJobDefinitionReference"
							},
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					},
					{
						"name": "helloworld",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "wordcount",
								"dependencyConditions": [
									"Completed"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "helloworld",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					},
					{
						"name": "Copy Result",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "wordcount",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"wildcardFolderPath": "result",
									"wildcardFileName": "part-*",
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "ParquetSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "ParquetWriteSettings"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "resultwordcounttxt",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "resultwordcountparquet",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"folder": {
					"name": "_adhoc"
				},
				"annotations": [],
				"lastPublishTime": "2022-03-23T05:25:43Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sparkJobDefinitions/wordcount')]",
				"[concat(variables('workspaceId'), '/notebooks/helloworld')]",
				"[concat(variables('workspaceId'), '/datasets/resultwordcounttxt')]",
				"[concat(variables('workspaceId'), '/datasets/resultwordcountparquet')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/wait')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Wait1",
						"type": "Wait",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"waitTimeInSeconds": 15
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"test": {
						"type": "string",
						"defaultValue": "SELECT PATINDEX('%schools%', 'W3Schools.com');"
					}
				},
				"folder": {
					"name": "_adhoc"
				},
				"annotations": [],
				"lastPublishTime": "2022-03-23T05:25:45Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSynapseAnalyticsTable1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "wplushiramsynapseLoaderXL",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {
					"schema": "staging",
					"table": "orcsample"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapseLoaderXL')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Binary1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "wplushiramsynapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "part-00000",
						"folderPath": "result",
						"fileSystem": "wplushiramsynapsefs"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DelimitedText1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "wplushiramsynapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "resultcsv",
						"fileSystem": "wplushiramsynapsefs"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DestinationDataset_nuj')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "wplushiramsynapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "03.02",
						"fileSystem": "dwtemp"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/HTTPLabFilesZip')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "LabFilesZip",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/LabFilesZip')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/HolCSV')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "wplushiramsynapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "user/trusted-service-user/hol.csv",
						"fileSystem": "wplushiramsynapsefs"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LastChanges')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [],
				"typeProperties": {
					"schema": "dbo",
					"table": "lastchanges"
				},
				"sqlPool": {
					"referenceName": "wplussynapsedw",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/wplussynapsedw')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LengthOfStay_cooked_small')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "wplushiramsynapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "LengthOfStay_cooked_small.csv",
						"fileSystem": "wplushiramsynapsefs"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "hematocrit",
						"type": "String"
					},
					{
						"name": "neutrophils",
						"type": "String"
					},
					{
						"name": "sodium",
						"type": "String"
					},
					{
						"name": "glucose",
						"type": "String"
					},
					{
						"name": "bloodureanitro",
						"type": "String"
					},
					{
						"name": "creatinine",
						"type": "String"
					},
					{
						"name": "bmi",
						"type": "String"
					},
					{
						"name": "pulse",
						"type": "String"
					},
					{
						"name": "respiration",
						"type": "String"
					},
					{
						"name": "number_of_issues",
						"type": "String"
					},
					{
						"name": "asthma",
						"type": "String"
					},
					{
						"name": "depress",
						"type": "String"
					},
					{
						"name": "dialysisrenalendstage",
						"type": "String"
					},
					{
						"name": "fibrosisandother",
						"type": "String"
					},
					{
						"name": "gender",
						"type": "String"
					},
					{
						"name": "hemo",
						"type": "String"
					},
					{
						"name": "irondef",
						"type": "String"
					},
					{
						"name": "malnutrition",
						"type": "String"
					},
					{
						"name": "pneum",
						"type": "String"
					},
					{
						"name": "psychologicaldisordermajor",
						"type": "String"
					},
					{
						"name": "psychother",
						"type": "String"
					},
					{
						"name": "secondarydiagnosisnonicd9",
						"type": "String"
					},
					{
						"name": "substancedependence",
						"type": "String"
					},
					{
						"name": "rcount",
						"type": "String"
					},
					{
						"name": "lengthofstay",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LoaderXLLastChanges')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "wplushiramsynapseLoaderXL",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [
					{
						"name": "key",
						"type": "varchar"
					},
					{
						"name": "time",
						"type": "bigint",
						"precision": 19
					},
					{
						"name": "newValue",
						"type": "varchar"
					},
					{
						"name": "deleted",
						"type": "bit"
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "lastchanges"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapseLoaderXL')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LoaderXLTarget')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "wplushiramsynapseLoaderXL",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [
					{
						"name": "key",
						"type": "varchar"
					},
					{
						"name": "value",
						"type": "varchar"
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "ReportTarget"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapseLoaderXL')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/OrcSample')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [],
				"typeProperties": {
					"schema": "staging",
					"table": "orcsample"
				},
				"sqlPool": {
					"referenceName": "wplussynapsedw",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/wplussynapsedw')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Orc_Sample')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "wplushiramsynapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Orc",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "*.*",
						"folderPath": "orc_sample",
						"fileSystem": "wplushiramsynapsefs"
					},
					"enablePhysicalSchema": true
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PublicDataset')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "PublicDataset",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"folderPath": "crime-data-2016",
						"container": "training"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/PublicDataset')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SourceDataset_nuj')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "PublicDataset",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"folderPath": "crime-data-2016",
						"container": "training"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/PublicDataset')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Target')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [],
				"typeProperties": {
					"schema": "dbo",
					"table": "Target"
				},
				"sqlPool": {
					"referenceName": "wplussynapsedw",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/wplussynapsedw')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/blobLabFilesZip')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "wplushiramsynapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "LabFiles.zip",
						"fileSystem": "wplushiramsynapsefs"
					},
					"compression": {
						"type": "ZipDeflate"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bloblUnzipLabFiles')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "wplushiramsynapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileSystem": "nyctaxistaging"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/copyLoS2')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "wplushiramsynapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileSystem": "dwtemp"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dominicks_oj_train')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "wplushiramsynapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "dominicks_OJ_train.csv",
						"fileSystem": "wplushiramsynapsefs"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "WeekStarting",
						"type": "String"
					},
					{
						"name": "Store",
						"type": "String"
					},
					{
						"name": "Brand",
						"type": "String"
					},
					{
						"name": "Quantity",
						"type": "String"
					},
					{
						"name": "Advert",
						"type": "String"
					},
					{
						"name": "Price",
						"type": "String"
					},
					{
						"name": "Age60",
						"type": "String"
					},
					{
						"name": "COLLEGE",
						"type": "String"
					},
					{
						"name": "INCOME",
						"type": "String"
					},
					{
						"name": "Hincome150",
						"type": "String"
					},
					{
						"name": "Large HH",
						"type": "String"
					},
					{
						"name": "Minorities",
						"type": "String"
					},
					{
						"name": "WorkingWoman",
						"type": "String"
					},
					{
						"name": "SSTRDIST",
						"type": "String"
					},
					{
						"name": "SSTRVOL",
						"type": "String"
					},
					{
						"name": "CPDIST5",
						"type": "String"
					},
					{
						"name": "CPWVOL5",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/freeadx')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "freeadx",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureDataExplorerTable",
				"schema": [],
				"typeProperties": {
					"table": "LengthOfStay_cooked_small"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/freeadx')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/moviesCSV')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "PublicDataset",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "moviesDB.csv",
						"folderPath": "SampleData",
						"container": "mycontainer"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "movie",
						"type": "String"
					},
					{
						"name": "title",
						"type": "String"
					},
					{
						"name": "genres",
						"type": "String"
					},
					{
						"name": "year",
						"type": "String"
					},
					{
						"name": "Rating",
						"type": "String"
					},
					{
						"name": "RottenTomato",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/PublicDataset')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/resultwordcountparquet')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "@concat('resultparquet_', string(utcnow()))",
				"linkedServiceName": {
					"referenceName": "wplushiramsynapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Parquet",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "resultparquet",
						"fileSystem": "wplushiramsynapsefs"
					},
					"compressionCodec": "snappy"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/resultwordcounttxt')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "wplushiramsynapse-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "result",
						"fileSystem": "wplushiramsynapsefs"
					},
					"columnDelimiter": "$",
					"escapeChar": "$",
					"quoteChar": "'"
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/wplushiramsynapse-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/vDeltaLastChanges')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SQLOnDemand",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "dbo",
					"table": "vDeltaLastChanges"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SQLOnDemand')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/vDeltaTarget')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "SQLOnDemand",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlTable",
				"schema": [],
				"typeProperties": {
					"schema": "dbo",
					"table": "vDeltaTarget"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/SQLOnDemand')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureKeyVault1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('AzureKeyVault1_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LabFilesZip')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('LabFilesZip_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LaptopFact')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "FileServer",
				"typeProperties": {
					"host": "[parameters('LaptopFact_properties_typeProperties_host')]",
					"userId": "[parameters('LaptopFact_properties_typeProperties_userId')]",
					"password": {
						"type": "SecureString",
						"value": "[parameters('LaptopFact_password')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PublicDataset')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"sasUri": "[parameters('PublicDataset_sasUri')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQLOnDemand')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('SQLOnDemand_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/adbhf')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDatabricks",
				"typeProperties": {
					"domain": "https://adb-1962311870977623.3.azuredatabricks.net",
					"accessToken": {
						"type": "SecureString",
						"value": "[parameters('adbhf_accessToken')]"
					},
					"existingClusterId": "0910-004205-suede164"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/freeadx')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureDataExplorer",
				"typeProperties": {
					"endpoint": "https://kvc43f0ee6600e24ef2b0e.southcentralus.kusto.windows.net",
					"database": "[parameters('freeadx_properties_typeProperties_database')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/iotmonitoringsa10774')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('iotmonitoringsa10774_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nyctaxistaging')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('nyctaxistaging_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/powerapps')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "AzureKeyVault1",
							"type": "LinkedServiceReference"
						},
						"secretName": "[parameters('powerapps_properties_typeProperties_connectionString_secretName')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureKeyVault1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sqldbedgeoulvzyml')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureMLService",
				"typeProperties": {
					"subscriptionId": "[parameters('sqldbedgeoulvzyml_properties_typeProperties_subscriptionId')]",
					"resourceGroupName": "[parameters('sqldbedgeoulvzyml_properties_typeProperties_resourceGroupName')]",
					"mlWorkspaceName": "sqldbedgeoulvzyml",
					"authentication": "MSI"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/wplushiramsynapse-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('wplushiramsynapse-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/wplushiramsynapse-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('wplushiramsynapse-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/wplushiramsynapseLoaderXL')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('wplushiramsynapseLoaderXL_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Daily')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "MoveUnzippedFiles",
							"type": "PipelineReference"
						},
						"parameters": {}
					},
					{
						"pipelineReference": {
							"referenceName": "DevOpsCICD",
							"type": "PipelineReference"
						},
						"parameters": {
							"environment": "[parameters('Daily_properties_DevOpsCICD_parameters_environment')]"
						}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Day",
						"interval": 1,
						"startTime": "2021-07-22T19:58:00Z",
						"endTime": "2021-07-23T19:58:00Z",
						"timeZone": "UTC",
						"schedule": {
							"hours": [
								7
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/MoveUnzippedFiles')]",
				"[concat(variables('workspaceId'), '/pipelines/DevOpsCICD')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Hourly')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Hour",
						"interval": 1,
						"startTime": "2021-09-02T15:00:00",
						"timeZone": "Eastern Standard Time"
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Last Day of Month')]",
			"type": "Microsoft.Synapse/workspaces/triggers",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"runtimeState": "Stopped",
				"pipelines": [
					{
						"pipelineReference": {
							"referenceName": "wait",
							"type": "PipelineReference"
						},
						"parameters": {}
					}
				],
				"type": "ScheduleTrigger",
				"typeProperties": {
					"recurrence": {
						"frequency": "Month",
						"interval": 15,
						"startTime": "2021-07-01T18:53:00Z",
						"timeZone": "UTC",
						"schedule": {
							"minutes": [
								58
							],
							"hours": [
								8
							],
							"monthDays": [
								-1
							]
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/pipelines/wait')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IntegrationRuntime1')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Laptop')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Self-hosted')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "SelfHosted",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AddSam')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "create user [samerc@microsoft.com] from EXTERNAL PROVIDER ",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "samdemo",
						"poolName": "samdemo"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CopyIntoDevs')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.objects O JOIN sys.schemas S ON O.schema_id = S.schema_id WHERE O.NAME = 'devs' AND O.TYPE = 'U' AND S.NAME = 'dbo')\nCREATE TABLE dbo.devs\n\t(\n\t [C1] nvarchar(4000),\n\t [C2] nvarchar(4000)\n\t)\nWITH\n\t(\n\tDISTRIBUTION = ROUND_ROBIN,\n\t CLUSTERED COLUMNSTORE INDEX\n\t -- HEAP\n\t)\nGO\n\n--Uncomment the 4 lines below to create a stored procedure for data pipeline orchestration\n--CREATE PROC bulk_load_devs\n--AS\n--BEGIN\nCOPY INTO dbo.devs\n(C1 1, C2 2)\nFROM 'https://wplushiramsynapseadlsv2.dfs.core.windows.net/wplushiramsynapsefs/devs.csv'\nWITH\n(\n\tFILE_TYPE = 'CSV'\n\t,MAXERRORS = 0\n\t,ERRORFILE = 'https://wplushiramsynapseadlsv2.dfs.core.windows.net/wplushiramsynapsefs/'\n)\n--END\nGO\n\nSELECT TOP 100 * FROM dbo.devs\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create external table with SQL')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "/* Note: this script is filtered on a specific month. You can modify the location to read the entire dataset. */\nIF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat')\n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat]\n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'nyctlc_azureopendatastorage_blob_core_windows_net')\n\tCREATE EXTERNAL DATA SOURCE [nyctlc_azureopendatastorage_blob_core_windows_net]\n\tWITH (\n\t\tLOCATION = 'wasbs://nyctlc@azureopendatastorage.blob.core.windows.net',\n\t\tTYPE     = HADOOP\n\t)\nGO\n\nCREATE EXTERNAL TABLE nyc_tlc_yellow_trip_ext (\n\t[vendorID] varchar(8000),\n\t[tpepPickupDateTime] datetime2(7),\n\t[tpepDropoffDateTime] datetime2(7),\n\t[passengerCount] int,\n\t[tripDistance] float,\n\t[puLocationId] varchar(8000),\n\t[doLocationId] varchar(8000),\n\t[startLon] float,\n\t[startLat] float,\n\t[endLon] float,\n\t[endLat] float,\n\t[rateCodeId] int,\n\t[storeAndFwdFlag] varchar(8000),\n\t[paymentType] varchar(8000),\n\t[fareAmount] float,\n\t[extra] float,\n\t[mtaTax] float,\n\t[improvementSurcharge] varchar(8000),\n\t[tipAmount] float,\n\t[tollsAmount] float,\n\t[totalAmount] float\n\t)\n\tWITH (\n    LOCATION = 'yellow/puYear=2014/puMonth=3/',\n    -- LOCATION = 'yellow'\n\tDATA_SOURCE = [nyctlc_azureopendatastorage_blob_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat],\n\tREJECT_TYPE = VALUE,\n\tREJECT_VALUE = 0\n\t)\nGO\n\nSELECT TOP 100 * FROM nyc_tlc_yellow_trip_ext\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CreateLogins')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "M01L05Lab01"
				},
				"content": {
					"query": "--use master \nCREATE LOGIN DWLoader WITH PASSWORD = 'Str0ng_password';\nCREATE LOGIN dwloaderrc10 WITH PASSWORD = 'Str0ng_password';\n\nCREATE USER DWLoader FOR LOGIN DWLoader;\nCREATE USER dwloaderrc10 FOR LOGIN dwloaderrc10;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CreateUser')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "M01L05Lab01"
				},
				"content": {
					"query": "--use wplussynapsedw\nCREATE USER DWLoader FOR LOGIN DWLoader;\nCREATE USER dwloaderrc10 FOR LOGIN dwloaderrc10;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dateformats')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- string to date\nif object_id('dateformats') is not null drop table dateformats \ngo\n\ncreate table dateformats (\n    id int,\n    createdate varchar(120)\n) with \n(\n    distribution = round_robin,\n    clustered columnstore index\n)\ngo \n\ninsert into dateformats\nselect 1, getutcdate() --implicit \n\nselect * from dateformats\n\nselect convert(datetime,createdate,126) from dateformats\n\n-- back to string\nif object_id('dateformats2') is not null drop table dateformats2 \n\ncreate table dateformats2 with \n(\n    distribution = round_robin,\n    clustered columnstore index\n) as \nselect 1 as id , getutcdate() as createdate\n\nselect convert(varchar(120),createdate,126) from dateformats2\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeltaDropTarget')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"content": {
					"query": "--  1. DeltaDropTarget\nif object_id('DeltaDropTarget') is not null drop proc DeltaDropTarget\ngo \ncreate proc DeltaDropTarget\nas \n    if object_id('target') is not null\n    begin\n        drop table target\n    end \ngo\n\n-- 2. DeltaDropLastChanges\nif object_id('DeltaDropLastChanges') is not null drop proc DeltaDropLastChanges\ngo \ncreate proc DeltaDropLastChanges\nas \n    if object_id('LastChanges') is not null\n    begin\n        drop table LastChanges\n    end \ngo\n\n--3. ShakeNBake\nif object_id('ShakeNBake') is not null drop proc ShakeNBake\ngo \ncreate proc ShakeNBake\nas \n    -- rename object target2hash TO ReportTarget;\n    \n    --ctas to merge\n    if object_id('target2hash') is not null drop table [target2hash];\n    create table [target2hash]\n    with (\n        distribution = hash([key]),\n        clustered columnstore index\n    ) as select * from [target];\n\n    --merge\n    merge into [target2hash] as t\n    using (select * from [lastchanges]) as lc ([key], [time], [newvalue], [deleted])\n    on lc.[key] = t.[key]\n    when matched and lc.[deleted] = 1 \n        then delete\n    when matched\n        then update set t.[key] = lc.[key], t.[value] = lc.[newValue]\n    when not matched and lc.[deleted] = 0 \n        then insert ([key], [value]) values(lc.[key], lc.[newValue]);\n\n    --rename\n    if object_id('ReportTarget') is not null drop table [ReportTarget];\n    rename object target2hash TO ReportTarget;\ngo",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeltaSQLPoolMerge')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"content": {
					"query": "-- Merge statements with a WHEN NOT MATCHED [BY TARGET] clause must target a hash distributed table.\n\nif object_id('target2hash') is not null drop table [target2hash];\n\ncreate table [target2hash]\nwith (\n    distribution = hash([key]),\n    clustered columnstore index\n)\nas\nselect * from [target]\ngo \n\nmerge into [target2hash] as t\nusing (select * from [lastchanges]) as lc ([key], [time], [newvalue], [deleted])\non lc.[key] = t.[key]\nwhen matched and lc.[deleted] = 1 \n    then delete\nwhen matched\n    then update set t.[key] = lc.[key], t.[value] = lc.[newValue]\nwhen not matched and lc.[deleted] = 0 \n    then insert ([key], [value]) values(lc.[key], lc.[newValue]);\n\nselect * from target2hash;\n\n-- remarks: https://docs.microsoft.com/en-us/sql/t-sql/statements/merge-transact-sql?view=azure-sqldw-latest#remarks",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeltaSrvlessViews')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"content": {
					"query": "create credential [https://wplushiramsynapseadlsv2.dfs.core.windows.net/wplushiramsynapsefs] \nwith \n    identity = 'Managed Identity'\ngo \n\nselect * from sys.credentials;\ngo\n\ndrop view if exists vDeltaTarget;\ngo\ncreate view vDeltaTarget\nas\nSELECT [key], [value]\nFROM OPENROWSET(\n    BULK 'abfss://wplushiramsynapsefs@wplushiramsynapseadlsv2.dfs.core.windows.net/synapse/workspaces/wplushiramsynapse/warehouse/target',\n    FORMAT = 'delta') as rows\ngo\n\ndrop view if exists vDeltaLastChanges;\ngo\ncreate view vDeltaLastChanges\nas\nSELECT [key], [time], [newvalue], [deleted]\nFROM OPENROWSET(\n    BULK 'abfss://wplushiramsynapsefs@wplushiramsynapseadlsv2.dfs.core.windows.net/synapse/workspaces/wplushiramsynapse/warehouse/lastchanges',\n    FORMAT = 'delta') as rows \ngo\n\nselect * from vDeltaTarget\nselect * from vDeltaLastChanges\n\n/*\n{\n    \"errorCode\": \"2200\",\n    \"message\": \"Failure happened on 'Source' side. ErrorCode=SqlOperationFailed,\n    'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,\n    Message=A database operation failed with the following error: 'Invalid column name 'value'.',\n    Source=,''Type=System.Data.SqlClient.SqlException,Message=Invalid column name 'value'.,\n    Source=.Net SqlClient Data Provider,SqlErrorNumber=207,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=207,State=1,Message=Invalid column name 'value'.,},],'\",\n    \"failureType\": \"UserError\",\n    \"target\": \"LastChanges\",\n    \"details\": []\n}\n\n{\n    \"errorCode\": \"2200\",\n    \"message\": \"Failure happened on 'Source' side. ErrorCode=SqlOperationFailed,\n    'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,\n    Message=A database operation failed with the following error: 'Invalid column name 'value'.',\n    Source=,''Type=System.Data.SqlClient.SqlException,Message=Invalid column name 'value'.,\n    Source=.Net SqlClient Data Provider,SqlErrorNumber=207,Class=16,ErrorCode=-2146232060,State=1,\n    Errors=[{Class=16,Number=207,State=1,Message=Invalid column name 'value'.,},],'\",\n    \"failureType\": \"UserError\",\n    \"target\": \"LastChanges\",\n    \"details\": []\n}\n*/",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "hiram",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ExternalStuff')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "create database scoped credential wplushiramsynapsefs_cred\nwith \n     identity = 'wplushiramsynapseadlsv2', \n     secret = '' --my storage key, sas not needed\ngo \n\n-- if behind private endpoint, in order to auth FW need to use Service Idendity. Using the key wont work.\n\ncreate database scoped credential msi_cred\nwith \n     identity = 'Managed Identity' -- same as 'Managed Service Identity'\ngo \n\ndrop external data source wplushiramsynapsefs\ngo \n\ncreate external data source wplushiramsynapsefs\nwith (    \n    location = 'abfs://wplushiramsynapsefs@wplushiramsynapseadlsv2.blob.core.windows.net/',\n    credential = msi_cred,\n    type = HADOOP\n)\n\n-- Ref: abfs recommened for big data workloads & adls gen2.\n-- https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction#key-features-of-data-lake-storage-gen2\n\ndrop external file format skipHeader_CSV\ngo \n\ncreate external file format skipHeader_CSV\nwith (FORMAT_TYPE = DELIMITEDTEXT,\n      FORMAT_OPTIONS(\n          FIELD_TERMINATOR = ',',\n          STRING_DELIMITER = '\"',\n          FIRST_ROW = 2, \n          USE_TYPE_DEFAULT = True)\n)\n\nif object_id('extDevs') is not null drop external table extDevs\ngo \n\nCREATE EXTERNAL TABLE extDevs (\n    [first] varchar(50),\n    [last] varchar(50)\n)  \nWITH (\n    LOCATION = 'devs.csv',\n    DATA_SOURCE = wplushiramsynapsefs,  \n    FILE_FORMAT = skipHeader_CSV\n)\nGO\n\nSELECT TOP 10 * FROM extDevs",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/GetRowCounts')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "dw not supported",
				"folder": {
					"name": "Tools"
				},
				"content": {
					"query": "if object_id('GetRowCounts') is not null drop proc GetRowCounts;\ngo \n\ncreate proc GetRowCounts --'SRUSSegmentedPolicyRecords_Final2017Revised, HLRSegmentedPolicyRecords_Final2017Revised, BrockSegmentedPolicyRecords_Final2017Revised'\n    @tables nvarchar(max)\nas \nset nocount on;\n    \n    declare @sql nvarchar(max), @cnt int, @cnt2 int, @name nvarchar(128);\n\n    if object_id('tempdb.dbo.#t') is not null drop table #t\n    create table #t (\n        name        nvarchar(128),\n        rows        char(20),\n        reserved    varchar(18),\n        data        varchar(18),\n        index_size  varchar(18),\n        unused      varchar(18)\n    );\n\n    if object_id('tempdb.dbo.#tables') is not null drop table #tables\n    create table #tables (\n        name        nvarchar(128)\n    );\n    insert into #tables (name) select trim(value) from string_split(@tables,',');\n    \n    select @cnt = count(*) from information_schema.tables where table_name in (select name from #tables);\n    select @cnt2 = count(*) from #t;\n   \n    while @cnt>@cnt2\n    begin\n        select  top 1 @sql='insert into #t exec sp_spaceused '''+table_name+'''', @name=table_name\n        from    information_schema.tables \n        where   table_name in (select top 1 name from #tables);\n        \n        exec sp_executesql @sql; --print @sql;\n        \n        delete from #tables where name = @name;\n        select  @cnt2 = count(*) from #t;\n    end\n    select * from #t\ngo\n\nexec GetRowCounts 'reporttarget'",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LoaderXL')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Tools"
				},
				"content": {
					"query": "--dw master\n\nif exists (select 1 from sys.sql_logins where name='LoaderXL') drop login LoaderXL;\ncreate login LoaderXL with password = 'XL!sqlPools2021'\nalter login LoaderXL enable\ngo \ndrop user LoaderXL;\ngo\ncreate user LoaderXL for login LoaderXL\nalter role dbmanager add member LoaderXL\n\n-- use hiramdw\ndrop user LoaderXL;\ncreate user LoaderXL for login LoaderXL\ngo\nexec sp_addrolemember db_owner, LoaderXL;\nEXEC sp_addrolemember 'xlargerc', 'LoaderXL'\n\ngrant view definition to public;\ngrant showplan to LoaderXL;\ngrant connect to LoaderXL\ngo ",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MegreLatest')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"content": {
					"query": "if object_id('MegreLatest') is not null drop proc [dbo].[MegreLatest]\ngo \n\ncreate proc [dbo].[MegreLatest] \nas \n\n    merge into [ReportTarget] as t\n    using (select * from [lastchanges]) as lc ([key], [time], [newvalue], [deleted])\n    on lc.[key] = t.[key]\n    when matched and lc.[deleted] = 1 \n        then delete\n    when matched\n        then update set t.[key] = lc.[key], t.[value] = lc.[newValue]\n    when not matched and lc.[deleted] = 0 \n        then insert ([key], [value]) values(lc.[key], lc.[newValue]);\n\ngo ",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PDWBackups')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "select   *\nfrom     sys.pdw_loader_backup_runs\norder by run_id desc\n;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/QueryActivity')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Tools"
				},
				"content": {
					"query": "if object_id('QueryActivity') is not null drop proc QueryActivity;\ngo\ncreate proc [dbo].[QueryActivity] AS \n-- + -------------- +\n-- | Query Activity |\n-- + -------------- +\nwith cte as (\n    select      top 100\n                request_id, \n                status, \n                total_elapsed_time/60000 as TotalElapsedMinutes, \n                resource_class, \n                importance\n    from        sys.dm_pdw_exec_requests \n    where       status not in ('Completed','Failed','Cancelled','Suspended')\n                -- and request_id in ('QID3187802','QID3188080')\n    and         session_id <> session_id()\n    order by    submit_time desc\n)\nselect      es.app_name,\n            rs.request_id, \n            rs.step_index, \n            rs.operation_type, \n            rs.distribution_type, \n            rs.location_type, \n            rs.status, \n            rs.error_id, \n            rs.start_time, \n            rs.end_time, \n            case when rs.end_time is not null then cast(rs.end_time - rs.start_time as time) end as duration, \n            rs.total_elapsed_time/60000 as TotalElapsedMinutes, \n            rs.total_elapsed_time/3600000 as TotalElapsedHours, \n            rs.row_count, \n            rs.command, \n            cte.*\nfrom        sys.dm_pdw_request_steps rs\ninner join  cte \n    on rs.request_id = cte.request_id\nleft join  sys.dm_pdw_exec_sessions es \n    on rs.request_id = es.request_id\nwhere       rs.status<>'complete'\norder by    step_index;\n\n--request_id\tstep_index\toperation_type\tdistribution_type\tlocation_type\tstatus\terror_id\tstart_time\tend_time\tduration\tTotalElapsedMinutes\tTotalElapsedHours\trow_count\tcommand\trequest_id\tstatus\tTotalElapsedMinutes\tresource_class\timportance\n--QID827319\t3\tOnOperation\tAllDistributions\tCompute\tRunning\tNULL\t2019-09-05 21:30:07.010\tNULL\tNULL\t199\t3\t-1\tINSERT INTO [sql-eastus2-int-dev-reboot-analytics_wh].[dbo].[Inforce12New] WITH (TABLOCK) ([JOIN_KEY_ACTUALS], [SourceTableName], [BLOCK], [Date_Incurred], [Date_Reported], [Date_Posted], [YRMO], [InforceQuarter], [TLID], [TLID_SecondInsd], [Treaty_Sage_ID], [Treaty_TRS_ID], [Policy_Number], [Policy_Record_Number], [Policy_Rider_Number], [Coverage_Sage_CCID], [ConversionFlag], [OriginalCCID], [OriginalCCID_2], [OriginalPolicy], [OriginalPolicy_2], [FACE], [ReinsuredFace], [NAR], [sourceTableNameInput], [sourceTableNameInputTS], [sourceFileName], [IssueAgeL1], [SmokerL1], [BusinessBlockShort], [mosesBand], [mosesSeries], [mosesClassL1], [GenderL1], [mosesPlan], [IssueDate], [FalloutType1], [Treaty_Model_ID], [Treaty_PeopleSoft_ID], [BusinessBlock], [JOIN_KEY_MODEL], [JOIN_KEY_NO_DATE], [BenefitType])SELECT [T1_1].[JOIN_KEY_ACTUALS], [T1_1].[SourceTableName], [T1_1].[BLOCK], [T1_1].[Date_Incurred], [T1_1].[Date_Reported], [T1_1].[Date_Posted], [T1_1].[YRMO], [T1_1].[InforceQuarter], [T1_1].[TLID], [T1_1].[TLID_SecondInsd], [T1_1].[Treaty_Sage_ID], [T1_1].[Treaty_TRS_ID], [T1_1].[Policy_Number], [T1_1].[Policy_Record_Number], [T1_1].[Policy_Rider_Number], [T1_1].[Coverage_Sage_CCID], [T1_1].[ConversionFlag], [T1_1].[OriginalCCID], [T1_1].[OriginalCCID_2], [T1_1].[OriginalPolicy], [T1_1].[OriginalPolicy_2], [T1_1].[FACE], [T1_1].[ReinsuredFace], [T1_1].[NAR], [T1_1].[sourceTableNameInput], [T1_1].[sourceTableNameInputTS], [T1_1].[sourceFileName], [T1_1].[IssueAgeL1], [T1_1].[SmokerL1], [T1_1].[BusinessBlockShort], [T1_1].[mosesBand], [T1_1].[mosesSeries], [T1_1].[mosesClassL1], [T1_1].[GenderL1], [T1_1].[mosesPlan], [T1_1].[IssueDate], [T1_1].[FalloutType1], [T1_1].[Treaty_Model_ID], [T1_1].[col], [T1_1].[BusinessBlock], [T1_1].[JOIN_KEY_MODEL], [T1_1].[JOIN_KEY_NO_DATE], [T1_1].[BenefitType] FROM (SELECT CASE WHEN ([T2_1].[Treaty_PeopleSoft_ID] LIKE CAST (N'HA%' COLLATE SQL_Latin1_General_CP1_CI_AS AS VARCHAR (3)) COLLATE SQL_Latin1_General_CP1_CI_AS) THEN concat([T2_1].[Treaty_PeopleSoft_ID], CAST (N'-001' COLLATE SQL_Latin1_General_CP1_CI_AS AS VARCHAR (4)) COLLATE SQL_Latin1_General_CP1_CI_AS) ELSE [T2_1].[Treaty_PeopleSoft_ID]END AS [col], [T2_1].[JOIN_KEY_ACTUALS] AS [JOIN_KEY_ACTUALS], [T2_1].[SourceTableName] AS [SourceTableName], [T2_1].[BLOCK] AS [BLOCK], [T2_1].[Date_Incurred] AS [Date_Incurred], [T2_1].[Date_Reported] AS [Date_Reported], [T2_1].[Date_Posted] AS [Date_Posted], [T2_1].[YRMO] AS [YRMO], [T2_1].[InforceQuarter] AS [InforceQuarter], [T2_1].[TLID] AS [TLID], [T2_1].[TLID_SecondInsd] AS [TLID_SecondInsd], [T2_1].[Treaty_Sage_ID] AS [Treaty_Sage_ID], [T2_1].[Treaty_TRS_ID] AS [Treaty_TRS_ID], [T2_1].[Policy_Number] AS [Policy_Number], [T2_1].[Policy_Record_Number] AS [Policy_Record_Number], [T2_1].[Policy_Rider_Number] AS [Policy_Rider_Number], [T2_1].[Coverage_Sage_CCID] AS [Coverage_Sage_CCID], [T2_1].[ConversionFlag] AS [ConversionFlag], [T2_1].[OriginalCCID] AS [OriginalCCID], [T2_1].[OriginalCCID_2] AS [OriginalCCID_2], [T2_1].[OriginalPolicy] AS [OriginalPolicy], [T2_1].[OriginalPolicy_2] AS [OriginalPolicy_2], [T2_1].[FACE] AS [FACE], [T2_1].[ReinsuredFace] AS [ReinsuredFace], [T2_1].[NAR] AS [NAR], [T2_1].[sourceTableNameInput] AS [sourceTableNameInput], [T2_1].[sourceTableNameInputTS] AS [sourceTableNameInputTS], [T2_1].[sourceFileName] AS [sourceFileName], [T2_1].[IssueAgeL1] AS [IssueAgeL1], [T2_1].[SmokerL1] AS [SmokerL1], [T2_1].[BusinessBlockShort] AS [BusinessBlockShort], [T2_1].[mosesBand] AS [mosesBand], [T2_1].[mosesSeries] AS [mosesSeries], [T2_1].[mosesClassL1] AS [mosesClassL1], [T2_1].[GenderL1] AS [GenderL1], [T2_1].[mosesPlan] AS [mosesPlan], [T2_1].[IssueDate] AS [IssueDate], [T2_1].[FalloutType1] AS [FalloutType1], [T2_1].[Treaty_Model_ID] AS [Treaty_Model_ID], [T2_1].[BusinessBlock] AS [BusinessBlock], [T2_1].[JOIN_KEY_MODEL] AS [JOIN_KEY_MODEL], [T2_1].[JOIN_KEY_NO_DATE] AS [JOIN_KEY_NO_DATE], [T2_1].[BenefitType] AS [BenefitType] FROM [sql-eastus2-int-dev-reboot-analytics_w\tQID827319\tRunning\t199\txlargerc\tnormal\n/*\nSELECT * FROM sys.dm_pdw_sql_requests WHERE request_id = 'QID891260' AND status <>'Complete' -- and step_index = 3;\nDBCC PDW_SHOWEXECUTIONPLAN(57, 192);\nSELECT 'DBCC PDW_SHOWEXECUTIONPLAN(',distribution_id,',',spid,')' FROM sys.dm_pdw_sql_requests WHERE request_id = 'QID891260' AND step_index = 3;\n\ndbcc pdw_showspaceused(ctasSuperTableSum)\ndbcc pdw_showspaceused(mapTreatyTermsReplicate)\n*/\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Reset')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"content": {
					"query": "--reset table\nselect * from [target]\ngo \ninsert [target] values('a','0')\ninsert [target] values('b','1')\ninsert [target] values('c','2')\ninsert [target] values('d','3')\ngo  \nselect * from [target];\n\n--set target back to post merged 3rd changes.\ndelete from [target]; \ngo \ninsert [target] values('d','3')\ninsert [target] values('c','200')\ngo\n\n--reset lastchanges\nselect * from lastchanges;\ndelete from [lastchanges];\ninsert [lastchanges] values ('a','1',NULL,1)\ninsert [lastchanges] values ('c','5','200',0)\ninsert [lastchanges] values ('b','2',NULL,1)\ngo\n\n--reset reporttarget\nselect * from [reporttarget];\ndelete from [reporttarget]; \ngo \ninsert [reporttarget] values('a','0')\ninsert [reporttarget] values('b','1')\ninsert [reporttarget] values('c','2')\ninsert [reporttarget] values('d','3')\ngo  \n\nexec mergelatest",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ServerlessOpenRowsetDevs')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- devs.csv\n-- first,last\n-- hiram,fleitas\n-- christina,sosa\n-- brent,carpenetti\n\nSELECT\nTOP100*\nFROM\nOPENROWSET(\nBULK'https://wplushiramsynapseadlsv2.dfs.core.windows.net/wplushiramsynapsefs/devs.csv',\nFORMAT='CSV',\nPARSER_VERSION='2.0'\n    )AS[result]\n\ngo \n\nSELECT\nTOP100*\nFROM\nOPENROWSET(\nBULK'https://wplushiramsynapseadlsv2.dfs.core.windows.net/wplushiramsynapsefs/user/trusted-service-user/hol.csv',\nFORMAT='CSV',\nPARSER_VERSION='2.0'\n    )AS[result]\ngo",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ServerlessVersion')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Tools"
				},
				"content": {
					"query": "select @@servername, @@version",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ShakeNBake')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"content": {
					"query": "if object_id('ShakeNBake') is not null DROP PROCEDURE [dbo].[ShakeNBake]\nGO\n\nCREATE PROC [dbo].[ShakeNBake] AS \n    -- rename object target2hash TO ReportTarget;\n    \n    --ctas to merge\n    if object_id('target2hash') is not null drop table [target2hash];\n    create table [target2hash]\n    with (\n        distribution = hash([key]),\n        clustered columnstore index\n    ) as select * from [target];\n\n    --merge\n    merge into [target2hash] as t\n    using (select * from [lastchanges]) as lc ([key], [time], [newvalue], [deleted])\n    on lc.[key] = t.[key]\n    when matched and lc.[deleted] = 1 \n        then delete\n    when matched\n        then update set t.[key] = lc.[key], t.[value] = lc.[newValue]\n    when not matched and lc.[deleted] = 0 \n        then insert ([key], [value]) values(lc.[key], lc.[newValue]);\n\n    --rename\n    if object_id('ReportTarget') is not null drop table [ReportTarget];\n    rename object target2hash TO ReportTarget;\n    \n\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TrollHunters')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Pixies"
				},
				"content": {
					"query": "--  Troll Hunters: http://trollhunters.wikia.com/wiki/Trollhunters_Wiki , https://en.wikipedia.org/wiki/Trollhunters\n--  :connect localhost\n--  :connect TrollHunters\n--  By: Hiram Fleitas, hiramfleitas@hotmail.com. Special thx to Andy and Dmitri.\ngo\n\n--  +------+\n--  | Lore |\n--  +------+\nif db_id(N',,') is not null --drop/create db \nbegin\n\talter database [,,] set single_user with rollback immediate \n\tdrop database [,,] end\nelse begin \n\tcreate database [,,]\n\ton primary (name='TrollHunters',filename='C:\\Program Files\\Microsoft SQL Server\\MSSQL15.MSSQLSERVER\\MSSQL\\DATA\\TrollHunters.mdf', size=64MB, maxsize=unlimited, filegrowth=64MB) --S:\\SQLDATA\\, C:\\Program Files\\Microsoft SQL Server\\MSSQL13.MSSQLSERVER\\MSSQL\\DATA\\\n\tlog on (name='TrollHunters_log',filename='C:\\Program Files\\Microsoft SQL Server\\MSSQL15.MSSQLSERVER\\MSSQL\\DATA\\TrollHunters_log.ldf', size=8MB, filegrowth=64MB) --L:\\SQLLOGS\\, C:\\Program Files\\Microsoft SQL Server\\MSSQL13.MSSQLSERVER\\MSSQL\\DATA\\\nend\t\ngo\n\nuse [,,]\ngo \ncreate table [Character] ( \n\tCharacterId\t\tint \tnot null,\n    FullName\t\tvarchar(50)\t\t\tnot null,\n\tAffiliation\t\tvarchar(60)\t\t\tnull, \n\tCategory\t\tvarchar(10)\t\t\tnull, \n\tAka\t\t\t\tvarchar(300)\t\tnull, --*\n\t[Status]\t\tvarchar(35)\t\t\tnull,\n\tRace\t\t\tvarchar(50)\t\t\tnull,\n\tAge\t\t\t\tint\t\t\t\t\tnull,\n\tHome\t\t\tvarchar(50)\t\t\tnull,\n\tRelatives\t\tvarchar(300)\t\tnull, --*\n\tWeapons\t\t\tvarchar(100)\t\tnull, --*\n\tEyeColor\t\tvarchar(20)\t\t\tnull,\n\tHairColor\t\tvarchar(50)\t\t\tnull,\n\tMinions\t\t\tvarchar(100)\t\tnull,\n\tVoicedBy\t\tvarchar(50)\t\t\tnull)\n\tconstraint pk_Character primary key clustered (CharacterId asc));\ngo\ncreate table [Locations] ( LocationId int not null, LocationName varchar(50), CharacterId int )\n-- , constraint pk_Locations primary key clustered (LocationId asc));\n--create table Aka (AkaId int identity(1,1) not null, Aka varchar(50), CharacterId int foreign key references [Character](CharacterId), ByCharacterId int foreign key references [Character](CharacterId), constraint pk_Aka primary key clustered (AkaId asc));\n--create table Relatives (RelativeId int identity(1,1) not null, CharacterId int foreign key references [Character](CharacterId), RelativeCharacterId int foreign key references [Character](CharacterId), Relation varchar(50), constraint pk_Relative primary key clustered (RelativeId asc));\n--create table Weapons (WeaponId int identity(1,1) not null, CharacterId int foreign key references [Character](CharacterId), WeaponName varchar(50), [Type] varchar(20), Origin varchar(20), constraint pk_Weapons primary key clustered (WeaponId asc));\n--create table WeaponUser (WeaponUserId int identity(1,1) not null, CharacterId int foreign key references [Character](CharacterId), Occurrance varchar(50), Victims varchar(50), constraint pk_WeaponUser primary key clustered (WeaponUserId asc)); \ngo\ncreate table [Episode] ( \n    CharacterId\t\tint           not null,\n    FullName\t\tnvarchar(64)  not null,\n    VoicedBy\t\tnvarchar(64)  not null,\n\tBirthYear\t\tint\t\t\t  not null, \n\tCategory\t\tnvarchar(64)  not null,\n\t[Status]\t\tnvarchar(64)  not null,\n    Age\t\t\t\tint\t\t\t  not null)\n    constraint PK_Episode primary key clustered (CharacterId asc));\ngo\nset identity_insert [Character] on; --drop table [Character] \n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values \n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (1,'Jim Lake Jr.','Good','Hero','Young Atlas, Jimbo, Master Jim, Fleshbag, Jim \"Fake\" Jr., Jimmy Jam, Little Gynt','Alive','Human',16,'Arcadia Oaks','Jim Lake Sr., Barbara Lake, Claire Nuez','Sword of Daylight, Sword of Eclipse','Blue','Black','Toby Domzalski','Anton Yelchin')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (2,'Claire Maria Nuez','Good (cursed by overpowering the shadow staff)','Hero','C-bomb, Shadowdancer','Alive','Human',15,'Arcadia','Mr. Nuez, Mrs. Nuez, Baby Enrique, Jim Lake Jr.','Shadow Staff','Brown','Black with white and purple streaks','NotEnrique','Lexi Medrano')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (3,'Gunmar the Black','Evil, Gumm-Gumm Army, The Janus Order','Villain','Dark Underlord, Skullcrusher','Alive','Gumm-Gumm',null,'Darklands, Trollmarket','Bular','Decimaar Blade','Blue','Black','Otto Scaarbach, Stricklander, Nomura, Gladys, NotEnrique, Stalklings, Various Changeling Trolls','Clancy Brown')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (4,'Ararghaumont','Gunmar, Heartstone Trollmarket, Trollhunters','Hero','AAARRRGGHH!!!, Wingman','Alive','Krubera Troll',null,'Deep Caverns, Heartstone Trollmarket',null,'Fists and Strength','Green','Dark green',null,'Fred Tatasciore')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (5,'Stricklander','Gunmar, The Janus Order, Bular, Barbara Lake, Jim','Villain','Strikler, Impure, Boss man','Alive','Changeling Troll',null,'Arcadia','Walter Strickler','Knives','Green','Black','NotEnrique, Angor Rot, Antamonstrum, Gladys, Goblins, Fragwa','Jonathan Hyde')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (6,'Angor Rot',null,'Villain',null,'Deceased','Troll',null,null,'his village','Shadow Staff, Creeper Sun Blade, Sword of Daylight','Yellow, White',null,'Pixies','Ike Amadi')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (7,'Zelda Nomura','The Janus Order, Gunmar, Bular, Stricklander','Villain','Impure, Nomura','Alive','Changeling',null,'Arcadia Oaks',null,'Dual-Khopesh','Green','Black',null,'Lauren Tom')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (8,'Draal the Deadly','Good','Hero',null,'Cursed by Gunmar''s Decimer Blade','Troll',null,'Heartstone Trollmarket','Kanjigar','Fists','Yellow and Red',null,null,'Matthew Waterson')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (9,'Morgana','Herself','Villain','Argante, Lady Pale, Eldritch Queen, Baba Yaga','Alive',null,null,'Forests of the Black Sea, Bulgaria',null,'Sorcery',null,'Gray','Angor Rot','Lena Headey')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (10,'Blinkous Galadrigal','Good','Hero','Blinky','Alive','Troll',600,'Heartstone Trollmarket','Dictatious Maximus Galadrigal',null,'Brown','Blue',null,'Kelsey Grammer')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (11,'Bular','Evil','Villain','Son of Gunmar','Deceased','Gumm-Gumm',null,'Darklands','Gunmar','Sword, Horns','Orange',null,'Changeling Trolls, NotEnrique, Nomura, Otto Scaarbach, Gladys, Customs Agent, Stalkling','Ron Perlman')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (12,'Dictatious Maximus Galadrigal','Bad','Villain',null,'Alive','Troll',4999,'Trollmarket, Darklands','Blinky',null,null,null,'Draal the Deadly (possessed by Gunmar), Gumm-Gumm Warriors, Krubera Warriors','Mark Hamill')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (13,'Tobias Domzalski','Good','Hero','Tobes, Toby-Pie, Wingman, T.P., Dumbzalski','Alive','Human',15,'Arcadia','Nana, Mr. Domzalski, Mrs. Domzalski','Warhammer, Glamour Mask','Green','Brown',null,'Charlie Saxton')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (14,'Usurna','Gumm-Gumm','Villain',null,'Alive','Krubera',null,null,null,'Poison Knives',null,null,'Krubera soldiers, Krax','Anjelica Huston')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (15,'Elijah Leslie Pepperjack','Good, Creepslayerz','Hero','Eli Pepperjack','Alive','Human',15,'Arcadia',null,'Mace, shurikens, umbrellas','Green','Black',null,'Cole Sand')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (16,'Steve Palchuk','Neutral, Good, Creepslayerz, The Trollhunters','Ally',null,'Alive','Human',16,'Arcadia',null,'His fists','Brown','Light brown','Various jocks','Steven Yeun')\n\t-- http://trollhunters.wikia.com/wiki/Category:Characters?page=2\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (17,'Vendel','Good','Hero',null,'Deceased','Troll',9000,'Heartstone Trollmarket','Rundle, Kilfred','Staff','White','Grayish brown',null,'Victor Raider-Wexler')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (18,'Kanjigar the Courageous','Good','Hero',null,'Dead','Troll',null,'The Void, The Hero''s Forge', 'Draal','Sword of Daylight','Yellow',null,null,'James Purefoy')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (19,'Merlin','Good',null,null,null,'Wizard',null,'Camelot',null,'Sorcery',null,null,'Trollhunters','David Bradley')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (20,'Jim Lake Sr.',null,null,null,null,'Human',null,null,'Jim Lake Jr., Barbara Lake',null,'Green','Black',null,null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (21,'Otto Scaarbach','Gunmar, The Janus Order','Villain','Otto, Mr. Evilman','Deceased','Changeling',null,null,null,'Polymorph abilities','Blue','Black','Goblins, Lower-ranking Changelings, NotEnrique','Tom Kenny')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (22,'Gladysgro','Gunmar, Bular, The Janus Order, Stricklander','Villain','Gladys','Deceased','Changeling',null,'Arcadia',null,'Dental tools','Blue','Brown',null,'Melanie Paxson')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (23,'Darci Scott','Good','Ally',null,'Alive','Human',15,'Arcadia',null,null,'Brown','Brown','Mary Wang','Yara Shahidi')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (24,'Mary J. Wang','Good','Hero','Maria, Mare','Alive','Human',15,'Arcadia',null,null,'Brown','Black',null,'Lauren Tom')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (25,'Deya the Deliverer','Good','Hero','Deya \"The Deliverer\"','Dead','Troll',null,'The Void, The Hero''s Forge',null,'Sword of Daylight',null,null,null,null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (26,'Unkar the Unfortunate','Good','Hero',null,'Deceased','Troll',null,'The Void, The Hero''s Forge',null,'Sword of Daylight',null,null,null,'Wallace Shawn')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (27,'Barbara Lake','Good','Hero',null,'Alive','Human',null,'Arcadia Oaks','Jim Lake Jr.',null,'Blue','Brown',null,'Amy Landecker')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (28,'Enrique Nuez','Good',null,'Baby Enrique','Alive','Human',1,'Arcadia Oaks, Darklands','Claire Nuez, Mr. Nuez, Mrs. Nuez',null,null,'Blonde',null,null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (29,'Gnome Chompsky','Good','Hero',null,'Alive','Gnome',null,'Hearstone Trollmarket, Toby''s dollhouse','Space Girldoll','Teeth',null,null,null,null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (30,'Waltolemew Strickler','Good',null,'Baby Strickler','Alive','Human',1,'Darklands',null,null,'Green','Brown',null,null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (31,'Goblins','Gumm-Gumm','Villain',null,'Alive','Trolls',null,'Arcadia','Large groups, Teeth',null,null,'Blue, Green',null,null)\n\t-- http://trollhunters.wikia.com/wiki/Category:Characters?page=3\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (32,'Gatto','Neutral',null,null,'Alive','Volcanic Troll',null,'Argentina',null,'Lava, His Stomach',null,null,null,null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (33,'Seor Uhl',null,'Teacher','Uhl the unforgiving','Alive','Human',38,'Arcadia',null,null,null,'Blonde',null,'Fred Tatasciore')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (34,'Shannon Longhannon','Good','Ally',null,'Alive','Human',15,'Arcadia',null,null,'Blue','Brown',null,'Bebe Wood')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (35,'Wumpa','Quagawumps','Ally',null,'Alive','Quagawump Troll',null,'Swamps of Florida','Blungo, Quagawumps','Spears','Green','Green','Quagawumps',null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (36,'Nyarlagroth','Gumm-Gumm','Bad',null,'Deceased','Huge Black Snake',null,'Darklands',null,'Large horns, Thick scales, Teeth, Tongues',null,null,null,null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (37,'Mrs. Nuez',null,null,'Mom','Alive','Human',null,'Arcadia','Claire Nuez, Enrique Nuez, Mr. Nuez','Politics, Soy Sausage','Brown','Black',null,'Andrea Navedo')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (38,'Mr. Nuez',null,null,'Dad','Alive','Human',null,'Arcadia','Claire Nuez, Enrique Nuez, Mrs. Nuez','BBQ','Brown','Black',null,'Tom Kenny')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (39,'Bagdwella','Good',null,null,'Alive','Troll',null,'Heartstone Trollmarket',null,null,'Orange','Red',null,'Fred Tatasciore')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (40,'Coach Lawrence',null,'Teacher','Coach','Alive','Human',56,'Arcadia',null,null,'Blue','Black',null,'Tom Wilson')\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (41,'Gnomes','Thieves',null,'Scum of the earth','Alive','Gnome',null,'Hearstone Trollmarket',null,'Speed, Size, Teeth',null,null,null,null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (42,'The Pixies','Bad',null,null,'Alive','Pixies',null,'Container',null,'Size, Speed, Hallucination',null,null,null,null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (43,'Blood Goblins','Gumm-Gumm','Villain',null,'Alive','Trolls',null,'Draklands, Arcadia','Large groups, Teeth',null,null,'White',null,null)\n\tinsert into [Character] (CharacterId, FullName, Affiliation, Category, Aka, [Status], Race, Age, Home, Relatives, Weapons, EyeColor, HairColor, Minions, VoicedBy) values (44,'Nana','Good',null,null,'Alive','Human',null,'Arcadia','Toby Domzalski','Lack of sight','Blue','Gray',null,null);\nset identity_insert [Character] off;\ngo \nset identity_insert [Locations] on; --drop table [Locations] \n\tinsert into Locations (LocationId, LocationName, CharacterId) values\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(1,'Arcadia Oaks High', 5)\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(2,'Arcadia Oaks High', 40)\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(3,'Arcadia Oaks High', 33)\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(4,'Arcadia Oaks High', 1)\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(5,'Arcadia Oaks High', 13)\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(6,'Arcadia Oaks High', 2)\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(7,'Arcadia Oaks High', 16)\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(8,'Arcadia Oaks High', 15)\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(9,'Arcadia Oaks High', 24)\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(10,'Arcadia Oaks High', 23)\n\tinsert into Locations (LocationId, LocationName, CharacterId) values(11,'Arcadia Oaks High', 34);\nset identity_insert Locations off;\ngo \ninsert\t[Episode] --drop table [Episode] \nselect\tc.CharacterId, FullName, VoicedBy, year(dateadd(year,-age,getdate())), Category, [Status], Age \nfrom\t[Character] c \njoin\t[Locations] l on c.CharacterId=l.CharacterId \nand\t\tc.Race='Human'\nand\t\tl.LocationName='Arcadia Oaks High'\norder by c.CharacterId;\ngo\n\n--  +-----------------------------+------------------------------------------------+\n--  | Angor spies on Trollhunters | Summon SHARK ! Kernel prevents local-to-local. |\n--  +-----------------------------+------------------------------------------------+\n--  :connect TrollHunters\nselect\t'Angor spies on Jim and friends', getdate(), * \nfrom\tdbo.[Character] c \njoin\t[Locations] l on c.CharacterId=l.CharacterId \nand\t\tc.Race='Human'\nand\t\tl.LocationName='Arcadia Oaks High';\ngo\n\n--  +---------------------------------------------+-------------------------------------------------------------------------------+\n--  | Angor releases Pixies, goes in for the kill | Release Pixies c# console app. 1101100000111011000010000001100011100111001000 |\n--  +---------------------------------------------+-------------------------------------------------------------------------------+\n--  :connect Pixes10101\n-- use [,,];\nselect 'Angor releases the Pixies', getdate(), * from dbo.Episode;\ngo\nselect 'Angor goes in for the kill', getdate(), * \nfrom\tdbo.[Character] c \njoin\tLocations l on c.CharacterId=l.CharacterId \nand\t\tc.Race='Human'\nand\t\tl.LocationName='Arcadia Oaks High';\ngo\n\n--  +----------------+\n--  | Toby saves Jim |\n--  +----------------+\n--  Turn on Force Encryption, with Certificate. Restart mssql svc.\n--  no more hallucinations. Jim and Toby kick Angor's butt and save Arcadia.\n--  :connect Pixes10101\ndrop login angor\ngo\n\n--  +----------+\n--  | Lore DDM |\n--  +----------+\n--  :connect localhost\n-- use [,,]\n-- go \n-- add users with select\ndeclare @sql nvarchar(max), @count int\nselect @count = count(*) from [Character] c inner join sysusers su on c.FullName = su.name where Race = 'Human';\nwhile @count < 17\nbegin\n\tselect top 1 @sql = 'create user ['+FullName+'] without login; grant select on [Character] to ['+FullName+'];' \n\tfrom [Character] where FullName not in (select name from sysusers) and Race = 'Human';\n\texec sp_executesql @sql;\n\tselect @count = count(*) from [Character] c inner join sysusers su on c.FullName = su.name where Race = 'Human';\nend\t\ngo\nselect name from sysusers su inner join [Character] c on c.FullName = su.name;\ngo\n--drop users \n/*\nuse [,,]\ngo \ndeclare @sql nvarchar(max), @count int\nselect @count = count(*) from [Character] c inner join sysusers su on c.FullName = su.name where Race = 'Human';\nwhile @count between 1 and 17 \nbegin\n\tselect top 1 @sql = 'drop user if exists ['+FullName+'] ;' from [Character] where FullName in (select name from sysusers) and Race = 'Human';\n\texec sp_executesql @sql;\n\tselect @count = count(*) from [Character] c inner join sysusers su on c.FullName = su.name where Race = 'Human';\nend\t\ngo*/\n-- Mask the Agent column with DDM.\ngo\n\n--  +------------------+\n--  | Killahead Bridge |\n--  +------------------+\nalter table [Character] alter column FullName\tadd masked with (function = 'Partial(0, \"---\", 0)');\nalter table [Character] alter column Aka\t\tadd masked with (function = 'Partial(0, \"---\", 0)');\nalter table [Character] alter column Race\t\tadd masked with (function = 'Partial(0, \"---\", 0)');\nalter table [Character] alter column Age\t\tadd masked with (function = 'default()'); --random(0,0)\nalter table [Character] alter column Relatives\tadd masked with (function = 'Partial(0, \"---\", 0)');\nalter table [Character] alter column EyeColor\tadd masked with (function = 'Partial(0, \"---\", 0)');\nalter table [Character] alter column HairColor\tadd masked with (function = 'Partial(0, \"---\", 0)');\nalter table [Character] alter column Minions\tadd masked with (function = 'Partial(0, \"---\", 0)');\ngo \n\n--  +-------------------------------------------------------+\n--  | For the glory of Merlin, Daylight is mine to command! |\n--  +-------------------------------------------------------+\ngrant unmask to [Jim Lake Jr.];\ngo\n\n--  +-----------------------------+\n--  | Jim becomes the Trollhunter |\n--  +-----------------------------+\nexecute as user = 'Jim Lake Jr.';\n\tselect 'Seen as Jim' as Person, * from [Character]; \nrevert;\ngo\n-- Mom doesn't know.\nexecute as user = 'Barbara Lake';\n\tselect 'Seen as Barbara' as Person, * from [Character]; \nrevert;\ngo\n-- Mom senses something's up.\nexecute as user = 'Barbara Lake';\n\tselect 'Seen as Barbara' as Person, *\n\tfrom [Character]\n\twhere FullName like '%Jim%' \nrevert;\ngo\n\n--  +---------------+\n--  | Mom finds out | --SAFEGUARD: DENY CREATE TABLE TO READ ONLY USERS\n--  +---------------+\ncreate table Letters (\n\tLetter \tvarchar(1) \tnot null)\n\t\ngo --drop table letters\n\ninsert Letters (Letter) values ('A')\ninsert Letters (Letter) values ('B')\ninsert Letters (Letter) values ('C')\ninsert Letters (Letter) values ('D')\ninsert Letters (Letter) values ('E')\ninsert Letters (Letter) values ('F')\ninsert Letters (Letter) values ('G')\ninsert Letters (Letter) values ('H')\ninsert Letters (Letter) values ('I')\ninsert Letters (Letter) values ('J')\ninsert Letters (Letter) values ('K')\ninsert Letters (Letter) values ('L')\ninsert Letters (Letter) values ('M')\ninsert Letters (Letter) values ('N')\ninsert Letters (Letter) values ('')\ninsert Letters (Letter) values ('O')\ninsert Letters (Letter) values ('P')\ninsert Letters (Letter) values('Q')\ninsert Letters (Letter) values ('R')\ninsert Letters (Letter) values ('S')\ninsert Letters (Letter) values('T')\ninsert Letters (Letter) values ('U')\ninsert Letters (Letter) values ('V')\ninsert Letters (Letter) values('W')\ninsert Letters (Letter) values ('X')\ninsert Letters (Letter) values ('Y')\ninsert Letters (Letter) values ('Z')\ninsert Letters (Letter) values(' ')\ninsert Letters (Letter) values ('.')\ninsert Letters (Letter) values ('0')\ninsert Letters (Letter) values ('1')\ninsert Letters (Letter) values ('3');\ngo\ngrant select on Letters to [Barbara Lake]; \ngo\nexecute as user = 'Barbara Lake';\n\tselect \t'Seen as Barbara' as Person\n\t,c.FullName\n\t,L01.Letter as L01\t,L02.Letter as L02\t,L03.Letter as L03\t,L04.Letter as L04\n\t,L05.Letter as L05\t,L06.Letter as L06\t,L07.Letter as L07\t,L08.Letter as L08\n\t,L09.Letter as L09\t,L10.Letter as L10\t,L11.Letter as L11\t,L12.Letter as L12\n\t,L13.Letter as L13\t,L14.Letter as L14\t,L15.Letter as L15\t,L16.Letter as L16\n\t,L17.Letter as L17\t,L18.Letter as L18\t,L19.Letter as L19\t,L20.Letter as L20\n\t,L21.Letter as L21\t,L22.Letter as L22\t,L23.Letter as L23\t,L24.Letter as L24\n\t,L25.Letter as L25\t,L26.Letter as L26\t,L27.Letter as L27\t,L28.Letter as L28\n\t,L29.Letter as L29\n\tfrom \t[Character] c\n\tinner join Letters L01 on (L01.Letter = substring(c.FullName,01,1))\n\tinner join Letters L02 on (L02.Letter = substring(c.FullName,02,1))\n\tinner join Letters L03 on (L03.Letter = substring(c.FullName,03,1))\n\tinner join Letters L04 on (L04.Letter = substring(c.FullName,04,1))\n\tinner join Letters L05 on (L05.Letter = substring(c.FullName,05,1))\n\tinner join Letters L06 on (L06.Letter = substring(c.FullName,06,1))\n\tinner join Letters L07 on (L07.Letter = substring(c.FullName,07,1))\n\tinner join Letters L08 on (L08.Letter = substring(c.FullName,08,1))\n\tinner join Letters L09 on (L09.Letter = substring(c.FullName,09,1))\n\tinner join Letters L10 on (L10.Letter = substring(c.FullName,10,1))\n\tinner join Letters L11 on (L11.Letter = substring(c.FullName,11,1))\n\tinner join Letters L12 on (L12.Letter = substring(c.FullName,12,1))\n\tinner join Letters L13 on (L13.Letter = substring(c.FullName,13,1))\n\tinner join Letters L14 on (L14.Letter = substring(c.FullName,14,1))\n\tinner join Letters L15 on (L15.Letter = substring(c.FullName,15,1))\n\tinner join Letters L16 on (L16.Letter = substring(c.FullName,16,1))\n\tinner join Letters L17 on (L17.Letter = substring(c.FullName,17,1))\n\tinner join Letters L18 on (L18.Letter = substring(c.FullName,18,1))\n\tinner join Letters L19 on (L19.Letter = substring(c.FullName,19,1))\n\tinner join Letters L20 on (L20.Letter = substring(c.FullName,20,1))\n\tinner join Letters L21 on (L21.Letter = substring(c.FullName,21,1))\n\tinner join Letters L22 on (L22.Letter = substring(c.FullName,22,1))\n\tinner join Letters L23 on (L23.Letter = substring(c.FullName,23,1))\n\tinner join Letters L24 on (L24.Letter = substring(c.FullName,24,1))\n\tinner join Letters L25 on (L25.Letter = substring(c.FullName,25,1))\n\tinner join Letters L26 on (L26.Letter = substring(c.FullName,26,1))\n\tinner join Letters L27 on (L27.Letter = substring(c.FullName,27,1))\n\tinner join Letters L28 on (L28.Letter = substring(c.FullName,28,1))\n\tinner join Letters L29 on (L29.Letter = substring(c.FullName,29,1))\nrevert;\ngo\n\n--  +--------------------------+\n--  | Killahead Bridge Dropped | --WE FINISHED DDM HACK.\n--  +--------------------------+\nalter table [Character] alter column FullName\tdrop masked ;\nalter table [Character] alter column Aka\t\tdrop masked ;\nalter table [Character] alter column Race\t\tdrop masked ;\nalter table [Character] alter column Age\t\tdrop masked ;\nalter table [Character] alter column Relatives\tdrop masked ;\nalter table [Character] alter column EyeColor\tdrop masked ;\nalter table [Character] alter column HairColor\tdrop masked ;\nalter table [Character] alter column Minions\tdrop masked ;\ngo \n\n--  +----------+\n--  | Lore RLS |\n--  +----------+\n--  :connect localhost\n--  add users with select. lines 172-181.\ndrop security policy if exists PortalPolicy;\ndrop function if exists Portal.fn_PortalAccess;\ndrop schema if exists Portal;\ndrop view if exists [Humans];\ngo\ncreate view [Humans] as \n\tselect\tFullName, [Status], Age, Home, Relatives, EyeColor, HairColor\n\tfrom\t[Character]\n\twhere\t( FullName = user_name() or Relatives like '%'+user_name()+'%' )\n\tor\t\tuser_name()='Jim Lake Jr.';\ngo\n--  Everyone in Arcadia knows eachother.\ndeclare @sql nvarchar(max)=null, @count int = null;\nselect @count = count(*) from sys.database_permissions where permission_name = 'select' and object_name(major_id) = 'Humans' and user_name(grantee_principal_id) in (select FullName from [Character] where Race = 'Human')\nwhile @count < 17\nbegin\n\tselect top 1 @sql = 'grant select on [Humans] to ['+FullName+'];'\n\tfrom [Character] where FullName in (select name from sysusers) and Race = 'Human' \n\tand FullName not in (select user_name(grantee_principal_id) from sys.database_permissions where permission_name = 'select' and object_name(major_id) = 'Humans');\n\texec sp_executesql @sql;\n\tselect @count = count(*) from sys.database_permissions where permission_name = 'select' and object_name(major_id) = 'Humans' and user_name(grantee_principal_id) in (select FullName from [Character] where Race = 'Human')\nend\t\ngo\nselect\tuser_name(grantee_principal_id) as [User], permission_name, object_name(major_id) as [OnObject] \nfrom\tsys.database_permissions \nwhere\tpermission_name = 'select' \nand\t\tobject_name(major_id) = 'Humans' \nand\t\tuser_name(grantee_principal_id) in (select\tFullName \n\t\t\t\t\t\t\t\t\t\t\tfrom\t[Character] \n\t\t\t\t\t\t\t\t\t\t\twhere\tRace = 'Human')\ngo\n--  Mom doesn't know about the Trolls.\nexecute as user = 'Barbara Lake';\n\tselect 'Seen as Barbara' as Person, * from [Humans]; \nrevert;\ngo\n--  By the glory of Merlin, Daylight is mine to command!\nexecute as user = 'Jim Lake Jr.';\n\tselect 'Seen as Jim' as Person, * from [Humans]; \nrevert;\ngo\n--  Only the heartstone can open the portal to Trollmarkert.\ncreate schema Portal; \ngo\ncreate function Portal.fn_PortalAccess (@FullName as sysname, @Relatives as sysname) \nreturns table with schemabinding as\nreturn\tselect\t1 as PortalAccess \n\t\twhere\t( @FullName = user_name() or @Relatives like '%'+user_name()+'%' )\n\t\tor\t\tuser_name() = 'Jim Lake Jr.';\ngo \ncreate security policy PortalPolicy\nadd filter predicate Portal.fn_PortalAccess (FullName, Relatives) on dbo.[Character] with (state = on);\ngo \n--  Trollhunters check if an RLS enchantment exists.\nselect object_name(object_id) as ObjectName, * from sys.security_policies;\nselect object_name(object_id) as ObjectName, * from sys.security_predicates;\ngo\n--  Mom doesn't know about the Trolls.\nexecute as user = 'Barbara Lake';\n\tselect 'Seen as Barbara' as Person, * from [Character]; \nrevert;\ngo\n--  By the glory of Merlin, Daylight is mine to command!\nexecute as user = 'Jim Lake Jr.';\n\tselect 'Seen as Jim' as Person, * from [Character]; \nrevert;\ngo\n--  Gunmar gets the heartstone.\nalter security policy PortalPolicy with (state = off);\ngo\n--  Trollhunters check if an RLS enchantment exists.\nselect object_name(object_id) as ObjectName, * from sys.security_policies;\nselect object_name(object_id) as ObjectName, * from sys.security_predicates;\ngo\n--  Mom learns the trolls when Gunmar attacked Arcadia.\nexecute as user = 'Barbara Lake';\n\tselect 'Seen as Barbara' as Person, * from [Character]; \nrevert; \ngo\n--  Mom helps the trollhunters save Arcadia from the Gum-Gum army.\nexecute as user = 'Barbara Lake';\n\tselect * from sys.partitions where object_id = object_id('dbo.Character');\nrevert;\ngo\nexecute as user = 'Barbara Lake';\n    select 'Seen as Barbara' as Person, 1 / (Age - 16), * from [Character]; --Divide by zero error encountered.\nrevert;\ngo\n--Jim and the trollhunters fight the gum-gum army.\nalter security policy PortalPolicy with (state = on); --Command(s) completed successfully.\ngo\n--  Mom continues to help Jim save Arcadia.\nexecute as user = 'Barbara Lake';\n    select 'Seen as Barbara' as Person, 1 / (Age - 16), * from [Character]; --Patched error in SQL2016 CUs. SQL2017 RTM still occurs.\nrevert;\ngo\nexecute as user = 'Barbara Lake';\n    select 'Seen as Barbara' as Person, * from [Character]\n\twhere 1 = 1 / (Age - 56); --Divide by zero error encountered. \nrevert; --9000, 4999, 600, 56, 38, 16, 15, 1\ngo\nexecute as user = 'Barbara Lake';\n\texec sp_columns [Character]; \n\texec sp_pkeys [Character]; \n\texec sp_fkeys [Character];\nrevert;\ngo\n\n--  +--------------------------------------------------------------------+\n--  | Barbara can brute-force devide by 0 to identify every integer.     |\n--  | Barbara can use the DDM letters table to identify every character. |\n--  | When all seems lost, our hero...\t\t\t\t\t\t\t\t\t |\n--\t| Jim the Trollhunter has the Amulet Of Daylight!\t\t\t\t\t |\n--  +--------------------------------------------------------------------+",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WhoConnected')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Tools"
				},
				"content": {
					"query": "if object_id('WhoConnected') is not null drop proc WhoConnected;\ngo\ncreate proc WhoConnected \nas\n    select \n        min(cast(login_time as date)) as MinLoginDate, \n        max(cast(login_time as date)) as MaxLoginDate\n    from sys.dm_pdw_exec_sessions \n    where session_id <> session_id()\n\n    select \n        cast(login_time as date) as LoginDate, \n        login_name, \n        client_id, \n        app_name, \n        status\n    from sys.dm_pdw_exec_sessions \n    where session_id <> session_id()\n    group by \n        cast(login_time as date),\n        login_name, \n        client_id, \n        app_name, \n        status\n    order by LoginDate desc\ngo\n\nexec WhoConnected",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WordCountResult')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "--use serverless: default db\n\nSELECT count(*)\nFROM OPENROWSET(\nBULK 'https://wplushiramsynapseadlsv2.dfs.core.windows.net/wplushiramsynapsefs/resultparquet',\nFORMAT = 'parquet') as resultparquet\n\n\n/* fails due to unicode and \nSELECT *\nFROM OPENROWSET(\n    BULK 'https://wplushiramsynapseadlsv2.dfs.core.windows.net/wplushiramsynapsefs/result/part-00000',\n    FORMAT = 'csv',\n    FIELDTERMINATOR ='',\n    PARSER_VERSION='2.0'\n) WITH ( C1 nVARCHAR(100) COLLATE Latin1_General_100_BIN2_UTF8,\n    C2 nVARCHAR(100) COLLATE Latin1_General_100_BIN2_UTF8,\n    C3 nVARCHAR(100) COLLATE Latin1_General_100_BIN2_UTF8   ) as results\n\nSELECT *\nFROM OPENROWSET(\n    BULK 'https://wplushiramsynapseadlsv2.dfs.core.windows.net/wplushiramsynapsefs/resultcsv',\n    FORMAT = 'csv',\n    PARSER_VERSION='2.0',\n    -- FIELDTERMINATOR = '$',\n    FIELDQUOTE = '-',\n    ESCAPECHAR = '$'\n)  as results\n*/",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "default",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkloadMgmt')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Tools"
				},
				"content": {
					"query": "--https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-how-to-convert-resource-classes-workload-groups\n\nSELECT Request_min_resource_grant_percent = Effective_request_min_resource_grant_percent\n  FROM sys.dm_workload_management_workload_groups_stats\n  WHERE name = 'staticrc40'\n\n\nCREATE WORKLOAD GROUP wgDataLoads WITH  \n( REQUEST_MIN_RESOURCE_GRANT_PERCENT = 10\n ,MIN_PERCENTAGE_RESOURCE = 10\n ,CAP_PERCENTAGE_RESOURCE = 40\n ,QUERY_EXECUTION_TIMEOUT_SEC = 3600)\n\n\nCREATE WORKLOAD CLASSIFIER wcDataLoads WITH  \n( WORKLOAD_GROUP = 'wgDataLoads'\n ,MEMBERNAME = 'AdfLogin'\n ,WLM_LABEL = 'factloads')\n\n\n--Test as sql login\nSELECT SUSER_SNAME() --should be 'AdfLogin'\n\n--change to a valid table AdfLogin has access to\nSELECT TOP 10 *\n  FROM nation\n  OPTION (label='factloads')\n\nSELECT request_id, [label], classifier_name, group_name, command\n  FROM sys.dm_pdw_exec_requests\n  WHERE [label] = 'factloads'\n  ORDER BY submit_time DESC",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/alterdw')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Tools"
				},
				"content": {
					"query": "-- use master\n\nalter database wplussynapsedw set READ_COMMITTED_SNAPSHOT on with NO_WAIT\nalter database wplussynapsedw set RESULT_SET_CACHING on with NO_WAIT   ",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dominicks_OJ_train')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) \n [WeekStarting]\n,[Store]\n,[Brand]\n,[Quantity]\n,[Advert]\n,[Price]\n,[Age60]\n,[COLLEGE]\n,[INCOME]\n,[Hincome150]\n,[Large_HH]\n,[Minorities]\n,[WorkingWoman]\n,[SSTRDIST]\n,[SSTRVOL]\n,[CPDIST5]\n,[CPWVOL5]\n FROM [default].[dbo].[dominicks_oj_train]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "default",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ext_dominicks_OJ_train')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "create schema ext\ngo \n\nselect * from sys.database_scoped_credentials\ngo\nselect * from sys.external_data_sources\ngo \nselect * from sys.external_file_formats\ngo\n\ncreate external table ext.dominicks_OJ_train\n(\n\t[WeekStarting] date,\n\t[Store] int,\n\t[Brand] nvarchar(4000),\n\t[Quantity] bigint,\n\t[Advert] bit,\n\t[Price] float,\n\t[Age60] float,\n\t[COLLEGE] float,\n\t[INCOME] float,\n\t[Hincome150] float,\n\t[Large HH] float,\n\t[Minorities] float,\n\t[WorkingWoman] float,\n\t[SSTRDIST] float,\n\t[SSTRVOL] float,\n\t[CPDIST5] float,\n\t[CPWVOL5] float\n)\nWITH\n(\n\tLOCATION = 'dominicks_OJ_train.csv',\n\tDATA_SOURCE = wplushiramsynapsefs,\n\tFILE_FORMAT = skipHeader_CSV\n)\n\nselect count(*) from  ext.dominicks_OJ_train\nSELECT TOP 100 * FROM ext.dominicks_OJ_train\nGO\n\ncreate table dominicks_OJ_train \nwith (distribution = Round_Robin)\nas select * from ext.dominicks_OJ_train \n\nselect count(*) from dominicks_OJ_train",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/fwrules')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Tools"
				},
				"content": {
					"query": "-- DW / Dedicated SQL Pool\n-- use master\n\nselect * from master.sys.firewall_rules;\n\nexec sp_delete_firewall_rule 'AllowAllWindowsAzureIps';  ",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/helloworld')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "select 'hello world'",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sp_spaceused_all')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "dw not supported",
				"folder": {
					"name": "Tools"
				},
				"content": {
					"query": "if object_id('sp_spaceused_all') is not null drop proc sp_spaceused_all;\ngo\n\ncreate procedure sp_spaceused_all\n    @SourceDB nvarchar(max)\nas\nset nocount on\n\ndeclare @sql nvarchar(max)\n    create table #tables(name varchar(128))\n    \n    select @sql = 'insert #tables select TABLE_NAME from ' + @SourceDB + '.INFORMATION_SCHEMA.TABLES where TABLE_TYPE = ''BASE TABLE'''\n    exec (@sql)\n    \n    create table #SpaceUsed (name varchar(128), rows varchar(11), reserved varchar(18), data varchar(18), index_size varchar(18), unused varchar(18))\n    declare @name varchar(128)\n    select @name = ''\n    while exists (select * from #tables where name > @name)\n    begin\n        select @name = min(name) from #tables where name > @name\n        select @sql = N'exec ' + @SourceDB + N'..sp_executesql N''insert #SpaceUsed exec sp_spaceused ' + @name + ''''\n        exec (@sql)\n    end\n    select * from #SpaceUsed\n    drop table #tables\n    drop table #SpaceUsed\ngo\n\nexec sp_spaceused_all 'wplussynapsedw'",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/vTableSizes')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Tools"
				},
				"content": {
					"query": "if object_id('vTableSizes') is not null drop view vTableSizes;\ngo\nCREATE VIEW dbo.vTableSizes\nAS\nWITH base AS (\n    --https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-tables-overview\n    SELECT \n    GETDATE()                                                             AS  [execution_time]\n    , DB_NAME()                                                            AS  [database_name]\n    , s.name                                                               AS  [schema_name]\n    , t.name                                                               AS  [table_name]\n    , QUOTENAME(s.name)+'.'+QUOTENAME(t.name)                              AS  [two_part_name]\n    , nt.[name]                                                            AS  [node_table_name]\n    , ROW_NUMBER() OVER(PARTITION BY nt.[name] ORDER BY (SELECT NULL))     AS  [node_table_name_seq]\n    , tp.[distribution_policy_desc]                                        AS  [distribution_policy_name]\n    , c.[name]                                                             AS  [distribution_column]\n    , nt.[distribution_id]                                                 AS  [distribution_id]\n    , i.[type]                                                             AS  [index_type]\n    , i.[type_desc]                                                        AS  [index_type_desc]\n    , nt.[pdw_node_id]                                                     AS  [pdw_node_id]\n    , pn.[type]                                                            AS  [pdw_node_type]\n    , pn.[name]                                                            AS  [pdw_node_name]\n    , di.name                                                              AS  [dist_name]\n    , di.position                                                          AS  [dist_position]\n    , nps.[partition_number]                                               AS  [partition_nmbr]\n    , nps.[reserved_page_count]                                            AS  [reserved_space_page_count]\n    , nps.[reserved_page_count] - nps.[used_page_count]                    AS  [unused_space_page_count]\n    , nps.[in_row_data_page_count] \n        + nps.[row_overflow_used_page_count] \n        + nps.[lob_used_page_count]                                        AS  [data_space_page_count]\n    , nps.[reserved_page_count] \n    - (nps.[reserved_page_count] - nps.[used_page_count]) \n    - ([in_row_data_page_count] \n            + [row_overflow_used_page_count]+[lob_used_page_count])       AS  [index_space_page_count]\n    , nps.[row_count]                                                      AS  [row_count]\n    from sys.schemas s\n    INNER JOIN sys.tables t\n        ON s.[schema_id] = t.[schema_id]\n    INNER JOIN sys.indexes i\n        ON  t.[object_id] = i.[object_id]\n        AND i.[index_id] <= 1\n    INNER JOIN sys.pdw_table_distribution_properties tp\n        ON t.[object_id] = tp.[object_id]\n    INNER JOIN sys.pdw_table_mappings tm\n        ON t.[object_id] = tm.[object_id]\n    INNER JOIN sys.pdw_nodes_tables nt\n        ON tm.[physical_name] = nt.[name]\n    INNER JOIN sys.dm_pdw_nodes pn\n        ON  nt.[pdw_node_id] = pn.[pdw_node_id]\n    INNER JOIN sys.pdw_distributions di\n        ON  nt.[distribution_id] = di.[distribution_id]\n    INNER JOIN sys.dm_pdw_nodes_db_partition_stats nps\n        ON nt.[object_id] = nps.[object_id]\n        AND nt.[pdw_node_id] = nps.[pdw_node_id]\n        AND nt.[distribution_id] = nps.[distribution_id]\n    LEFT OUTER JOIN (select * from sys.pdw_column_distribution_properties where distribution_ordinal = 1) cdp\n        ON t.[object_id] = cdp.[object_id]\n    LEFT OUTER JOIN sys.columns c\n        ON cdp.[object_id] = c.[object_id]\n        AND cdp.[column_id] = c.[column_id]\n), size AS (\n    SELECT\n    [execution_time]\n    ,  [database_name]\n    ,  [schema_name]\n    ,  [table_name]\n    ,  [two_part_name]\n    ,  [node_table_name]\n    ,  [node_table_name_seq]\n    ,  [distribution_policy_name]\n    ,  [distribution_column]\n    ,  [distribution_id]\n    ,  [index_type]\n    ,  [index_type_desc]\n    ,  [pdw_node_id]\n    ,  [pdw_node_type]\n    ,  [pdw_node_name]\n    ,  [dist_name]\n    ,  [dist_position]\n    ,  [partition_nmbr]\n    ,  [reserved_space_page_count]\n    ,  [unused_space_page_count]\n    ,  [data_space_page_count]\n    ,  [index_space_page_count]\n    ,  [row_count]\n    ,  ([reserved_space_page_count] * 8.0)                                 AS [reserved_space_KB]\n    ,  ([reserved_space_page_count] * 8.0)/1000                            AS [reserved_space_MB]\n    ,  ([reserved_space_page_count] * 8.0)/1000000                         AS [reserved_space_GB]\n    ,  ([reserved_space_page_count] * 8.0)/1000000000                      AS [reserved_space_TB]\n    ,  ([unused_space_page_count]   * 8.0)                                 AS [unused_space_KB]\n    ,  ([unused_space_page_count]   * 8.0)/1000                            AS [unused_space_MB]\n    ,  ([unused_space_page_count]   * 8.0)/1000000                         AS [unused_space_GB]\n    ,  ([unused_space_page_count]   * 8.0)/1000000000                      AS [unused_space_TB]\n    ,  ([data_space_page_count]     * 8.0)                                 AS [data_space_KB]\n    ,  ([data_space_page_count]     * 8.0)/1000                            AS [data_space_MB]\n    ,  ([data_space_page_count]     * 8.0)/1000000                         AS [data_space_GB]\n    ,  ([data_space_page_count]     * 8.0)/1000000000                      AS [data_space_TB]\n    ,  ([index_space_page_count]  * 8.0)                                   AS [index_space_KB]\n    ,  ([index_space_page_count]  * 8.0)/1000                              AS [index_space_MB]\n    ,  ([index_space_page_count]  * 8.0)/1000000                           AS [index_space_GB]\n    ,  ([index_space_page_count]  * 8.0)/1000000000                        AS [index_space_TB]\n    FROM base \n)\nSELECT * FROM size;\ngo \n\nSELECT \n     distribution_policy_name, table_name\n,    SUM(row_count)                as table_type_row_count\n,    SUM(reserved_space_GB)        as table_type_reserved_space_GB\n,    SUM(data_space_GB)            as table_type_data_space_GB\n,    SUM(index_space_GB)           as table_type_index_space_GB\n,    SUM(unused_space_GB)          as table_type_unused_space_GB\nFROM dbo.vTableSizes\nGROUP BY distribution_policy_name, table_name\norder by table_type_data_space_GB desc ;\n\ngo \nselect * from dbo.vTableSizes\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "wplussynapsedw",
						"poolName": "wplussynapsedw"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/01-CrossClusterDB')]",
			"type": "Microsoft.Synapse/workspaces/kqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "cluster(\"help\").database(\"Samples\").StormEvents\n| count",
					"metadata": {
						"language": "kql"
					},
					"currentConnection": {
						"poolName": "codesmall1",
						"databaseName": "db1"
					}
				},
				"type": "KqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/02-configDB')]",
			"type": "Microsoft.Synapse/workspaces/kqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": ".create table ['StageIoTRawData'] (['deviceId']:string, ['enqueuedTime']:datetime, ['messageProperties']:dynamic, ['messageSource']:string, ['telemetry']:dynamic, ['templateId']:string, ['schema']:string)\n\n.alter table StageIoTRawData policy ingestionbatching @'{\"MaximumBatchingTimeSpan\":\"00:00:30\"}'\n\n.create table ['StageIoTRawData'] ingestion json mapping 'StageIoTRawData_mapping' '[{\"column\":\"deviceId\", \"Properties\":{\"Path\":\"$[\\'deviceId\\']\"}},{\"column\":\"enqueuedTime\", \"Properties\":{\"Path\":\"$[\\'enqueuedTime\\']\"}},{\"column\":\"messageProperties\", \"Properties\":{\"Path\":\"$[\\'messageProperties\\']\"}},{\"column\":\"messageSource\", \"Properties\":{\"Path\":\"$[\\'messageSource\\']\"}},{\"column\":\"telemetry\", \"Properties\":{\"Path\":\"$[\\'telemetry\\']\"}},{\"column\":\"templateId\", \"Properties\":{\"Path\":\"$[\\'templateId\\']\"}},{\"column\":\"schema\", \"Properties\":{\"Path\":\"$[\\'schema\\']\"}}]'\n\n.create table Thermostats (EnqueuedTimeUTC: datetime, DeviceId: string, BatteryLevel: long, Temp: real, Humidity: real) \n\n.create table ['Thermostats'] ingestion csv mapping 'Thermostats_mapping' '[{\"column\":\"EnqueuedTimeUTC\", \"Properties\":{\"Ordinal\":\"0\"}},{\"column\":\"DeviceId\", \"Properties\":{\"Ordinal\":\"1\"}},{\"column\":\"BatteryLevel\", \"Properties\":{\"Ordinal\":\"2\"}},{\"column\":\"Temp\", \"Properties\":{\"Ordinal\":\"3\"}},{\"column\":\"Humidity\", \"Properties\":{\"Ordinal\":\"4\"}}]'\n\n.create-or-alter function with (docstring = \"Used for Thermostats Update Policy\",folder = \"Functions\") ExtractThermostatData {\n\tStageIoTRawData\n\t| where telemetry has 'temp'\n\t| project \n\tEnqueuedTimeUTC=enqueuedTime,\n\tDeviceId=deviceId,\n\tBatteryLevel = tolong(telemetry.['BatteryLevel']), \n\tTemp =  toreal(telemetry.['temp']),\n\tHumidity =  toreal(telemetry.['humidity'])\n}\n\n.create-or-alter function with (folder = \"Analytics/IoT\", skipvalidation = \"true\") GetDevicesbyStore(Office:string) {\n\tlet ADTendpoint = \"https://<dtURI>\";\n\tlet ADTquery = strcat(\"SELECT T.$dtId as Office, F.$dtId as Floor, D.$dtId as DeviceId FROM DIGITALTWINS T JOIN F RELATED T.officecontainsfloors JOIN D RELATED F.floorcontainsdevices where T.$dtId='\", Office, \"'\"); \n\tevaluate azure_digital_twins_query_request(ADTendpoint, ADTquery)\n    | project Office=tostring(Office), Floor=tostring(Floor), DeviceId=tostring(DeviceId)\n} \n\n// .create-or-alter function with (folder = \"Analytics/IoT\", skipvalidation = \"true\") GetDevicesbyStoreFloor(Office:string, Floor:string) {\n// \tlet ADTendpoint = \"https://<dtURI>\";\n// \tlet ADTquery = strcat(\"SELECT T.$dtId as Office, F.$dtId as Floor, D.$dtId as DeviceId FROM DIGITALTWINS T JOIN F RELATED T.officecontainsfloors JOIN D RELATED F.floorcontainsdevices where T.$dtId='\", Office, \"' AND F.$dtId = '\", Floor, \"'\"); \n// \tevaluate azure_digital_twins_query_request(ADTendpoint, ADTquery)\n//     | project Office=tostring(Office), Floor=tostring(Floor), DeviceId=tostring(DeviceId)\n// }\n\n.alter table Thermostats policy update \n@'[{ \"IsEnabled\": true, \"Source\": \"StageIoTRawData\", \"Query\": \"ExtractThermostatData()\", \"IsTransactional\": false, \"PropagateIngestionProperties\": false}]'\n\n",
					"metadata": {
						"language": "kql"
					},
					"currentConnection": {
						"poolName": "codesmall1",
						"databaseName": "db1"
					}
				},
				"type": "KqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/03-Thermostat')]",
			"type": "Microsoft.Synapse/workspaces/kqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "//setup data ingestion via portal\n\nStageIoTRawData | count \n\nStageIoTRawData | sample 10\n\n\nStageIoTRawData\n| project enqueuedTime, deviceId,\nBatteryLevel = tolong(telemetry.['BatteryLevel']), \nTemp =  toreal(telemetry.['temp']),\nHumidity =  toreal(telemetry.['humidity'])\n\n\n.create-or-alter function  ExtractThermostatData {\n//same here...\n}\n\n\nThermostats | sample 10\n\n//count of rows in table\nThermostats | count \n\n\n//What is the average temp every 1 min?\nThermostats\n| where EnqueuedTimeUTC > ago(7d)\n| where DeviceId == 'dd5f1451-9e9a-43a6-ad02-32bed6877f20'\n| summarize avg(Temp) by bin(EnqueuedTimeUTC,1m)\n| render timechart \n\n\n//Is there any missing data? \n//make-series\n//Create series of specified aggregated values along specified axis.\nThermostats\n| where EnqueuedTimeUTC > ago(6h)\n| where DeviceId == 'dd5f1451-9e9a-43a6-ad02-32bed6877f20'\n| make-series AvgTemp=avg(Temp) on EnqueuedTimeUTC from ago(6h) to now() step 1m   \n| render timechart \n\n\n//How can I fill the missing values?\n//series_fill_linear()\n//Performs linear interpolation of missing values in a series.\nThermostats\n| where EnqueuedTimeUTC > ago(6h)\n| where DeviceId == 'dd5f1451-9e9a-43a6-ad02-32bed6877f20'\n| make-series AvgTemp=avg(Temp) default=real(null) on EnqueuedTimeUTC from ago(6h) to now() step 1m   \n| extend NoGapsTemp=series_fill_linear(AvgTemp)\n| project EnqueuedTimeUTC, NoGapsTemp\n| render timechart \n\n\n\n//What will be the temprature for next one hour?\nThermostats\n| where EnqueuedTimeUTC > ago(2d)\n| where DeviceId == 'dd5f1451-9e9a-43a6-ad02-32bed6877f20'\n| make-series AvgTemp=avg(Temp) default=real(null) on EnqueuedTimeUTC from ago(2d) to now()+15m step 1m   \n| extend NoGapsTemp=series_fill_linear(AvgTemp)\n| project EnqueuedTimeUTC, NoGapsTemp\n| extend forecast = series_decompose_forecast(NoGapsTemp, 15)\n| render timechart with(title='Forecasting the next 15min by Time Series Decmposition')\n\n\n//Are there any anomalies for this device?\nThermostats\n| where EnqueuedTimeUTC > ago(3d)\n| where DeviceId == 'dd5f1451-9e9a-43a6-ad02-32bed6877f20'\n| make-series AvgTemp=avg(Temp) default=real(null) on EnqueuedTimeUTC from ago(3d) to now() step 1m   \n| extend NoGapsTemp=series_fill_linear(AvgTemp)\n| project EnqueuedTimeUTC, NoGapsTemp\n| extend anomalies = series_decompose_anomalies(NoGapsTemp,1) \n| render anomalychart with(anomalycolumns=anomalies)\n\n\n//What the anomalies I should focus on across all devices?\nThermostats\n| where EnqueuedTimeUTC > ago(3d)\n| make-series AvgTemp=avg(Temp) default=real(null) on EnqueuedTimeUTC from ago(3d) to now() step 1m by DeviceId\n| extend NoGapsTemp=series_fill_linear(AvgTemp)\n| project EnqueuedTimeUTC, DeviceId, NoGapsTemp\n| extend anomalies = series_decompose_anomalies(NoGapsTemp, 1)\n| mv-expand EnqueuedTimeUTC, anomalies, NoGapsTemp\n| where anomalies == 1\n\n//10 devices\nThermostats\n| summarize count(DeviceId) by DeviceId\n| count\n\n//Hilo & Axceta\n//30days of all devices in \nThermostats\n| where ingestion_time() > ago(30d)\n| project EnqueuedTimeUTC, ingestion_time(), latency=(ingestion_time()-EnqueuedTimeUTC) //get latency\n| summarize latency=percentile(latency, 95) by timewindow=bin(ingestion_time(), 15m) //get percentile by buckets\n| summarize timewindow=make_list(timewindow), latency=make_list(latency), anomaly=series_decompose_anomalies(make_list(latency)) //get anomalies\n| project timewindow, latency, anomaly //keep needed columns for chart\n| render anomalychart with(anomalycolumns = anomaly)\n\n.show materialized-views\n\n.create-or-alter materialized-view Hourly_Average_Mview on table Thermostats {\nThermostats | summarize avg_Battery_Level=avg(BatteryLevel),avg_Temp=avg(Temp), avg_Humidity=avg(Humidity) by DeviceId, bin(EnqueuedTimeUTC,1h)\n}\n\n.create-or-alter materialized-view Current_Mview on table Thermostats {\nThermostats | summarize (curr_Event_Time,curr_Battery_Level,curr_Temp, curr_Humidity)=arg_max(EnqueuedTimeUTC,BatteryLevel,Temp,Humidity) by DeviceId\n}\n\n//Materialized views\nHourly_Average_Mview\n| where EnqueuedTimeUTC > ago(1h)\n| take 1000 \n\nCurrent_Mview\n\n\n.show external tables\n\n\n\nexternal_table(\"ext_Thermostats\")\n| where EnqueuedTimeUTC between (datetime('03-01-2020 11:00 am') .. datetime('03-01-2020 01:00 pm')) and DeviceId == '637086754472373714'\n| project EnqueuedTimeUTC, BatteryLevel, Temp, Humidity",
					"metadata": {
						"language": "kql"
					},
					"currentConnection": {
						"poolName": "codesmall1",
						"databaseName": "db1"
					}
				},
				"type": "KqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/04-Formula1')]",
			"type": "Microsoft.Synapse/workspaces/kqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": ".show tables \n\nCarTelemetry\n| limit 10 \n\nEXPLAIN\nSELECT * FROM [CarTelemetry]\n\nSELECT count(*) FROM CarTelemetry where Timestamp > '5/23/2022';\n\n\nParticipant\n| limit 100 \n\n//make Participant a DW dimension. AdventureWorks racing team\n\n#connect cluster('adxpm10774.eastus').database(IoTAnalytics)\n\n//sqlmi\nevaluate sql_request(\n  'Server=tcp:oceanmi.public.ddc9e22985bb.database.windows.net,3342;'\n    'Authentication=\"Active Directory Integrated\";'\n    'Initial Catalog=OceanMI;',\n  'select * from [dbo].[facttable]')\n| where id > 0\n| project value\n\n// mix kql telemetry with DW/Lakedb or in PBI model. -> stretch. wow. all-up synapse. \n// viz in PBI\n// ai model train/score/ops. 2nd presentation.\n\n\n//cross cluster query-ingest\n.set-or-replace dxpooltable1 <| cluster('help').database('Samples').demo_make_series1\n\n.drop table dxpooltable1 ifexists ",
					"metadata": {
						"language": "kql"
					},
					"currentConnection": {
						"poolName": "formula1",
						"databaseName": "f1"
					}
				},
				"type": "KqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/wordcount')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.2",
				"language": "python",
				"jobProperties": {
					"name": "wordcount",
					"file": "abfss://wplushiramsynapsefs@wplushiramsynapseadlsv2.dfs.core.windows.net/wordcount.py",
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d8640e8c-77b1-4624-90bd-ee197dbca54f"
					},
					"args": [
						"abfss://wplushiramsynapsefs@wplushiramsynapseadlsv2.dfs.core.windows.net/shakespeare.txt",
						"abfss://wplushiramsynapsefs@wplushiramsynapseadlsv2.dfs.core.windows.net/result"
					],
					"jars": [],
					"pyFiles": [],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Delta-Changes')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9fd9c382-7255-441f-8fd3-4208daf298c3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "scala"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## 0. Create the class"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"case class Data(key: String, value: String)\r\n",
							"\r\n",
							"case class ChangeData(key: String, newValue: String, deleted: Boolean, time: Long) {\r\n",
							"assert(newValue != null ^ deleted)\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## 1. Add sequence of changes"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val changeDataSource = Seq(\n",
							"  ChangeData(\"a\", \"10\", deleted = false, time = 0),\n",
							"  ChangeData(\"a\", null, deleted = true, time = 1),   // a was updated and then deleted\n",
							"  ChangeData(\"b\", null, deleted = true, time = 2),   // b was just deleted once\n",
							"  ChangeData(\"c\", null, deleted = true, time = 3),   // c was deleted and then updated twice\n",
							"  ChangeData(\"c\", \"20\", deleted = false, time = 4),\n",
							"  ChangeData(\"c\", \"200\", deleted = false, time = 5)\n",
							").toDF().createOrReplaceTempView(\"changes\")"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## 2. Read changes into DF"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val changesDF = spark.sql(\"select * from changes\")"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## 3. Expression for only latest changes"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val latestChangeForEachKey = changesDF.selectExpr(\"key\", \"struct(time, newValue, deleted) as otherCols\").groupBy(\"key\").agg(max(\"otherCols\").as(\"latest\")).selectExpr(\"key\", \"latest.*\")\r\n",
							"latestChangeForEachKey.show()"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## 4. Temp view of latest changes"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"latestChangeForEachKey.createOrReplaceTempView(\"lastchanges\")"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## 5. Save delta table"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"latestChangeForEachKey.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lastchanges\") "
						],
						"outputs": [],
						"execution_count": 23
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Delta-FromScratch')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1c86c96b-91c0-42b1-83ba-91d03fb222dc"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "scala"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"spark.version"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"case class Data(key: String, value: String)\r\n",
							"\r\n",
							"case class ChangeData(key: String, newValue: String, deleted: Boolean, time: Long) {\r\n",
							"  assert(newValue != null ^ deleted)\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val target = Seq(\r\n",
							"  Data(\"a\", \"0\"),\r\n",
							"  Data(\"b\", \"1\"),\r\n",
							"  Data(\"c\", \"2\"),\r\n",
							"  Data(\"d\", \"3\")\r\n",
							").toDF()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"sql(\"drop table if exists target\")\r\n",
							"target.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"target\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val targetDF = spark.sqlContext.sql(\"select * from target\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## New API docs\r\n",
							"https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/synapse-spark-sql-pool-import-export"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"//Add required imports\r\n",
							"import org.apache.spark.sql.DataFrame\r\n",
							"import org.apache.spark.sql.SaveMode\r\n",
							"import com.microsoft.spark.sqlanalytics.utils.Constants\r\n",
							"import org.apache.spark.sql.SqlAnalyticsConnector._"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"//new write options\r\n",
							"val writeOptions:Map[String, String] = Map(Constants.SERVER -> \"wplushiramsynapse.sql.azuresynapse.net\", \r\n",
							"Constants.TEMP_FOLDER -> \"abfss://wplushiramsynapsefs@wplushiramsynapseadlsv2.dfs.core.windows.net/synapse/workspaces/wplushiramsynapse/temp\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"//Set up optional callback/feedback function that can receive post write metrics of the job performed.\r\n",
							"var errorDuringWrite:Option[Throwable] = None\r\n",
							"val callBackFunctionToReceivePostWriteMetrics: (Map[String, Any], Option[Throwable]) => Unit =\r\n",
							"    (feedback: Map[String, Any], errorState: Option[Throwable]) => {\r\n",
							"    println(s\"Feedback map - ${feedback.map{case(key, value) => s\"$key -> $value\"}.mkString(\"{\",\",\\n\",\"}\")}\")\r\n",
							"    errorDuringWrite = errorState\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"//write to Synapse Dedicated SQL Pool (note - default SaveMode is set to ErrorIfExists)\r\n",
							"targetDF.\r\n",
							"    write.\r\n",
							"    options(writeOptions).\r\n",
							"    mode(SaveMode.Overwrite).\r\n",
							"    synapsesql(tableName = \"wplussynapsedw.dbo.target\", \r\n",
							"                tableType = Constants.INTERNAL, //For external table type value is Constants.EXTERNAL\r\n",
							"                location = None, //Not required for writing to an internal table\r\n",
							"                callBackHandle = Some(callBackFunctionToReceivePostWriteMetrics))\r\n",
							"\r\n",
							"//If write request has failed, raise an error and fail the Cell's execution.\r\n",
							"if(errorDuringWrite.isDefined) throw errorDuringWrite.get   "
						],
						"outputs": [],
						"execution_count": 13
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Delta-Guide-Py')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "65c82832-f94d-4389-8283-908aa2a9ddab"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Hitchhiker's Guide to Delta Lake (Python)\n",
							"\n",
							"This tutorial has been adapted for more clarity from its original counterpart [here](https://docs.delta.io/latest/quick-start.html). This notebook helps you quickly explore the main features of [Delta Lake](https://github.com/delta-io/delta). It provides code snippets that show how to read from and write to Delta Lake tables from interactive, batch, and streaming queries.\n",
							"\n",
							"Here's what we will cover:\n",
							"* Create a table\n",
							"* Understanding meta-data\n",
							"* Read data\n",
							"* Update table data\n",
							"* Overwrite table data\n",
							"* Conditional update without overwrite\n",
							"* Read older versions of data using Time Travel\n",
							"* Write a stream of data to a table\n",
							"* Read a stream of changes from a table"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Configuration\n",
							"Make sure you modify this as appropriate."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import random\n",
							"\n",
							"session_id = random.randint(0,1000000)\n",
							"delta_table_path = \"/delta/delta-table-{0}\".format(session_id)\n",
							"\n",
							"delta_table_path"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Create a table\n",
							"To create a Delta Lake table, write a DataFrame out in the **delta** format. You can use existing Spark SQL code and change the format from parquet, csv, json, and so on, to delta.\n",
							"\n",
							"These operations create a new Delta Lake table using the schema that was inferred from your DataFrame. For the full set of options available when you create a new Delta Lake table, see Create a table and Write to a table (subsequent cells in this notebook)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"data = spark.range(0,5)\n",
							"data.show()\n",
							"data.write.format(\"delta\").save(delta_table_path)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Understanding Meta-data\n",
							"\n",
							"In Delta Lake, meta-data is no different from data i.e., it is stored next to the data. Therefore, an interesting side-effect here is that you can peek into meta-data using regular Spark APIs. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"[log_line.value for log_line in spark.read.text(delta_table_path + \"/_delta_log/\").collect()]"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read data\n",
							"\n",
							"You read data in your Delta Lake table by specifying the path to the files."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.format(\"delta\").load(delta_table_path)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Update table data\n",
							"\n",
							"Delta Lake supports several operations to modify tables using standard DataFrame APIs. This example runs a batch job to overwrite the data in the table.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"data = spark.range(5,10)\n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"When you now inspect the meta-data, what you will notice is that the original data is over-written. Well, not in a true sense but appropriate entries are added to Delta's transaction log so it can provide an \"illusion\" that the original data was deleted. We can verify this by re-inspecting the meta-data. You will see several entries indicating reference removal to the original data."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"[log_line.value for log_line in spark.read.text(delta_table_path + \"/_delta_log/\").collect()]"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Save as catalog tables\n",
							"\n",
							"Delta Lake can write to managed or external catalog tables."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Write data to a new managed catalog table.\n",
							"data.write.format(\"delta\").saveAsTable(\"ManagedDeltaTable\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"# Define an external catalog table that points to the existing Delta Lake data in storage.\n",
							"spark.sql(\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '{0}'\".format(delta_table_path))"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"# List the 2 new tables.\n",
							"spark.sql(\"SHOW TABLES\").show()\n",
							"\n",
							"# Explore their properties.\n",
							"spark.sql(\"DESCRIBE EXTENDED ManagedDeltaTable\").show(truncate=False)\n",
							"spark.sql(\"DESCRIBE EXTENDED ExternalDeltaTable\").show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Conditional update without overwrite\n",
							"\n",
							"Delta Lake provides programmatic APIs to conditional update, delete, and merge (upsert) data into tables. For more information on these operations, see [Table Deletes, Updates, and Merges](https://docs.delta.io/latest/delta-update.html)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from delta.tables import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"delta_table = DeltaTable.forPath(spark, delta_table_path)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"# Update every even value by adding 100 to it\n",
							"delta_table.update(\n",
							"  condition = expr(\"id % 2 == 0\"),\n",
							"  set = { \"id\": expr(\"id + 100\") })\n",
							"delta_table.toDF().show()"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"# Delete every even value\n",
							"delta_table.delete(\"id % 2 == 0\")\n",
							"delta_table.toDF().show()"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"# Upsert (merge) new data\n",
							"new_data = spark.range(0,20).alias(\"newData\")\n",
							"\n",
							"delta_table.alias(\"oldData\")\\\n",
							"    .merge(new_data.alias(\"newData\"), \"oldData.id = newData.id\")\\\n",
							"    .whenMatchedUpdate(set = { \"id\": lit(\"-1\")})\\\n",
							"    .whenNotMatchedInsert(values = { \"id\": col(\"newData.id\") })\\\n",
							"    .execute()\n",
							"\n",
							"delta_table.toDF().show(100)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"source": [
							"## History\n",
							"Delta's most powerful feature is the ability to allow looking into history i.e., the changes that were made to the underlying Delta Table. The cell below shows how simple it is to inspect the history."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().show(20, 1000, False)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read older versions of data using Time Travel\n",
							"\n",
							"You can query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option.\n",
							"\n",
							"Once you run the cell below, you should see the first set of data, from before you overwrote it. Time Travel is an extremely powerful feature that takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again. For more information, see [Query an older snapshot of a table (time travel)](https://docs.delta.io/latest/delta-batch.html#deltatimetravel)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write a stream of data to a table\n",
							"\n",
							"You can also write to a Delta Lake table using Spark's Structured Streaming. The Delta Lake transaction log guarantees exactly-once processing, even when there are other streams or batch queries running concurrently against the table. By default, streams run in append mode, which adds new records to the table.\n",
							"\n",
							"For more information about Delta Lake integration with Structured Streaming, see [Table Streaming Reads and Writes](https://docs.delta.io/latest/delta-streaming.html).\n",
							"\n",
							"In the cells below, here's what we are doing:\n",
							"\n",
							"1. *Cell 28* Setup a simple Spark Structured Streaming job to generate a sequence and make the job write into our Delta Table\n",
							"2. *Cell 30* Show the newly appended data\n",
							"3. *Cell 31* Inspect history\n",
							"4. *Cell 32* Stop the structured streaming job\n",
							"5. *Cell 33* Inspect history <-- You'll notice appends have stopped"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"streaming_df = spark.readStream.format(\"rate\").load()\n",
							"stream = streaming_df\\\n",
							"    .selectExpr(\"value as id\")\\\n",
							"    .writeStream\\\n",
							"    .format(\"delta\")\\\n",
							"    .option(\"checkpointLocation\", \"/tmp/checkpoint-{0}\".format(session_id))\\\n",
							"    .start(delta_table_path)"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read a stream of changes from a table\n",
							"\n",
							"While the stream is writing to the Delta Lake table, you can also read from that table as streaming source. For example, you can start another streaming query that prints all the changes made to the Delta Lake table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.toDF().sort(col(\"id\").desc()).show(100)"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(20, 1000, False)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"source": [
							"stream.stop()"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"source": [
							"delta_table.history().drop(\"userId\", \"userName\", \"job\", \"notebook\", \"clusterId\", \"isolationLevel\", \"isBlindAppend\").show(100, 1000, False)"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Compaction\n",
							"\n",
							"If a Delta Table is growing too large, you can compact it by repartitioning into a smaller number of files.\n",
							"\n",
							"The option `dataChange = false` is an optimization that tells Delta Lake to do the repartition without marking the underlying data as \"modified\". This ensures that any other concurrent operations (such as streaming reads/writes) aren't negatively impacted.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"partition_count = 2\n",
							"\n",
							"spark.read\\\n",
							"    .format(\"delta\")\\\n",
							"    .load(delta_table_path)\\\n",
							"    .repartition(partition_count)\\\n",
							"    .write.option(\"dataChange\", \"false\")\\\n",
							"    .format(\"delta\")\\\n",
							"    .mode(\"overwrite\")\\\n",
							"    .save(delta_table_path)    "
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Convert Parquet to Delta\n",
							"You can do an in-place conversion from the Parquet format to Delta."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_path = \"/parquet/parquet-table-{0}\".format(session_id)\n",
							"\n",
							"data = spark.range(0,5)\n",
							"data.write.parquet(parquet_path)\n",
							"\n",
							"# Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"source": [
							"DeltaTable.convertToDelta(spark, \"parquet.`{0}`\".format(parquet_path))\n",
							"\n",
							"# Confirm that the converted data is now in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "markdown",
						"source": [
							"## SQL Support\n",
							"Delta supports table utility commands through SQL.  You can use SQL to:\n",
							"* Get a DeltaTable's history\n",
							"* Vacuum a DeltaTable\n",
							"* Convert a Parquet file to Delta\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DESCRIBE HISTORY delta.`{0}`\".format(delta_table_path)).show()"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"VACUUM delta.`{0}`\".format(delta_table_path)).show(10, 1000, False)"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"source": [
							"parquet_id = random.randint(0,1000)\n",
							"parquet_path = \"/parquet/parquet-table-{0}-{1}\".format(session_id, parquet_path)\n",
							"\n",
							"data = spark.range(0,5)\n",
							"data.write.parquet(parquet_path)\n",
							"\n",
							"# Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)\n",
							"\n",
							"# Use SQL to convert the parquet table to Delta\n",
							"spark.sql(\"CONVERT TO DELTA parquet.`{0}`\".format(parquet_path))\n",
							"\n",
							"DeltaTable.isDeltaTable(spark, parquet_path)"
						],
						"outputs": [],
						"execution_count": 38
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Delta-Guide-Scala')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cf5a5b96-8160-4723-bbf3-70b93fe2bc93"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "Synapse Spark"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Hitchhiker's Guide to Delta Lake (Scala)\n",
							"\n",
							"This tutorial has been adapted for more clarity from its original counterpart [here](https://docs.delta.io/latest/quick-start.html). This notebook helps you quickly explore the main features of [Delta Lake](https://github.com/delta-io/delta). It provides code snippets that show how to read from and write to Delta Lake tables from interactive, batch, and streaming queries.\n",
							"\n",
							"Here's what we will cover:\n",
							"* Create a table\n",
							"* Understanding meta-data\n",
							"* Read data\n",
							"* Update table data\n",
							"* Overwrite table data\n",
							"* Conditional update without overwrite\n",
							"* Read older versions of data using Time Travel\n",
							"* Write a stream of data to a table\n",
							"* Read a stream of changes from a table"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Configuration\n",
							"Make sure you modify this as appropriate."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"val sessionId = scala.util.Random.nextInt(1000000)\n",
							"val deltaTablePath = s\"/delta/delta-table-$sessionId\";"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Create a table\n",
							"To create a Delta Lake table, write a DataFrame out in the **delta** format. You can use existing Spark SQL code and change the format from parquet, csv, json, and so on, to delta.\n",
							"\n",
							"These operations create a new Delta Lake table using the schema that was inferred from your DataFrame. For the full set of options available when you create a new Delta Lake table, see Create a table and Write to a table (subsequent cells in this notebook)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"val data = spark.range(0, 5)\n",
							"data.show\n",
							"data.write.format(\"delta\").save(deltaTablePath)"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Understanding Meta-data\n",
							"\n",
							"In Delta Lake, meta-data is no different from data i.e., it is stored next to the data. Therefore, an interesting side-effect here is that you can peek into meta-data using regular Spark APIs. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.read.text(s\"$deltaTablePath/_delta_log/\").collect.foreach(println)"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read data\n",
							"\n",
							"You read data in your Delta Lake table by specifying the path to the files: \"/tmp/delta-table-scala\""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"val df = spark.read.format(\"delta\").load(deltaTablePath)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Update table data\n",
							"\n",
							"Delta Lake supports several operations to modify tables using standard DataFrame APIs. This example runs a batch job to overwrite the data in the table.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"val data = spark.range(5, 10)\n",
							"data.write.format(\"delta\").mode(\"overwrite\").save(deltaTablePath)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "markdown",
						"source": [
							"When you now inspect the meta-data, what you will notice is that the original data is over-written. Well, not in a true sense but appropriate entries are added to Delta's transaction log so it can provide an \"illusion\" that the original data was deleted. We can verify this by re-inspecting the meta-data. You will see several entries indicating reference removal to the original data."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.read.text(s\"$deltaTablePath/_delta_log/\").collect.foreach(println)"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Save as catalog tables\n",
							"\n",
							"Delta Lake can write to managed or external catalog tables."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"// Write data to a new managed catalog table.\n",
							"data.write.format(\"delta\").saveAsTable(\"ManagedDeltaTable\")"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"source": [
							"// Define an external catalog table that points to the existing Delta Lake data in storage.\n",
							"spark.sql(s\"CREATE TABLE ExternalDeltaTable USING DELTA LOCATION '$deltaTablePath'\")"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"source": [
							"// List the 2 new tables.\n",
							"spark.sql(\"SHOW TABLES\").show\n",
							"\n",
							"// Explore their properties.\n",
							"spark.sql(\"DESCRIBE EXTENDED ManagedDeltaTable\").show(truncate=false)\n",
							"spark.sql(\"DESCRIBE EXTENDED ExternalDeltaTable\").show(truncate=false)"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Conditional update without overwrite\n",
							"\n",
							"Delta Lake provides programmatic APIs to conditional update, delete, and merge (upsert) data into tables. For more information on these operations, see [Table Deletes, Updates, and Merges](https://docs.delta.io/latest/delta-update.html)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import io.delta.tables._\n",
							"import org.apache.spark.sql.functions._\n",
							"\n",
							"val deltaTable = DeltaTable.forPath(deltaTablePath)"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"source": [
							"// Update every even value by adding 100 to it\n",
							"deltaTable.update(\n",
							"  condition = expr(\"id % 2 == 0\"),\n",
							"  set = Map(\"id\" -> expr(\"id + 100\")))\n",
							"deltaTable.toDF.show"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"source": [
							"// Delete every even value\n",
							"deltaTable.delete(condition = expr(\"id % 2 == 0\"))\n",
							"deltaTable.toDF.show"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"source": [
							"// Upsert (merge) new data\n",
							"val newData = spark.range(0, 20).toDF\n",
							"\n",
							"deltaTable.as(\"oldData\").\n",
							"  merge(\n",
							"    newData.as(\"newData\"),\n",
							"    \"oldData.id = newData.id\").\n",
							"  whenMatched.\n",
							"  update(Map(\"id\" -> lit(-1))).\n",
							"  whenNotMatched.\n",
							"  insert(Map(\"id\" -> col(\"newData.id\"))).\n",
							"  execute()\n",
							"\n",
							"deltaTable.toDF.show()"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "markdown",
						"source": [
							"## History\n",
							"Delta's most powerful feature is the ability to allow looking into history i.e., the changes that were made to the underlying Delta Table. The cell below shows how simple it is to inspect the history."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.history.show(false)"
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read older versions of data using Time Travel\n",
							"\n",
							"You can query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option.\n",
							"\n",
							"Once you run the cell below, you should see the first set of data, from before you overwrote it. Time Travel is an extremely powerful feature that takes advantage of the power of the Delta Lake transaction log to access data that is no longer in the table. Removing the version 0 option (or specifying version 1) would let you see the newer data again. For more information, see [Query an older snapshot of a table (time travel)](https://docs.delta.io/latest/delta-batch.html#deltatimetravel)."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"val df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(deltaTablePath)\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Write a stream of data to a table\n",
							"\n",
							"You can also write to a Delta Lake table using Spark's Structured Streaming. The Delta Lake transaction log guarantees exactly-once processing, even when there are other streams or batch queries running concurrently against the table. By default, streams run in append mode, which adds new records to the table.\n",
							"\n",
							"For more information about Delta Lake integration with Structured Streaming, see [Table Streaming Reads and Writes](https://docs.delta.io/latest/delta-streaming.html).\n",
							"\n",
							"In the cells below, here's what we are doing:\n",
							"\n",
							"1. *Cell 28* Setup a simple Spark Structured Streaming job to generate a sequence and make the job write into our Delta Table\n",
							"2. *Cell 30* Show the newly appended data\n",
							"3. *Cell 31* Inspect history\n",
							"4. *Cell 32* Stop the structured streaming job\n",
							"5. *Cell 33* Inspect history <-- You'll notice appends have stopped"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"val streamingDf = spark.readStream.format(\"rate\").load()\n",
							"val stream = streamingDf.select($\"value\" as \"id\").writeStream.format(\"delta\").option(\"checkpointLocation\", s\"/tmp/checkpoint-$sessionId\").start(deltaTablePath)"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Read a stream of changes from a table\n",
							"\n",
							"While the stream is writing to the Delta Lake table, you can also read from that table as streaming source. For example, you can start another streaming query that prints all the changes made to the Delta Lake table."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.toDF.sort($\"id\".desc).show"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.history.show"
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"source": [
							"stream.stop"
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"source": [
							"deltaTable.history.show"
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Compaction\n",
							"\n",
							"If a Delta Table is growing too large, you can compact it by repartitioning into a smaller number of files.\n",
							"\n",
							"The option `dataChange = false` is an optimization that tells Delta Lake to do the repartition without marking the underlying data as \"modified\". This ensures that any other concurrent operations (such as streaming reads/writes) aren't negatively impacted.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"val partitionCount = 2\n",
							"\n",
							"spark.\n",
							"    read.\n",
							"    format(\"delta\").\n",
							"    load(deltaTablePath).\n",
							"    repartition(partitionCount).\n",
							"    write.\n",
							"    option(\"dataChange\", \"false\").\n",
							"    format(\"delta\").\n",
							"    mode(\"overwrite\").\n",
							"    save(deltaTablePath)    "
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Convert Parquet to Delta\n",
							"You can do an in-place conversion from the Parquet format to Delta.## Cell title\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"val parquetPath = s\"/parquet/parquet-table-$sessionId\"\n",
							"\n",
							"val data = spark.range(0,5)\n",
							"data.write.parquet(parquetPath)\n",
							"\n",
							"// Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(parquetPath)"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"source": [
							"DeltaTable.convertToDelta(spark, s\"parquet.`$parquetPath`\")\n",
							"\n",
							"// Confirm that the converted data is now in the Delta format\n",
							"DeltaTable.isDeltaTable(parquetPath)"
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "markdown",
						"source": [
							"## SQL Support\n",
							"Delta supports table utility commands through SQL.  You can use SQL to:\n",
							"* Get a DeltaTable's history\n",
							"* Vacuum a DeltaTable\n",
							"* Convert a Parquet file to Delta\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(s\"DESCRIBE HISTORY delta.`$deltaTablePath`\").show()"
						],
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(s\"VACUUM delta.`$deltaTablePath`\").show()"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"source": [
							"val parquetId = scala.util.Random.nextInt(1000)\n",
							"val parquetPath = s\"/parquet/parquet-table-$sessionId-$parquetId\"\n",
							"\n",
							"val data = spark.range(0,5)\n",
							"data.write.parquet(parquetPath)\n",
							"\n",
							"// Confirm that the data isn't in the Delta format\n",
							"DeltaTable.isDeltaTable(parquetPath)\n",
							"\n",
							"// Use SQL to convert the parquet table to Delta\n",
							"spark.sql(s\"CONVERT TO DELTA parquet.`$parquetPath`\")\n",
							"\n",
							"DeltaTable.isDeltaTable(parquetPath)"
						],
						"outputs": [],
						"execution_count": 56
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Delta-ReadLog')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f08ba0f5-a309-4d40-8fb9-e808c97d9846"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "scala"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Action items\r\n",
							"1. spark read delta table from adls, use expression to group by for latestchanges.\r\n",
							"2. call write api from adb to synapse sql dedicated pool to store latestchanges.\r\n",
							"3. auth to sql pool using keyvault"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"val accountName = \"wplushiramsynapseadlsv2\"\r\n",
							"val accountKey = \"\"\r\n",
							"val containerName = \"wplushiramsynapsefs\"\r\n",
							"val deltaPath = \"synapse/workspaces/wplushiramsynapse/warehouse/lastchanges\""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"val df_delta = spark.read.format(\"delta\").load(\"abfss://\"+containerName+\"@\"+accountName+\".dfs.core.windows.net/\"+deltaPath)\r\n",
							"df_delta.show()"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val latestonly = df_delta.selectExpr(\"key\", \"struct(time, newValue, deleted) as otherCols\").groupBy(\"key\").agg(max(\"otherCols\").as(\"latest\")).selectExpr(\"key\", \"latest.*\")\r\n",
							"latestonly.show()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"//Add required imports\r\n",
							"import org.apache.spark.sql.DataFrame\r\n",
							"import org.apache.spark.sql.SaveMode\r\n",
							"import com.microsoft.spark.sqlanalytics.utils.Constants\r\n",
							"import org.apache.spark.sql.SqlAnalyticsConnector._\r\n",
							"\r\n",
							"//new write options\r\n",
							"val writeOptions:Map[String, String] = Map(Constants.SERVER -> \"wplushiramsynapse.sql.azuresynapse.net\", \r\n",
							"Constants.TEMP_FOLDER -> \"abfss://wplushiramsynapsefs@wplushiramsynapseadlsv2.dfs.core.windows.net/synapse/workspaces/wplushiramsynapse/temp\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"//write to Synapse Dedicated SQL Pool (note - default SaveMode is set to ErrorIfExists)\r\n",
							"latestonly.\r\n",
							"    write.\r\n",
							"    options(writeOptions).\r\n",
							"    mode(SaveMode.Overwrite).\r\n",
							"    synapsesql(tableName = \"wplussynapsedw.dbo.latestonly\", \r\n",
							"                tableType = Constants.INTERNAL)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## ref.\r\n",
							"1. https://docs.databricks.com/data/data-sources/azure/synapse-analytics.html#frequently-asked-questions-faq\r\n",
							"2. https://github.com/Azure-Samples/azure-sql-db-databricks/blob/main/notebooks/03b-parallel-switch-in-load-into-partitioned-table-single.ipynb"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeltaChanges-Original')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta/Old"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "3dee81a7-3a88-4d79-9ed9-7a9e60df8984"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "scala"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Delta Demo\r\n",
							"\r\n",
							"1.\tSpark, scala delta, apis, sql magics - merge (see [this notebook](https://github.com/hfleitas/synapsedelta/blob/main/notebook/DeltaChanges3.json))\r\n",
							"2.\tServerless, credentials, open query (see sql [script](https://github.com/hfleitas/synapsedelta/blob/main/sqlscript/DeltaServerless.json))\r\n",
							"3.\tIntegration pipeline, copy activity with overwrite if exists (see [pipeline](https://github.com/hfleitas/synapsedelta/blob/main/pipeline/Delta-Import.json))\r\n",
							"4.\tSql pools - merge (see sql [script](https://github.com/hfleitas/synapsedelta/blob/main/sqlscript/DeltaSQLPoolMerge.json))"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"spark.version"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"case class Data(key: String, value: String)\r\n",
							"\r\n",
							"case class ChangeData(key: String, newValue: String, deleted: Boolean, time: Long) {\r\n",
							"  assert(newValue != null ^ deleted)\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val target = Seq(\r\n",
							"  Data(\"a\", \"0\"),\r\n",
							"  Data(\"b\", \"1\"),\r\n",
							"  Data(\"c\", \"2\"),\r\n",
							"  Data(\"d\", \"3\")\r\n",
							").toDF()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"sql(\"drop table if exists target\")\r\n",
							"target.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"target\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// import com.microsoft.spark.sqlanalytics.utils.Constants\r\n",
							"// import org.apache.spark.sql.SqlAnalyticsConnector._\r\n",
							"\r\n",
							"val targetDF = spark.sqlContext.sql(\"select * from target\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from target order by key"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"describe extended target"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#  Shake it up"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val changeDataSource = Seq(\r\n",
							"  ChangeData(\"a\", \"10\", deleted = false, time = 0),\r\n",
							"  ChangeData(\"a\", null, deleted = true, time = 1),   // a was updated and then deleted\r\n",
							"  ChangeData(\"b\", null, deleted = true, time = 2),   // b was just deleted once\r\n",
							"  ChangeData(\"c\", null, deleted = true, time = 3),   // c was deleted and then updated twice\r\n",
							"  ChangeData(\"c\", \"20\", deleted = false, time = 4),\r\n",
							"  ChangeData(\"c\", \"200\", deleted = false, time = 5)\r\n",
							").toDF().createOrReplaceTempView(\"changes\")"
						],
						"outputs": [],
						"execution_count": 61
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### SQL Example"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql \r\n",
							"MERGE INTO target t\r\n",
							"USING (\r\n",
							"  -- Find the latest change for each key based on the timestamp\r\n",
							"  SELECT key, latest.newValue as newValue, latest.deleted as deleted FROM (    \r\n",
							"    -- Note: For nested structs, max on struct is computed as \r\n",
							"    -- max on first struct field, if equal fall back to second fields, and so on.\r\n",
							"    SELECT key, max(struct(time, newValue, deleted)) as latest FROM changes GROUP BY key\r\n",
							"  )\r\n",
							") s\r\n",
							"ON s.key = t.key\r\n",
							"WHEN MATCHED AND s.deleted = true THEN DELETE\r\n",
							"WHEN MATCHED THEN UPDATE SET key = s.key, value = s.newValue\r\n",
							"WHEN NOT MATCHED AND s.deleted = false THEN INSERT (key, value) VALUES (key, newValue)"
						],
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val changesDF = spark.sql(\"select * from changes\")\r\n",
							"changesDF.head()"
						],
						"outputs": [],
						"execution_count": 63
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val latestChangeForEachKey = changesDF.\r\n",
							"selectExpr(\"key\", \"struct(time, newValue, deleted) as otherCols\").\r\n",
							"groupBy(\"key\").\r\n",
							"agg(max(\"otherCols\").\r\n",
							"as(\"latest\")).\r\n",
							"selectExpr(\"key\", \"latest.*\")"
						],
						"outputs": [],
						"execution_count": 64
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"latestChangeForEachKey.show() // shows the latest change for each key"
						],
						"outputs": [],
						"execution_count": 65
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"latestChangeForEachKey.createOrReplaceTempView(\"lastchanges\")"
						],
						"outputs": [],
						"execution_count": 66
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from lastchanges"
						],
						"outputs": [],
						"execution_count": 67
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"latestChangeForEachKey.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lastchanges\") "
						],
						"outputs": [],
						"execution_count": 68
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### SCALA Merge example (Skipped)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// deltaTable.as(\"t\")\r\n",
							"//   .merge(\r\n",
							"//     latestChangeForEachKey.as(\"s\"),\r\n",
							"//     \"s.key = t.key\")\r\n",
							"//   .whenMatched(\"s.deleted = true\")\r\n",
							"//   .delete()\r\n",
							"//   .whenMatched()\r\n",
							"//   .updateExpr(Map(\"key\" -> \"s.key\", \"value\" -> \"s.newValue\"))\r\n",
							"//   .whenNotMatched(\"s.deleted = false\")\r\n",
							"//   .insertExpr(Map(\"key\" -> \"s.key\", \"value\" -> \"s.newValue\"))\r\n",
							"//   .execute()"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import com.microsoft.spark.sqlanalytics.utils.Constants\r\n",
							"import org.apache.spark.sql.SqlAnalyticsConnector._\r\n",
							"\r\n",
							"val lastchangesDF = spark.sqlContext.sql(\"select * from lastchanges\")\r\n",
							"\r\n",
							"lastchangesDF.write.option(Constants.\r\n",
							"SERVER, \"wplushiramsynapse.sql.azuresynapse.net\").\r\n",
							"synapsesql(\"wplussynapsedw.dbo.lastchanges\", Constants.INTERNAL)"
						],
						"outputs": [],
						"execution_count": 70
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Resources\r\n",
							"* https://docs.databricks.com/_static/notebooks/merge-in-cdc.html\r\n",
							"* https://techcommunity.microsoft.com/t5/azure-synapse-analytics/query-delta-lake-files-using-t-sql-language-in-azure-synapse/ba-p/2388398\r\n",
							"* https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/query-delta-lake-format\r\n",
							"* https://databricks.com/blog/2019/03/19/efficient-upserts-into-data-lakes-databricks-delta.html\r\n",
							"* https://docs.microsoft.com/en-us/sql/t-sql/statements/merge-transact-sql?view=azure-sqldw-latest\r\n",
							"* https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html\r\n",
							"* https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-develop-ctas\r\n",
							"* https://www.mssqltips.com/sqlservertip/6282/azure-data-factory-multiple-file-load-example-part-2/"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeltaChanges2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta/Old"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9bdc40d6-2e79-4cdf-aa12-d21a73015881"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "Synapse Spark"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"case class Data(key: String, value: String)\r\n",
							"\r\n",
							"case class ChangeData(key: String, newValue: String, deleted: Boolean, time: Long) {\r\n",
							"  assert(newValue != null ^ deleted)\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val target = Seq(\r\n",
							"  Data(\"a\", \"0\"),\r\n",
							"  Data(\"b\", \"1\"),\r\n",
							"  Data(\"c\", \"2\"),\r\n",
							"  Data(\"d\", \"3\")\r\n",
							").toDF()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"sql(\"drop table if exists target\")\r\n",
							"target.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"target\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import com.microsoft.spark.sqlanalytics.utils.Constants\r\n",
							"import org.apache.spark.sql.SqlAnalyticsConnector._\r\n",
							"\r\n",
							"val targetDF = spark.sqlContext.sql(\"select * from target\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val changeDataSource = Seq(\n",
							"  ChangeData(\"a\", \"10\", deleted = false, time = 0),\n",
							"  ChangeData(\"a\", null, deleted = true, time = 1),   // a was updated and then deleted\n",
							"  ChangeData(\"b\", null, deleted = true, time = 2),   // b was just deleted once\n",
							"  ChangeData(\"c\", null, deleted = true, time = 3),   // c was deleted and then updated twice\n",
							"  ChangeData(\"c\", \"20\", deleted = false, time = 4),\n",
							"  ChangeData(\"c\", \"200\", deleted = false, time = 5)\n",
							").toDF().createOrReplaceTempView(\"changes\")\n",
							"\n",
							"// read from last checkpoint \n",
							"\n",
							"// val changeDataSource = Seq(\n",
							"//   ChangeData(\"d\", \"82\", deleted = false, time = 6),\n",
							"//   ChangeData(\"e\", \"19\", deleted = false, time = 7),\n",
							"//   ChangeData(\"f\", \"12\", deleted = false, time = 8),\n",
							"//   ChangeData(\"g\", null, deleted = true, time = 9),\n",
							"//   ChangeData(\"d\", \"21\", deleted = false, time = 10)\n",
							"// ).toDF().createOrReplaceTempView(\"changes\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql \r\n",
							"MERGE INTO target t\r\n",
							"USING (\r\n",
							"  -- Find the latest change for each key based on the timestamp\r\n",
							"  SELECT key, latest.newValue as newValue, latest.deleted as deleted FROM (    \r\n",
							"    -- Note: For nested structs, max on struct is computed as \r\n",
							"    -- max on first struct field, if equal fall back to second fields, and so on.\r\n",
							"    SELECT key, max(struct(time, newValue, deleted)) as latest FROM changes GROUP BY key\r\n",
							"  )\r\n",
							") s\r\n",
							"ON s.key = t.key\r\n",
							"WHEN MATCHED AND s.deleted = true THEN DELETE\r\n",
							"WHEN MATCHED THEN UPDATE SET key = s.key, value = s.newValue\r\n",
							"WHEN NOT MATCHED AND s.deleted = false THEN INSERT (key, value) VALUES (key, newValue)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val changesDF = spark.sql(\"select * from changes\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val latestChangeForEachKey = changesDF.selectExpr(\"key\", \"struct(time, newValue, deleted) as otherCols\").groupBy(\"key\").agg(max(\"otherCols\").as(\"latest\")).selectExpr(\"key\", \"latest.*\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"latestChangeForEachKey.createOrReplaceTempView(\"lastchanges\")"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"latestChangeForEachKey.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lastchanges\") "
						],
						"outputs": [],
						"execution_count": 15
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeltaChanges3')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Delta/Old"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "413f5533-b42d-4bbb-ad72-d21faf055903"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "scala"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Delta Demo\r\n",
							"\r\n",
							"1.\tSpark, scala delta, apis, sql magics - merge (see [this notebook](https://github.com/hfleitas/synapsedelta/blob/main/notebook/DeltaChanges3.json))\r\n",
							"2.\tServerless, credentials, open query (see sql [script](https://github.com/hfleitas/synapsedelta/blob/main/sqlscript/DeltaServerless.json))\r\n",
							"3.\tIntegration pipeline, copy activity with overwrite if exists (see [pipeline](https://github.com/hfleitas/synapsedelta/blob/main/pipeline/Delta-Import.json))\r\n",
							"4.\tSql pools - merge (see sql [script](https://github.com/hfleitas/synapsedelta/blob/main/sqlscript/DeltaSQLPoolMerge.json))"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"spark.version"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"source": [
							"case class Data(key: String, value: String)\r\n",
							"\r\n",
							"case class ChangeData(key: String, newValue: String, deleted: Boolean, time: Long) {\r\n",
							"  assert(newValue != null ^ deleted)\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val target = Seq(\r\n",
							"  Data(\"a\", \"0\"),\r\n",
							"  Data(\"b\", \"1\"),\r\n",
							"  Data(\"c\", \"2\"),\r\n",
							"  Data(\"d\", \"3\")\r\n",
							").toDF()"
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"sql(\"drop table if exists target\")\r\n",
							"target.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"target\")"
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// import com.microsoft.spark.sqlanalytics.utils.Constants\r\n",
							"// import org.apache.spark.sql.SqlAnalyticsConnector._\r\n",
							"\r\n",
							"val targetDF = spark.sqlContext.sql(\"select * from target\")"
						],
						"outputs": [],
						"execution_count": 58
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from target order by key"
						],
						"outputs": [],
						"execution_count": 59
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"describe extended target"
						],
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#  Shake it up"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val changeDataSource = Seq(\r\n",
							"  ChangeData(\"a\", \"10\", deleted = false, time = 0),\r\n",
							"  ChangeData(\"a\", null, deleted = true, time = 1),   // a was updated and then deleted\r\n",
							"  ChangeData(\"b\", null, deleted = true, time = 2),   // b was just deleted once\r\n",
							"  ChangeData(\"c\", null, deleted = true, time = 3),   // c was deleted and then updated twice\r\n",
							"  ChangeData(\"c\", \"20\", deleted = false, time = 4),\r\n",
							"  ChangeData(\"c\", \"200\", deleted = false, time = 5)\r\n",
							").toDF().createOrReplaceTempView(\"changes\")"
						],
						"outputs": [],
						"execution_count": 61
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### SQL Example"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql \r\n",
							"MERGE INTO target t\r\n",
							"USING (\r\n",
							"  -- Find the latest change for each key based on the timestamp\r\n",
							"  SELECT key, latest.newValue as newValue, latest.deleted as deleted FROM (    \r\n",
							"    -- Note: For nested structs, max on struct is computed as \r\n",
							"    -- max on first struct field, if equal fall back to second fields, and so on.\r\n",
							"    SELECT key, max(struct(time, newValue, deleted)) as latest FROM changes GROUP BY key\r\n",
							"  )\r\n",
							") s\r\n",
							"ON s.key = t.key\r\n",
							"WHEN MATCHED AND s.deleted = true THEN DELETE\r\n",
							"WHEN MATCHED THEN UPDATE SET key = s.key, value = s.newValue\r\n",
							"WHEN NOT MATCHED AND s.deleted = false THEN INSERT (key, value) VALUES (key, newValue)"
						],
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val changesDF = spark.sql(\"select * from changes\")\r\n",
							"changesDF.head()"
						],
						"outputs": [],
						"execution_count": 63
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val latestChangeForEachKey = changesDF.\r\n",
							"selectExpr(\"key\", \"struct(time, newValue, deleted) as otherCols\").\r\n",
							"groupBy(\"key\").\r\n",
							"agg(max(\"otherCols\").\r\n",
							"as(\"latest\")).\r\n",
							"selectExpr(\"key\", \"latest.*\")"
						],
						"outputs": [],
						"execution_count": 64
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"latestChangeForEachKey.show() // shows the latest change for each key"
						],
						"outputs": [],
						"execution_count": 65
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"latestChangeForEachKey.createOrReplaceTempView(\"lastchanges\")"
						],
						"outputs": [],
						"execution_count": 66
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from lastchanges"
						],
						"outputs": [],
						"execution_count": 67
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"latestChangeForEachKey.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lastchanges\") "
						],
						"outputs": [],
						"execution_count": 68
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### SCALA Merge example (Skipped)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// deltaTable.as(\"t\")\r\n",
							"//   .merge(\r\n",
							"//     latestChangeForEachKey.as(\"s\"),\r\n",
							"//     \"s.key = t.key\")\r\n",
							"//   .whenMatched(\"s.deleted = true\")\r\n",
							"//   .delete()\r\n",
							"//   .whenMatched()\r\n",
							"//   .updateExpr(Map(\"key\" -> \"s.key\", \"value\" -> \"s.newValue\"))\r\n",
							"//   .whenNotMatched(\"s.deleted = false\")\r\n",
							"//   .insertExpr(Map(\"key\" -> \"s.key\", \"value\" -> \"s.newValue\"))\r\n",
							"//   .execute()"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import com.microsoft.spark.sqlanalytics.utils.Constants\r\n",
							"import org.apache.spark.sql.SqlAnalyticsConnector._\r\n",
							"\r\n",
							"val lastchangesDF = spark.sqlContext.sql(\"select * from lastchanges\")\r\n",
							"\r\n",
							"lastchangesDF.write.option(Constants.\r\n",
							"SERVER, \"wplushiramsynapse.sql.azuresynapse.net\").\r\n",
							"synapsesql(\"wplussynapsedw.dbo.lastchanges\", Constants.INTERNAL)"
						],
						"outputs": [],
						"execution_count": 70
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Resources\r\n",
							"* https://docs.databricks.com/_static/notebooks/merge-in-cdc.html\r\n",
							"* https://techcommunity.microsoft.com/t5/azure-synapse-analytics/query-delta-lake-files-using-t-sql-language-in-azure-synapse/ba-p/2388398\r\n",
							"* https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/query-delta-lake-format\r\n",
							"* https://databricks.com/blog/2019/03/19/efficient-upserts-into-data-lakes-databricks-delta.html\r\n",
							"* https://docs.microsoft.com/en-us/sql/t-sql/statements/merge-transact-sql?view=azure-sqldw-latest\r\n",
							"* https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html\r\n",
							"* https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-develop-ctas\r\n",
							"* https://www.mssqltips.com/sqlservertip/6282/azure-data-factory-multiple-file-load-example-part-2/"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/E2E_MLFLOW_Sklearn_ADLS')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "_adhoc"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e9f76017-a49f-4cce-a95e-d8c8c63ccec5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Linear Regression (Sklearn)\r\n",
							"This tutorial shows how to use Predict on a Sklearn model.\r\n",
							"Duration 00:02:38\r\n",
							"# Train SKLearn Model"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import fsspec\r\n",
							"import pandas\r\n",
							"\r\n",
							"fsspec_handle = fsspec.open('abfss://wplushiramsynapsefs@wplushiramsynapseadlsv2.dfs.core.windows.net/LengthOfStay_cooked_small.csv')\r\n",
							"\r\n",
							"with fsspec_handle.open() as f:\r\n",
							"    train_df = pandas.read_csv(f)"
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"train_df.head()"
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import os\r\n",
							"import shutil\r\n",
							"import mlflow\r\n",
							"import json\r\n",
							"from mlflow.utils import model_utils\r\n",
							"\r\n",
							"import numpy as np\r\n",
							"import pandas as pd\r\n",
							"\r\n",
							"from sklearn.linear_model import LinearRegression"
						],
						"outputs": [],
						"execution_count": 58
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"class LinearRegressionModel():\r\n",
							"  _ARGS_FILENAME = 'args.json'\r\n",
							"  FEATURES_KEY = 'features'\r\n",
							"  TARGETS_KEY = 'targets'\r\n",
							"  TARGETS_PRED_KEY = 'targets_pred'\r\n",
							"\r\n",
							"  def __init__(self, fit_intercept, nb_input_features=9, nb_output_features=1):\r\n",
							"    self.fit_intercept = fit_intercept\r\n",
							"    self.nb_input_features = nb_input_features\r\n",
							"    self.nb_output_features = nb_output_features\r\n",
							"\r\n",
							"  def get_args(self):\r\n",
							"    args = {\r\n",
							"        'nb_input_features': self.nb_input_features,\r\n",
							"        'nb_output_features': self.nb_output_features,\r\n",
							"        'fit_intercept': self.fit_intercept\r\n",
							"    }\r\n",
							"    return args\r\n",
							"\r\n",
							"  def create_model(self):\r\n",
							"    self.model = LinearRegression(fit_intercept=self.fit_intercept)\r\n",
							"\r\n",
							"  def train(self, dataset):\r\n",
							"\r\n",
							"    features = np.stack([sample for sample in iter(\r\n",
							"        dataset[LinearRegressionModel.FEATURES_KEY])], axis=0)\r\n",
							"\r\n",
							"    targets = np.stack([sample for sample in iter(\r\n",
							"        dataset[LinearRegressionModel.TARGETS_KEY])], axis=0)\r\n",
							"\r\n",
							"\r\n",
							"    self.model.fit(features, targets)\r\n",
							"\r\n",
							"  def predict(self, dataset):\r\n",
							"    features = np.stack([sample for sample in iter(\r\n",
							"        dataset[LinearRegressionModel.FEATURES_KEY])], axis=0)\r\n",
							"    targets_pred = self.model.predict(features)\r\n",
							"    return targets_pred\r\n",
							"\r\n",
							"  def save(self, path):\r\n",
							"    if os.path.exists(path):\r\n",
							"      shutil.rmtree(path)\r\n",
							"\r\n",
							"    # save the sklearn model with mlflow\r\n",
							"    mlflow.sklearn.save_model(self.model, path)\r\n",
							"\r\n",
							"    # save args\r\n",
							"    self._save_args(path)\r\n",
							"\r\n",
							"  def _save_args(self, path):\r\n",
							"    args_filename = os.path.join(path, LinearRegressionModel._ARGS_FILENAME)\r\n",
							"    with open(args_filename, 'w') as f:\r\n",
							"      args = self.get_args()\r\n",
							"      json.dump(args, f)"
						],
						"outputs": [],
						"execution_count": 59
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def train(train_df, output_model_path):\r\n",
							"  print(f\"Start to train LinearRegressionModel.\")\r\n",
							"\r\n",
							"  # Initialize input dataset\r\n",
							"  dataset = train_df.to_numpy()\r\n",
							"  datasets = {}\r\n",
							"  datasets['targets'] = dataset[:, -1]\r\n",
							"  datasets['features'] = dataset[:, :9]\r\n",
							"\r\n",
							"  # Initialize model class obj\r\n",
							"  model_class = LinearRegressionModel(fit_intercept=10)\r\n",
							"  with mlflow.start_run(nested=True) as run:\r\n",
							"    model_class.create_model()\r\n",
							"    model_class.train(datasets)\r\n",
							"    model_class.save(output_model_path)\r\n",
							"    print(model_class.predict(datasets))"
						],
						"outputs": [],
						"execution_count": 60
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"train(train_df, './artifacts/output')"
						],
						"outputs": [],
						"execution_count": 61
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Upload to ADLS"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import fsspec\r\n",
							"import pandas \r\n",
							"from fsspec.core import split_protocol\r\n",
							"\r\n",
							"STORAGE_PATH = 'abfs://wplushiramsynapsefs/predict/models/mlflow/sklearn/e2e_linear_regression/'\r\n",
							"\r\n",
							"protocol, _ = split_protocol(STORAGE_PATH)\r\n",
							"print (protocol)\r\n",
							"\r\n",
							"fs = fsspec.filesystem(protocol)\r\n",
							"fs.put(\r\n",
							"    './artifacts/output',\r\n",
							"    STORAGE_PATH, \r\n",
							"    recursive=True, overwrite=True)"
						],
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Import SynapseML Predict"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import col, pandas_udf,udf,lit\r\n",
							"\r\n",
							"import azure.synapse.ml.predict as pcontext\r\n",
							"import azure.synapse.ml.predict.utils._logger as synapse_predict_logger\r\n",
							"\r\n",
							"print(pcontext.__version__)"
						],
						"outputs": [],
						"execution_count": 63
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Set some input parameters\r\n",
							"Model and Data are both stored on ADLS. Must use full abfss path, not the mount.\r\n",
							"\r\n",
							"Return type is int"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"DATA_FILE = \"abfss://wplushiramsynapsefs@wplushiramsynapseadlsv2.dfs.core.windows.net/LengthOfStay_cooked_small.csv\"\r\n",
							"ADLS_MODEL_URI_SKLEARN = \"abfss://wplushiramsynapsefs@wplushiramsynapseadlsv2.dfs.core.windows.net/predict/models/mlflow/sklearn/e2e_linear_regression/\"\r\n",
							"RETURN_TYPES = \"INT\""
						],
						"outputs": [],
						"execution_count": 64
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Enable SynapseML predict\r\n",
							"Set the spark conf spark.synapse.ml.predict.enabled as true to enable the library."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.conf.set(\"spark.synapse.ml.predict.enabled\",\"true\")"
						],
						"outputs": [],
						"execution_count": 65
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Bind Model"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"model = pcontext.bind_model(RETURN_TYPES, \"mlflow\", \"sklearn_linear_regression\", ADLS_MODEL_URI_SKLEARN).register()"
						],
						"outputs": [],
						"execution_count": 66
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Load Data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = spark.read \\\r\n",
							"    .format(\"csv\") \\\r\n",
							"    .option(\"header\", \"true\") \\\r\n",
							"    .csv(DATA_FILE,\r\n",
							"        inferSchema=True)\r\n",
							"df = df.select(df.columns[:9])\r\n",
							"df.createOrReplaceTempView('data')\r\n",
							"df.show(10)\r\n",
							"df"
						],
						"outputs": [],
						"execution_count": 67
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql \r\n",
							"select * from data"
						],
						"outputs": [],
						"execution_count": 68
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"predictions = spark.sql(\r\n",
							"                  \"\"\"\r\n",
							"                      SELECT PREDICT('sklearn_linear_regression', *) AS predict FROM data\r\n",
							"                  \"\"\"\r\n",
							"              ).show()"
						],
						"outputs": [],
						"execution_count": 69
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Special Thanks\r\n",
							"* Ajay Agarwal\r\n",
							"* Tian Wei\r\n",
							"* Nellie Gustafsson"
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Filemount')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SparkR"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c8eca52f-47cf-4c08-9b26-1fcb3ebafff6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"library(notebookutils)"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"source": [
							"# Via Linked Service (recommend):\r\n",
							"mssparkutils.fs.mount( \r\n",
							"    \"abfss://nyctaxistaging@wplushiramsynapseadlsv2.dfs.core.windows.net\", \r\n",
							"    \"/Dimension\", \r\n",
							"    list(\"linkedService\" = \"nyctaxistaging\")\r\n",
							") "
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/HiSpark')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "_adhoc"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "81443c93-b371-445e-8e76-ace6cdce067a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"print('hi spark')"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"import pkg_resources\r\n",
							"for d in pkg_resources.working_set:\r\n",
							"     print(d)"
						],
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Params')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "_adhoc"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c2304ea0-17fb-4577-86c6-e96ccd4b81c7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"p1 = 5"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"p=p1\r\n",
							"print(p)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azureml.opendatasets import PublicHolidays\r\n",
							"\r\n",
							"from datetime import datetime\r\n",
							"from dateutil import parser\r\n",
							"from dateutil.relativedelta import relativedelta\r\n",
							"\r\n",
							"\r\n",
							"end_date = datetime.today()\r\n",
							"start_date = datetime.today() - relativedelta(months=6)\r\n",
							"hol = PublicHolidays(start_date=start_date, end_date=end_date)\r\n",
							"hol_df = hol.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hol = hol_df.limit(p)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"csv_path = \"hol.csv\"\r\n",
							"hol.write.csv(csv_path, mode = 'overwrite', header = 'true')"
						],
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Read API demo')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "UPR"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bcc49c0e-143f-48f5-b106-c38b91abac52"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "scala"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Ref: https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/synapse-spark-sql-pool-import-export#usage"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"val df = spark.read.synapsesql(\"wplussynapsedw.staging.orcsample\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.createOrReplaceTempView(\"demo\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"select * from demo"
						],
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SKLearn-Predict')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "_adhoc"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "47a8a5fb-d9be-4274-a192-7ad077101dd0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import fsspec\r\n",
							"import pandas\r\n",
							"from fsspec.core import split_protocol\r\n",
							"\r\n",
							"fsspec_handle = fsspec.open('abfss://wplushiramsynapsefs@wplushiramsynapseadlsv2.dfs.core.windows.net/dominicks_OJ_train.csv')\r\n",
							"\r\n",
							"with fsspec_handle.open() as f:\r\n",
							"    train_df = pandas.read_csv(f)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# LinearRegresssion\r\n",
							"fails due to schema of dataset"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import os\r\n",
							"import shutil\r\n",
							"import mlflow\r\n",
							"import json\r\n",
							"from mlflow.utils import model_utils\r\n",
							"import numpy as np\r\n",
							"import pandas as pd\r\n",
							"from sklearn.linear_model import LinearRegression\r\n",
							"\r\n",
							"class LinearRegressionModel():\r\n",
							"    _ARGS_FILENAME = 'args.json'\r\n",
							"    FEATURES_KEY = 'features'\r\n",
							"    TARGETS_KEY = 'targets'\r\n",
							"    TARGETS_PRED_KEY = 'targets_pred'\r\n",
							"\r\n",
							"    def __init__(self, fit_intercept, nb_input_features=9, nb_output_features=1):\r\n",
							"        self.fit_intercept = fit_intercept\r\n",
							"        self.nb_input_features = nb_input_features\r\n",
							"        self.nb_output_features = nb_output_features\r\n",
							"\r\n",
							"    def get_args(self):\r\n",
							"        args = {\r\n",
							"            'nb_input_features': self.nb_input_features,\r\n",
							"            'nb_output_features': self.nb_output_features,\r\n",
							"            'fit_intercept': self.fit_intercept\r\n",
							"        }\r\n",
							"        return args\r\n",
							"\r\n",
							"    def create_model(self):\r\n",
							"        self.model = LinearRegression(fit_intercept=self.fit_intercept)\r\n",
							"\r\n",
							"    def train(self, dataset):\r\n",
							"        features = np.stack([sample for sample in iter(\r\n",
							"            dataset[LinearRegressionModel.FEATURES_KEY])], axis=0)\r\n",
							"        targets = np.stack([sample for sample in iter(\r\n",
							"            dataset[LinearRegressionModel.TARGETS_KEY])], axis=0)\r\n",
							"        self.model.fit(features, targets)\r\n",
							"\r\n",
							"    def predict(self, dataset):\r\n",
							"        features = np.stack([sample for sample in iter(\r\n",
							"            dataset[LinearRegressionModel.FEATURES_KEY])], axis=0)\r\n",
							"        targets_pred = self.model.predict(features)\r\n",
							"        return targets_pred\r\n",
							"\r\n",
							"    def save(self, path):\r\n",
							"        if os.path.exists(path):\r\n",
							"            shutil.rmtree(path)\r\n",
							"        mlflow.sklearn.save_model(self.model, path)\r\n",
							"        self._save_args(path)\r\n",
							"\r\n",
							"    def _save_args(self, path):\r\n",
							"        args_filename = os.path.join(path, LinearRegressionModel._ARGS_FILENAME)\r\n",
							"        with open(args_filename, 'w') as f:\r\n",
							"            args = self.get_args()\r\n",
							"            json.dump(args, f)\r\n",
							"\r\n",
							"def train(train_df, output_model_path):\r\n",
							"    print(f\"Start to train LinearRegressionModel.\")\r\n",
							"\r\n",
							"    dataset = train_df.to_numpy()\r\n",
							"    datasets = {}\r\n",
							"    datasets['targets'] = dataset[:, -1]\r\n",
							"    datasets['features'] = dataset[:, :9]\r\n",
							"\r\n",
							"    model_class = LinearRegressionModel(fit_intercept=10)\r\n",
							"    with mlflow.start_run(nested=True) as run:\r\n",
							"        model_class.create_model()\r\n",
							"        model_class.train(datasets)\r\n",
							"        model_class.save(output_model_path)\r\n",
							"        print(model_class.predict(datasets))\r\n",
							"\r\n",
							"train(train_df, './artifacts/output')\r\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SparkSQLCTE')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "_adhoc"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "09c21e28-bfe3-4516-9a90-a74d227d5781"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"df = spark.sql(\"SELECT * FROM `default`.`lastchanges`\")\n",
							"# df.show(10)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from default.lastchanges"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql \r\n",
							"with t as (\r\n",
							"    SELECT 'a' as x,'b' as y\r\n",
							") \r\n",
							"select * from t\r\n",
							""
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"WITH CTE \r\n",
							"AS \r\n",
							"    (SELECT key as x, time, deleted FROM default.lastchanges)\r\n",
							"\r\n",
							"SELECT * FROM CTE"
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TestPerformanceTuning')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "from Ismael",
				"folder": {
					"name": "Spark"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "92a5ddab-ab6b-4d81-8b8a-e554128cb512"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Start your Spark session.\r\n",
							"spark\r\n",
							"\r\n",
							"# Disable BroadcastHashJoin, so Spark will use standard SortMergeJoin. Currently, Hyperspace indexes utilize SortMergeJoin to speed up query.\r\n",
							"spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\r\n",
							"\r\n",
							"# Verify that BroadcastHashJoin is set correctly \r\n",
							"print(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.types import StructField, StructType, StringType, IntegerType\r\n",
							"\r\n",
							"# Sample department records\r\n",
							"departments = [(10, \"Accounting\", \"New York\"), (20, \"Research\", \"Dallas\"), (30, \"Sales\", \"Chicago\"), (40, \"Operations\", \"Boston\")]\r\n",
							"\r\n",
							"# Sample employee records\r\n",
							"employees = [(7369, \"SMITH\", 20), (7499, \"ALLEN\", 30), (7521, \"WARD\", 30), (7566, \"JONES\", 20), (7698, \"BLAKE\", 30)]\r\n",
							"\r\n",
							"# Create a schema for the dataframe\r\n",
							"dept_schema = StructType([StructField('deptId', IntegerType(), True), StructField('deptName', StringType(), True), StructField('location', StringType(), True)])\r\n",
							"emp_schema = StructType([StructField('empId', IntegerType(), True), StructField('empName', StringType(), True), StructField('deptId', IntegerType(), True)])\r\n",
							"\r\n",
							"departments_df = spark.createDataFrame(departments, dept_schema)\r\n",
							"employees_df = spark.createDataFrame(employees, emp_schema)\r\n",
							"\r\n",
							"#TODO ** customize this location path **\r\n",
							"emp_Location = \"/perftest/employees.parquet\"\r\n",
							"dept_Location = \"/perftest/departments.parquet\"\r\n",
							"\r\n",
							"employees_df.write.mode(\"overwrite\").parquet(emp_Location)\r\n",
							"departments_df.write.mode(\"overwrite\").parquet(dept_Location)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# emp_Location and dept_Location are the user-defined locations above to save parquet files\r\n",
							"emp_DF = spark.read.parquet(emp_Location)\r\n",
							"dept_DF = spark.read.parquet(dept_Location)\r\n",
							"\r\n",
							"# Verify the data is available and correct\r\n",
							"emp_DF.show()\r\n",
							"dept_DF.show()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.ls(\"/\")\r\n",
							"##mssparkutils.fs.rm(\"/<yourpath>\", True)\r\n",
							"##mssparkutils.fs.ls(\"/\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from hyperspace import *\r\n",
							"\r\n",
							"# Create an instance of Hyperspace\r\n",
							"hyperspace = Hyperspace(spark)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create index configurations\r\n",
							"emp_IndexConfig = IndexConfig(\"empIndex1\", [\"deptId\"], [\"empName\"])\r\n",
							"dept_IndexConfig1 = IndexConfig(\"deptIndex1\", [\"deptId\"], [\"deptName\"])\r\n",
							"dept_IndexConfig2 = IndexConfig(\"deptIndex2\", [\"location\"], [\"deptName\"])\r\n",
							"\r\n",
							"# Create indexes from configurations\r\n",
							"hyperspace.createIndex(emp_DF, emp_IndexConfig)\r\n",
							"hyperspace.createIndex(dept_DF, dept_IndexConfig1)\r\n",
							"hyperspace.createIndex(dept_DF, dept_IndexConfig2)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.deleteIndex(\"empIndex1\")\r\n",
							"hyperspace.deleteIndex(\"deptIndex1\")\r\n",
							"hyperspace.deleteIndex(\"deptIndex2\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.vacuumIndex(\"empIndex1\")\r\n",
							"hyperspace.vacuumIndex(\"deptIndex1\")\r\n",
							"hyperspace.vacuumIndex(\"deptIndex2\")\r\n",
							"hyperspace.indexes().show()"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create indexes from configurations\r\n",
							"hyperspace.createIndex(emp_DF, emp_IndexConfig)\r\n",
							"hyperspace.createIndex(dept_DF, dept_IndexConfig1)\r\n",
							"hyperspace.createIndex(dept_DF, dept_IndexConfig2)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.indexes().show()"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Enable Hyperspace\r\n",
							"Hyperspace.enable(spark)\r\n",
							"\r\n",
							"emp_DF = spark.read.parquet(emp_Location)\r\n",
							"dept_DF = spark.read.parquet(dept_Location)\r\n",
							"\r\n",
							"emp_DF.show(5)\r\n",
							"dept_DF.show(5)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Filter with equality predicate\r\n",
							"\r\n",
							"eqFilter = dept_DF.filter(\"\"\"deptId = 20\"\"\").select(\"\"\"deptName\"\"\")\r\n",
							"eqFilter.show()"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# eqFilter.explain(True)"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"emp_DF.createOrReplaceTempView(\"EMP\")\r\n",
							"dept_DF.createOrReplaceTempView(\"DEPT\")\r\n",
							"\r\n",
							"joinQuery = spark.sql(\"SELECT EMP.empName, DEPT.deptName FROM EMP, DEPT WHERE EMP.deptId = DEPT.deptId\")\r\n",
							"\r\n",
							"joinQuery.show()\r\n",
							"##joinQuery.explain(True)"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"joinQuery.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"Employees\")"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"OPTIMIZE Employees"
						],
						"outputs": [],
						"execution_count": 21
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TestPerformanceTuningDelta')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Spark"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bd0d05cb-a63f-41df-8458-d80c6bbb5e67"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Start your Spark session.\r\n",
							"spark\r\n",
							"\r\n",
							"# Disable BroadcastHashJoin, so Spark will use standard SortMergeJoin. Currently, Hyperspace indexes utilize SortMergeJoin to speed up query.\r\n",
							"spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\r\n",
							"\r\n",
							"# Verify that BroadcastHashJoin is set correctly \r\n",
							"print(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.types import StructField, StructType, StringType, IntegerType\r\n",
							"\r\n",
							"# Sample department records\r\n",
							"departments = [(10, \"Accounting\", \"New York\"), (20, \"Research\", \"Dallas\"), (30, \"Sales\", \"Chicago\"), (40, \"Operations\", \"Boston\")]\r\n",
							"\r\n",
							"# Sample employee records\r\n",
							"employees = [(7369, \"SMITH\", 20), (7499, \"ALLEN\", 30), (7521, \"WARD\", 30), (7566, \"JONES\", 20), (7698, \"BLAKE\", 30)]\r\n",
							"\r\n",
							"# Create a schema for the dataframe\r\n",
							"dept_schema = StructType([StructField('deptId', IntegerType(), True), StructField('deptName', StringType(), True), StructField('location', StringType(), True)])\r\n",
							"emp_schema = StructType([StructField('empId', IntegerType(), True), StructField('empName', StringType(), True), StructField('deptId', IntegerType(), True)])\r\n",
							"\r\n",
							"departments_df = spark.createDataFrame(departments, dept_schema)\r\n",
							"employees_df = spark.createDataFrame(employees, emp_schema)\r\n",
							"\r\n",
							"#TODO ** customize this location path **\r\n",
							"emp_Location = \"/perftest/employees.parquet\"\r\n",
							"dept_Location = \"/perftest/departments.parquet\"\r\n",
							"\r\n",
							"employees_df.write.format(\"delta\").mode(\"overwrite\").save(emp_Location)\r\n",
							"departments_df.write.format(\"delta\").mode(\"overwrite\").save(dept_Location)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# emp_Location and dept_Location are the user-defined locations above to save parquet files\r\n",
							"emp_DF = spark.read.format(\"delta\").load(emp_Location)\r\n",
							"dept_DF = spark.read.format(\"delta\").load(dept_Location)\r\n",
							"\r\n",
							"# Verify the data is available and correct\r\n",
							"emp_DF.show()\r\n",
							"dept_DF.show()"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.ls(\"/\")\r\n",
							"##mssparkutils.fs.rm(\"/<yourpath>\", True)\r\n",
							"##mssparkutils.fs.ls(\"/\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from hyperspace import *\r\n",
							"\r\n",
							"# Create an instance of Hyperspace\r\n",
							"hyperspace = Hyperspace(spark)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create index configurations\r\n",
							"\r\n",
							"emp_IndexConfig = IndexConfig(\"empIndex1\", [\"deptId\"], [\"empName\"])\r\n",
							"dept_IndexConfig1 = IndexConfig(\"deptIndex1\", [\"deptId\"], [\"deptName\"])\r\n",
							"dept_IndexConfig2 = IndexConfig(\"deptIndex2\", [\"location\"], [\"deptName\"])"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Run as needed\r\n",
							"If you had index definitions then you may want to delete and vacuum them"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.deleteIndex(\"empIndex1\")\r\n",
							"hyperspace.deleteIndex(\"deptIndex1\")\r\n",
							"hyperspace.deleteIndex(\"deptIndex2\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.vacuumIndex(\"empIndex1\")\r\n",
							"hyperspace.vacuumIndex(\"deptIndex1\")\r\n",
							"hyperspace.vacuumIndex(\"deptIndex2\")\r\n",
							"hyperspace.indexes().show()"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Index creation\r\n",
							"The error for delta:\r\n",
							"\r\n",
							"Only creating index over HDFS file based scan nodes is supported. Source plan: FileScan parquet [empId#586,empName#587,deptId#588] Batched: true, DataFilters: [], Format: Parquet, Location: "
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create indexes from configurations\r\n",
							"\r\n",
							"hyperspace.createIndex(emp_DF, emp_IndexConfig)\r\n",
							"hyperspace.createIndex(dept_DF, dept_IndexConfig1)\r\n",
							"hyperspace.createIndex(dept_DF, dept_IndexConfig2)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.indexes().show()"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Enable Hyperspace\r\n",
							"Hyperspace.enable(spark)\r\n",
							"\r\n",
							"emp_DF = spark.read.parquet(emp_Location)\r\n",
							"dept_DF = spark.read.parquet(dept_Location)\r\n",
							"\r\n",
							"emp_DF.show(5)\r\n",
							"dept_DF.show(5)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Filter with equality predicate\r\n",
							"\r\n",
							"eqFilter = dept_DF.filter(\"\"\"deptId = 20\"\"\").select(\"\"\"deptName\"\"\")\r\n",
							"eqFilter.show()"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"eqFilter.explain(True)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"emp_DF.createOrReplaceTempView(\"EMP\")\r\n",
							"dept_DF.createOrReplaceTempView(\"DEPT\")\r\n",
							"\r\n",
							"joinQuery = spark.sql(\"SELECT EMP.empName, DEPT.deptName FROM EMP, DEPT WHERE EMP.deptId = DEPT.deptId\")\r\n",
							"\r\n",
							"joinQuery.show()\r\n",
							"##joinQuery.explain(True)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"joinQuery.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"Employees\")"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Z Order\r\n",
							"This works as long as the format is **Delta**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"OPTIMIZE Employees"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"select count(*) from Employees where deptName like 'Research%';"
						],
						"outputs": [],
						"execution_count": 19
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TestPerformanceTuningParquet')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Spark"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "24e83685-38d9-48cd-90af-eb1c3931129f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Start your Spark session.\r\n",
							"spark\r\n",
							"\r\n",
							"# Disable BroadcastHashJoin, so Spark will use standard SortMergeJoin. Currently, Hyperspace indexes utilize SortMergeJoin to speed up query.\r\n",
							"spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\r\n",
							"\r\n",
							"# Verify that BroadcastHashJoin is set correctly \r\n",
							"print(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.types import StructField, StructType, StringType, IntegerType\r\n",
							"\r\n",
							"# Sample department records\r\n",
							"departments = [(10, \"Accounting\", \"New York\"), (20, \"Research\", \"Dallas\"), (30, \"Sales\", \"Chicago\"), (40, \"Operations\", \"Boston\")]\r\n",
							"\r\n",
							"# Sample employee records\r\n",
							"employees = [(7369, \"SMITH\", 20), (7499, \"ALLEN\", 30), (7521, \"WARD\", 30), (7566, \"JONES\", 20), (7698, \"BLAKE\", 30)]\r\n",
							"\r\n",
							"# Create a schema for the dataframe\r\n",
							"dept_schema = StructType([StructField('deptId', IntegerType(), True), StructField('deptName', StringType(), True), StructField('location', StringType(), True)])\r\n",
							"emp_schema = StructType([StructField('empId', IntegerType(), True), StructField('empName', StringType(), True), StructField('deptId', IntegerType(), True)])\r\n",
							"\r\n",
							"departments_df = spark.createDataFrame(departments, dept_schema)\r\n",
							"employees_df = spark.createDataFrame(employees, emp_schema)\r\n",
							"\r\n",
							"#TODO ** customize this location path **\r\n",
							"emp_Location = \"/perftest/employees.parquet\"\r\n",
							"dept_Location = \"/perftest/departments.parquet\"\r\n",
							"\r\n",
							"employees_df.write.mode(\"overwrite\").parquet(emp_Location)\r\n",
							"departments_df.write.mode(\"overwrite\").parquet(dept_Location)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# emp_Location and dept_Location are the user-defined locations above to save parquet files\r\n",
							"emp_DF = spark.read.parquet(emp_Location)\r\n",
							"dept_DF = spark.read.parquet(dept_Location)\r\n",
							"\r\n",
							"# Verify the data is available and correct\r\n",
							"emp_DF.show()\r\n",
							"dept_DF.show()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.ls(\"/\")\r\n",
							"##mssparkutils.fs.rm(\"/<yourpath>\", True)\r\n",
							"##mssparkutils.fs.ls(\"/\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from hyperspace import *\r\n",
							"\r\n",
							"# Create an instance of Hyperspace\r\n",
							"hyperspace = Hyperspace(spark)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create index configurations\r\n",
							"\r\n",
							"emp_IndexConfig = IndexConfig(\"empIndex1\", [\"deptId\"], [\"empName\"])\r\n",
							"dept_IndexConfig1 = IndexConfig(\"deptIndex1\", [\"deptId\"], [\"deptName\"])\r\n",
							"dept_IndexConfig2 = IndexConfig(\"deptIndex2\", [\"location\"], [\"deptName\"])"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Run as needed\r\n",
							"If you had index definitions then you may want to delete and vacuum them"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.deleteIndex(\"empIndex1\")\r\n",
							"hyperspace.deleteIndex(\"deptIndex1\")\r\n",
							"hyperspace.deleteIndex(\"deptIndex2\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.vacuumIndex(\"empIndex1\")\r\n",
							"hyperspace.vacuumIndex(\"deptIndex1\")\r\n",
							"hyperspace.vacuumIndex(\"deptIndex2\")\r\n",
							"hyperspace.indexes().show()"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Index creation"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create indexes from configurations\r\n",
							"\r\n",
							"hyperspace.createIndex(emp_DF, emp_IndexConfig)\r\n",
							"hyperspace.createIndex(dept_DF, dept_IndexConfig1)\r\n",
							"hyperspace.createIndex(dept_DF, dept_IndexConfig2)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"hyperspace.indexes().show()"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Enable Hyperspace\r\n",
							"Hyperspace.enable(spark)\r\n",
							"\r\n",
							"emp_DF = spark.read.parquet(emp_Location)\r\n",
							"dept_DF = spark.read.parquet(dept_Location)\r\n",
							"\r\n",
							"emp_DF.show(5)\r\n",
							"dept_DF.show(5)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Filter with equality predicate\r\n",
							"\r\n",
							"eqFilter = dept_DF.filter(\"\"\"deptId = 20\"\"\").select(\"\"\"deptName\"\"\")\r\n",
							"eqFilter.show()"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Weird. This fails. Uncomment for testing.\r\n",
							"\r\n",
							"##eqFilter.explain(True)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"emp_DF.createOrReplaceTempView(\"EMP\")\r\n",
							"dept_DF.createOrReplaceTempView(\"DEPT\")\r\n",
							"\r\n",
							"joinQuery = spark.sql(\"SELECT EMP.empName, DEPT.deptName FROM EMP, DEPT WHERE EMP.deptId = DEPT.deptId\")\r\n",
							"\r\n",
							"joinQuery.show()\r\n",
							"##joinQuery.explain(True)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Z Order over Delta table\r\n",
							"As you can see, the error describes that Delta is not supported for this kind of data structure"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"joinQuery.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"Employees\")"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"OPTIMIZE Employees"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TmpShakespeare')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SparkR"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fb202f8d-b9cb-427b-af2d-295fcfe994f1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparkr",
						"display_name": "Synapse SparkR"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"library(notebookutils)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create new directory\r\n",
							"mssparkutils.fs.mkdirs(\"/tmp/a/b/c\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Copy file\r\n",
							"mssparkutils.fs.cp(\"/shakespeare.txt\", \"/tmp/a/b/c/shakespeare.txt\", TRUE) # Set the third parameter as True to copy all files and directories recursively\r\n",
							"mssparkutils.fs.cp(\"/tmp/a/b/c/shakespeare.txt\", \"/tmp/a/b/c/shakespeare1.txt\", TRUE)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(mssparkutils.fs.head(\"/tmp/a/b/c/shakespeare.txt\", 1024))"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.mv(\"/tmp/a/b/c/shakespeare1.txt\", \"/tmp/a/b/c/shakespeare2.txt\", TRUE) # Set the last parameter as True to firstly create the parent directory if it does not exist"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Append content to a file\r\n",
							"mssparkutils.fs.append(\"/tmp/a/b/c/test.txt\",\" from mssparkutils\", TRUE)\r\n",
							"#mssparkutils.fs.head(\"/tmp/a/b/c/test.txt\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.rm(\"/tmp\", TRUE) # Set the last parameter as True to remove all files and directories recursively"
						],
						"outputs": [],
						"execution_count": 8
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/_notebookutilsdemo')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "from PG",
				"folder": {
					"name": "SparkR"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "336cdbc8-4231-4d12-9033-0c5a25ed6ac1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_sparkr",
						"display_name": "Synapse SparkR"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Requirements\r\n",
							"- Join the AAD group `MSSparkUtils R Users` to get access to this workspace and the associated storage account\r\n",
							"- To enable R support, please add feature request parameter `feature.rlanguage=true` to the site URL. e.g. `https://ms.web.azuresynapse.net/en-us/authoring/analyze/...?feature.rlanguage=true`\r\n",
							"- R support is only available in Spark 3.1 or above. We don't support this feature in Spark 2.4\r\n",
							"- `library(notebookutils)` has to be executed first to load the library. We will remove this constraint in the next release.\r\n",
							"\r\n",
							"## Known issue\r\n",
							"- Notebook reference run is not supported temporarily for now. It will be supported in the next release. If urgent, we can provide some workaround"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Loading notebookutils library"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"library(notebookutils)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## File System Utilities"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.help()"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# List files\r\n",
							"mssparkutils.fs.ls(\"/\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# View file properties\r\n",
							"files <- mssparkutils.fs.ls(\"/\")\r\n",
							"for (file in files) {\r\n",
							"    writeLines(paste(file$name, file$isDir, file$isFile, file$size))\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create new directory\r\n",
							"mssparkutils.fs.mkdirs(\"/tmp/a/b/c\")"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Copy file\r\n",
							"mssparkutils.fs.cp(\"/samples/python/shakespeare.txt\", \"/tmp/a/b/c/shakespeare.txt\", TRUE) # Set the third parameter as True to copy all files and directories recursively\r\n",
							"mssparkutils.fs.cp(\"/tmp/a/b/c/shakespeare.txt\", \"/tmp/a/b/c/shakespeare1.txt\", TRUE)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Preview file content\r\n",
							"print(mssparkutils.fs.head(\"/tmp/a/b/c/shakespeare.txt\", 1024))"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Move file\r\n",
							"mssparkutils.fs.mv(\"/tmp/a/b/c/shakespeare1.txt\", \"/tmp/a/b/c/shakespeare2.txt\", TRUE) # Set the last parameter as True to firstly create the parent directory if it does not exist"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write file\r\n",
							"mssparkutils.fs.put(\"/tmp/a/b/c/test.txt\",\"Hello world\", TRUE)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Append content to a file\r\n",
							"mssparkutils.fs.append(\"/tmp/a/b/c/test.txt\",\" from mssparkutils\", TRUE)\r\n",
							"#mssparkutils.fs.head(\"/tmp/a/b/c/test.txt\")"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Delete file or directory\r\n",
							"mssparkutils.fs.rm(\"/tmp\", TRUE) # Set the last parameter as True to remove all files and directories recursively"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### File mount/unmount"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Via Linked Service (recommend):\r\n",
							"mssparkutils.fs.mount( \r\n",
							"    \"abfss://nbutilsfs@nbutilsstorage.dfs.core.windows.net\", \r\n",
							"    \"/testR\", \r\n",
							"    list(\"linkedService\" = \"nbutilsmssparkutils\")\r\n",
							") "
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# How to access files under mount point via local file system API\r\n",
							"jobId <- mssparkutils.env.getJobId()\r\n",
							"f <- sprintf(\"/synfs/%s/testR/myFile.txt\", jobId)\r\n",
							"cat(\"Hello world.\\n\", file = f, sep = \"\", append = TRUE)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# How to access files under mount point using mssparktuils fs API\r\n",
							"# List dirs:\r\n",
							"jobId <- mssparkutils.env.getJobId()\r\n",
							"directory <- sprintf(\"synfs:/%s/testR\", jobId)\r\n",
							"mssparkutils.fs.ls(directory)"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Read file content:\r\n",
							"jobId <- mssparkutils.env.getJobId()\r\n",
							"file <- sprintf(\"synfs:/%s/testR/myFile.txt\", jobId)\r\n",
							"mssparkutils.fs.head(file)"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create directory:\r\n",
							"jobId <- mssparkutils.env.getJobId()\r\n",
							"directory <- sprintf(\"synfs:/%s/testR/newdir\", jobId)\r\n",
							"mssparkutils.fs.mkdirs(directory)"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# How to unmount the mount point\r\n",
							"mssparkutils.fs.unmount(\"/testR\")"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Notebook utilities"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.notebook.help()"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reference a notebook\r\n",
							"mssparkutils.notebook.run(\"/folder/Sample1\", 90, list(\"input\" = 20))"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Exit a notebook\r\n",
							"#mssparkutils.notebook.exit(\"value string\")"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Credentials utilities"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.credentials.help()"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get token\r\n",
							"mssparkutils.credentials.getToken(\"Synapse\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Validate token\r\n",
							"token <- mssparkutils.credentials.getToken(\"Synapse\")\r\n",
							"mssparkutils.credentials.isValidToken(token)"
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get connection string or credentials for linked service\r\n",
							"mssparkutils.credentials.getConnectionStringOrCreds(\"nbutilsmssparkutils\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get secret using workspace identity\r\n",
							"mssparkutils.credentials.getSecret(\"nbutilskv\",\"mytestSecret\",\"nbutilskvlinkedservice\")"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get secret using user credentials\r\n",
							"mssparkutils.credentials.getSecret(\"nbutilskv\",\"mytestSecret\")"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Put secret using workspace identity\r\n",
							"mssparkutils.credentials.putSecret(\"nbutilskv\", \"mytestSecret\",\"secretvaluewithmsi\",\"nbutilskvlinkedservice\")"
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Put secret using user credentials\r\n",
							"mssparkutils.credentials.putSecret(\"nbutilskv\", \"mytestSecret\",\"secretvaluewithusercred\")"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Environment utilities"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.env.help()"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get user name\r\n",
							"mssparkutils.env.getUserName()"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get user ID\r\n",
							"mssparkutils.env.getUserId()"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get job ID\r\n",
							"mssparkutils.env.getJobId()"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get workspace name\r\n",
							"mssparkutils.env.getWorkspaceName()"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get pool name\r\n",
							"mssparkutils.env.getPoolName()"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get cluster ID\r\n",
							"mssparkutils.env.getClusterId()"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Runtime Context"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ctx <- mssparkutils.runtime.context()\r\n",
							"for (key in ls(ctx)) {\r\n",
							"    writeLines(paste(key, ctx[[key]], sep = \"\\t\"))\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Visualization"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df <- read.df('abfss://users@contosolake.dfs.core.windows.net/NYCTripSmall.parquet', format='parquet')\r\n",
							"display(limit(df, 10))"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"displayHTML('<!DOCTYPE html>\r\n",
							"<meta charset=\"utf-8\">\r\n",
							"\r\n",
							"<!-- Load d3.js -->\r\n",
							"<script src=\"https://d3js.org/d3.v4.js\"></script>\r\n",
							"\r\n",
							"<!-- Create a div where the graph will take place -->\r\n",
							"<div id=\"my_dataviz\"></div>\r\n",
							"<script>\r\n",
							"\r\n",
							"// set the dimensions and margins of the graph\r\n",
							"var margin = {top: 10, right: 30, bottom: 30, left: 40},\r\n",
							"  width = 400 - margin.left - margin.right,\r\n",
							"  height = 400 - margin.top - margin.bottom;\r\n",
							"\r\n",
							"// append the svg object to the body of the page\r\n",
							"var svg = d3.select(\"#my_dataviz\")\r\n",
							".append(\"svg\")\r\n",
							"  .attr(\"width\", width + margin.left + margin.right)\r\n",
							"  .attr(\"height\", height + margin.top + margin.bottom)\r\n",
							".append(\"g\")\r\n",
							"  .attr(\"transform\",\r\n",
							"        \"translate(\" + margin.left + \",\" + margin.top + \")\");\r\n",
							"\r\n",
							"// Create Data\r\n",
							"var data = [12,19,11,13,12,22,13,4,15,16,18,19,20,12,11,9]\r\n",
							"\r\n",
							"// Compute summary statistics used for the box:\r\n",
							"var data_sorted = data.sort(d3.ascending)\r\n",
							"var q1 = d3.quantile(data_sorted, .25)\r\n",
							"var median = d3.quantile(data_sorted, .5)\r\n",
							"var q3 = d3.quantile(data_sorted, .75)\r\n",
							"var interQuantileRange = q3 - q1\r\n",
							"var min = q1 - 1.5 * interQuantileRange\r\n",
							"var max = q1 + 1.5 * interQuantileRange\r\n",
							"\r\n",
							"// Show the Y scale\r\n",
							"var y = d3.scaleLinear()\r\n",
							"  .domain([0,24])\r\n",
							"  .range([height, 0]);\r\n",
							"svg.call(d3.axisLeft(y))\r\n",
							"\r\n",
							"// a few features for the box\r\n",
							"var center = 200\r\n",
							"var width = 100\r\n",
							"\r\n",
							"// Show the main vertical line\r\n",
							"svg\r\n",
							".append(\"line\")\r\n",
							"  .attr(\"x1\", center)\r\n",
							"  .attr(\"x2\", center)\r\n",
							"  .attr(\"y1\", y(min) )\r\n",
							"  .attr(\"y2\", y(max) )\r\n",
							"  .attr(\"stroke\", \"black\")\r\n",
							"\r\n",
							"// Show the box\r\n",
							"svg\r\n",
							".append(\"rect\")\r\n",
							"  .attr(\"x\", center - width/2)\r\n",
							"  .attr(\"y\", y(q3) )\r\n",
							"  .attr(\"height\", (y(q1)-y(q3)) )\r\n",
							"  .attr(\"width\", width )\r\n",
							"  .attr(\"stroke\", \"black\")\r\n",
							"  .style(\"fill\", \"#69b3a2\")\r\n",
							"\r\n",
							"// show median, min and max horizontal \r\n",
							"svg\r\n",
							".selectAll(\"toto\")\r\n",
							".data([min, median, max])\r\n",
							".enter()\r\n",
							".append(\"line\")\r\n",
							"  .attr(\"x1\", center-width/2)\r\n",
							"  .attr(\"x2\", center+width/2)\r\n",
							"  .attr(\"y1\", function(d){ return(y(d))} )\r\n",
							"  .attr(\"y2\", function(d){ return(y(d))} )\r\n",
							"  .attr(\"stroke\", \"black\")\r\n",
							"</script>')"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Magic command"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%%sparkr\r\n",
							"df <- read.df('abfss://users@contosolake.dfs.core.windows.net/NYCTripSmall.parquet', format='parquet')\r\n",
							"display(limit(df, 10))"
						],
						"outputs": [],
						"execution_count": 47
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dominicks_OJ_train-Score')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "_adhoc"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d673e1e1-b7ce-45a9-89b3-86162c42c223"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.functions import col, pandas_udf,udf,lit\r\n",
							"from azureml.core import Workspace\r\n",
							"from azureml.core.authentication import ServicePrincipalAuthentication\r\n",
							"import azure.synapse.ml.predict as pcontext\r\n",
							"import azure.synapse.ml.predict.utils._logger as synapse_predict_logger"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"DATA_FILE = \"abfss://wplushiramsynapsefs@wplushiramsynapseadlsv2.dfs.core.windows.net/dominicks_OJ_train.csv\"\r\n",
							"AML_MODEL_URI = \"https://ml.azure.com/model/wplushiramsynapse-dominicks_oj_train-20211111125807-Best:1\" \r\n",
							"ADLS_MODEL_URI = \"abfss://wplushiramsynapseadlsv2.dfs.core.windows.net/wplushiramsynapsefs/wplushiramsynapse-dominicks_oj_train-20211111010118-Best_/\"\r\n",
							"RETURN_TYPES = \"double\" \r\n",
							"RUNTIME = \"mlflow\""
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from notebookutils.mssparkutils import azureML\r\n",
							"ws = azureML.getWorkspace(\"sqldbedgeoulvzyml\")\r\n",
							"ws"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.conf.set(\"spark.synapse.ml.predict.enabled\",\"true\")"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"model = pcontext.bind_model(\r\n",
							"    return_types=RETURN_TYPES, \r\n",
							"    runtime=RUNTIME, \r\n",
							"    model_alias=\"oj_forecast\", \r\n",
							"    model_uri=AML_MODEL_URI, \r\n",
							"    aml_workspace=ws \r\n",
							"    ).register()"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"model = pcontext.bind_model(\r\n",
							"    return_types=RETURN_TYPES, \r\n",
							"    runtime=RUNTIME, \r\n",
							"    model_alias=\"oj_forecast\", \r\n",
							"    model_uri=ADLS_MODEL_URI\r\n",
							"    ).register()"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Read data from ADLS\r\n",
							"df = spark.read \\\r\n",
							".format(\"csv\") \\\r\n",
							".option(\"header\", \"true\") \\\r\n",
							".csv(DATA_FILE,\r\n",
							"    inferSchema=True)\r\n",
							"df.createOrReplaceTempView('datafile')"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql \r\n",
							"select * from datafile"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							}
						},
						"source": [
							"%%sql\r\n",
							"SELECT PREDICT(oj_forecast, Quantity) AS predict FROM datafile"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Round 2 "
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Import libraries and read training dataset from ADLS\r\n",
							"\r\n",
							"import fsspec\r\n",
							"import pandas\r\n",
							"from fsspec.core import split_protocol\r\n",
							"\r\n",
							"adls_account_name = 'wplushiramsynapseadlsv2' #Provide exact ADLS account name\r\n",
							"adls_account_key = 'xyz' #Provide exact ADLS account key\r\n",
							"\r\n",
							"fsspec_handle = fsspec.open('abfss://wplushiramsynapsefs@wplushiramsynapseadlsv2.dfs.core.windows.net/wplushiramsynapse-dominicks_oj_train-20211111010118-Best_/model.pkl', account_name=adls_account_name, account_key=adls_account_key)\r\n",
							"\r\n",
							"with fsspec_handle.open() as f:\r\n",
							"       train_df = pandas.read_csv(f)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dominicks_OJ_train-Train')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "_adhoc"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b3b09124-0296-470a-bf8c-852fc5644f26"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import azureml.core\n",
							"\n",
							"from azureml.core import Experiment, Workspace, Dataset, Datastore\n",
							"from azureml.train.automl import AutoMLConfig\n",
							"from notebookutils import mssparkutils\n",
							"from azureml.data.dataset_factory import TabularDatasetFactory"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"linkedService_name = \"sqldbedgeoulvzyml\"\n",
							"experiment_name = \"wplushiramsynapse-dominicks_oj_train-20211111125807\"\n",
							"\n",
							"ws = mssparkutils.azureML.getWorkspace(linkedService_name)\n",
							"experiment = Experiment(ws, experiment_name)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.sql(\"SELECT * FROM default.dominicks_oj_train\")\n",
							"\n",
							"datastore = Datastore.get_default(ws)\n",
							"dataset = TabularDatasetFactory.register_spark_dataframe(df, datastore, name = experiment_name + \"-dataset\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.automl.core.forecasting_parameters import ForecastingParameters\n",
							"\n",
							"forecasting_parameters = ForecastingParameters(\n",
							"    time_column_name = \"WeekStarting\",\n",
							"    forecast_horizon = \"auto\",\n",
							"    time_series_id_column_names = [\"Store\",\"Brand\"],\n",
							")\n",
							"\n",
							"automl_config = AutoMLConfig(spark_context = sc,\n",
							"                             task = \"forecasting\",\n",
							"                             training_data = dataset,\n",
							"                             label_column_name = \"Quantity\",\n",
							"                             primary_metric = \"normalized_root_mean_squared_error\",\n",
							"                             experiment_timeout_hours = 0.25,\n",
							"                             max_concurrent_iterations = 2,\n",
							"                             n_cross_validations = 5,\n",
							"                             forecasting_parameters = forecasting_parameters)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"run = experiment.submit(automl_config)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"displayHTML(\"<a href={} target='_blank'>Your experiment in Azure Machine Learning portal: {}</a>\".format(run.get_portal_url(), run.id))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"run.wait_for_completion()\n",
							"\n",
							"import mlflow\n",
							"\n",
							"# Get best model from automl run\n",
							"best_run, non_onnx_model = run.get_output()\n",
							"\n",
							"artifact_path = experiment_name + \"_artifact\"\n",
							"\n",
							"mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
							"mlflow.set_experiment(experiment_name)\n",
							"\n",
							"with mlflow.start_run() as run:\n",
							"    # Save the model to the outputs directory for capture\n",
							"    mlflow.sklearn.log_model(non_onnx_model, artifact_path)\n",
							"\n",
							"    # Register the model to AML model registry\n",
							"    mlflow.register_model(\"runs:/\" + run.info.run_id + \"/\" + artifact_path, \"wplushiramsynapse-dominicks_oj_train-20211111125807-Best\")"
						],
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dominicks_OJ_train')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "_adhoc"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7d9b4600-4be1-4442-9245-0a15d5e8d3f5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"from pyspark.sql.types import StringType, DateType, DoubleType\r\n",
							"\r\n",
							"df = spark.read.load('abfss://wplushiramsynapsefs@wplushiramsynapseadlsv2.dfs.core.windows.net/dominicks_OJ_train.csv', format='csv'\r\n",
							"## Ifheaderexistsuncommentlinebelow\r\n",
							", header=True\r\n",
							")\r\n",
							"\r\n",
							"df = df.withColumnRenamed(\"Large HH\", \"Large_HH\")\r\n",
							"\r\n",
							"df = df \\\r\n",
							".withColumn(\"WeekStarting\", df[\"WeekStarting\"].cast(DateType())) \\\r\n",
							".withColumn(\"Store\", df[\"Store\"].cast(DoubleType())) \\\r\n",
							".withColumn(\"Brand\", df[\"Brand\"].cast(StringType())) \\\r\n",
							".withColumn(\"Quantity\", df[\"Quantity\"].cast(DoubleType())) \\\r\n",
							".withColumn(\"Advert\", df[\"Advert\"].cast(DoubleType())) \\\r\n",
							".withColumn(\"Price\", df[\"Price\"].cast(DoubleType())) \\\r\n",
							".withColumn(\"Age60\", df[\"Age60\"].cast(DoubleType())) \\\r\n",
							".withColumn(\"COLLEGE\", df[\"COLLEGE\"].cast(DoubleType())) \\\r\n",
							".withColumn(\"INCOME\", df[\"INCOME\"].cast(DoubleType())) \\\r\n",
							".withColumn(\"Hincome150\", df[\"Hincome150\"].cast(DoubleType())) \\\r\n",
							".withColumn(\"Large_HH\", df[\"Large_HH\"].cast(DoubleType())) \\\r\n",
							".withColumn(\"Minorities\", df[\"Minorities\"].cast(DoubleType())) \\\r\n",
							".withColumn(\"WorkingWoman\", df[\"WorkingWoman\"].cast(DoubleType())) \\\r\n",
							".withColumn(\"SSTRDIST\", df[\"SSTRDIST\"].cast(DoubleType())) \\\r\n",
							".withColumn(\"SSTRVOL\", df[\"SSTRVOL\"].cast(DoubleType())) \\\r\n",
							".withColumn(\"CPDIST5\", df[\"CPDIST5\"].cast(DoubleType())) \\\r\n",
							".withColumn(\"CPWVOL5\", df[\"CPWVOL5\"].cast(DoubleType())) \r\n",
							"\r\n",
							"df.write.mode(\"overwrite\").saveAsTable(\"dominicks_OJ_train\")"
						],
						"outputs": [],
						"execution_count": 17
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/dominicks_oj_train-RegressionOnnx')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "_adhoc"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "aed0d893-c5b3-42ba-b11e-0f1c8c7e4ffa"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import azureml.core\n",
							"\n",
							"from azureml.core import Experiment, Workspace, Dataset, Datastore\n",
							"from azureml.train.automl import AutoMLConfig\n",
							"from notebookutils import mssparkutils\n",
							"from azureml.data.dataset_factory import TabularDatasetFactory"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"linkedService_name = \"sqldbedgeoulvzyml\"\n",
							"experiment_name = \"wplushiramsynapse-dominicks_oj_train-20211118120913\"\n",
							"\n",
							"ws = mssparkutils.azureML.getWorkspace(linkedService_name)\n",
							"experiment = Experiment(ws, experiment_name)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"df = spark.sql(\"SELECT * FROM default.dominicks_oj_train\")\n",
							"\n",
							"datastore = Datastore.get_default(ws)\n",
							"dataset = TabularDatasetFactory.register_spark_dataframe(df, datastore, name = experiment_name + \"-dataset\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"automl_config = AutoMLConfig(spark_context = sc,\n",
							"                             task = \"regression\",\n",
							"                             training_data = dataset,\n",
							"                             label_column_name = \"Quantity\",\n",
							"                             primary_metric = \"normalized_root_mean_squared_error\",\n",
							"                             experiment_timeout_hours = 0.25,\n",
							"                             max_concurrent_iterations = 2,\n",
							"                             enable_onnx_compatible_models = True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"run = experiment.submit(automl_config)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"displayHTML(\"<a href={} target='_blank'>Your experiment in Azure Machine Learning portal: {}</a>\".format(run.get_portal_url(), run.id))"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"source": [
							"run.wait_for_completion()\n",
							"\n",
							"import onnxruntime\n",
							"import mlflow\n",
							"import mlflow.onnx\n",
							"\n",
							"from mlflow.models.signature import ModelSignature\n",
							"from mlflow.types import DataType\n",
							"from mlflow.types.schema import ColSpec, Schema\n",
							"\n",
							"# Get best model from automl run\n",
							"best_run, onnx_model = run.get_output(return_onnx_model=True)\n",
							"\n",
							"# Define utility functions to infer the schema of ONNX model\n",
							"def _infer_schema(data):\n",
							"    res = []\n",
							"    for _, col in enumerate(data):\n",
							"        t = col.type.replace(\"tensor(\", \"\").replace(\")\", \"\")\n",
							"        if t in [\"bool\"]:\n",
							"            dt = DataType.boolean\n",
							"        elif t in [\"int8\", \"uint8\", \"int16\", \"uint16\", \"int32\"]:\n",
							"            dt = DateType.integer\n",
							"        elif t in [\"uint32\", \"int64\"]:\n",
							"            dt = DataType.long\n",
							"        elif t in [\"float16\", \"bfloat16\", \"float\"]:\n",
							"            dt = DataType.float\n",
							"        elif t in [\"double\"]:\n",
							"            dt = DataType.double\n",
							"        elif t in [\"string\"]:\n",
							"            dt = DataType.string\n",
							"        else:\n",
							"            raise Exception(\"Unsupported type: \" + t)\n",
							"        res.append(ColSpec(type=dt, name=col.name))\n",
							"    return Schema(res)\n",
							"\n",
							"def _infer_signature(onnx_model):\n",
							"    onnx_model_bytes = onnx_model.SerializeToString()\n",
							"    onnx_runtime = onnxruntime.InferenceSession(onnx_model_bytes)\n",
							"    inputs = _infer_schema(onnx_runtime.get_inputs())\n",
							"    outputs = _infer_schema(onnx_runtime.get_outputs())\n",
							"    return ModelSignature(inputs, outputs)\n",
							"\n",
							"# Infer signature of ONNX model\n",
							"signature = _infer_signature(onnx_model)\n",
							"\n",
							"artifact_path = experiment_name + \"_artifact\"\n",
							"mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
							"mlflow.set_experiment(experiment_name)\n",
							"\n",
							"with mlflow.start_run() as run:\n",
							"    # Save the model to the outputs directory for capture\n",
							"    mlflow.onnx.log_model(onnx_model, artifact_path, signature=signature)\n",
							"\n",
							"    # Register the model to AML model registry\n",
							"    mlflow.register_model(\"runs:/\" + run.info.run_id + \"/\" + artifact_path, \"wplushiramsynapse-dominicks_oj_train-20211118120913-Best\")"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/helloworld')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "_adhoc"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5f4937a4-df05-4219-a8cd-b975adc8b239"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"print('hello world')"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/notebookutilsdemoR')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SparkR"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "bbcdd8e4-3df2-49d4-84df-a2431c75113f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Requirements\r\n",
							"- Join the AAD group `MSSparkUtils R Users` to get access to this workspace and the associated storage account\r\n",
							"- To enable R support, please add feature request parameter `feature.rlanguage=true` to the site URL. e.g. `https://ms.web.azuresynapse.net/en-us/authoring/analyze/...?feature.rlanguage=true`\r\n",
							"- R support is only available in Spark 3.1 or above. We don't support this feature in Spark 2.4\r\n",
							"- `library(notebookutils)` has to be executed first to load the library. We will remove this constraint in the next release.\r\n",
							"\r\n",
							"## Known issue\r\n",
							"- Notebook reference run is not supported temporarily for now. It will be supported in the next release. If urgent, we can provide some workaround"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Loading notebookutils library"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"library(notebookutils)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## File System Utilities"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.help()"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# List files\r\n",
							"mssparkutils.fs.ls(\"/\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# View file properties\r\n",
							"files <- mssparkutils.fs.ls(\"/\")\r\n",
							"for (file in files) {\r\n",
							"    writeLines(paste(file$name, file$isDir, file$isFile, file$size))\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create new directory\r\n",
							"mssparkutils.fs.mkdirs(\"/tmp/a/b/c\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Copy file\r\n",
							"mssparkutils.fs.cp(\"/shakespeare.txt\", \"/tmp/a/b/c/shakespeare.txt\", TRUE) # Set the third parameter as True to copy all files and directories recursively\r\n",
							"mssparkutils.fs.cp(\"/tmp/a/b/c/shakespeare.txt\", \"/tmp/a/b/c/shakespeare1.txt\", TRUE)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Preview file content\r\n",
							"print(mssparkutils.fs.head(\"/tmp/a/b/c/shakespeare.txt\", 1024))"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Rename by Move file\r\n",
							"mssparkutils.fs.mv(\"/tmp/a/b/c/shakespeare1.txt\", \"/tmp/a/b/c/shakespeare2.txt\", TRUE) # Set the last parameter as True to firstly create the parent directory if it does not exist"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write file\r\n",
							"mssparkutils.fs.put(\"/tmp/a/b/c/test.txt\",\"Hello world\", TRUE)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Append content to a file\r\n",
							"mssparkutils.fs.append(\"/tmp/a/b/c/test.txt\",\" from mssparkutils\", TRUE)\r\n",
							"#mssparkutils.fs.head(\"/tmp/a/b/c/test.txt\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Delete file or directory\r\n",
							"mssparkutils.fs.rm(\"/tmp\", TRUE) # Set the last parameter as True to remove all files and directories recursively"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### File mount/unmount"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Via Linked Service (recommend):\r\n",
							"mssparkutils.fs.mount( \r\n",
							"    \"abfss://nbutilsfs@nbutilsstorage.dfs.core.windows.net\", \r\n",
							"    \"/testR\", \r\n",
							"    list(\"linkedService\" = \"nbutilsmssparkutils\")\r\n",
							") "
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# How to access files under mount point via local file system API\r\n",
							"jobId <- mssparkutils.env.getJobId()\r\n",
							"f <- sprintf(\"/synfs/%s/testR/myFile.txt\", jobId)\r\n",
							"cat(\"Hello world.\\n\", file = f, sep = \"\", append = TRUE)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# How to access files under mount point using mssparktuils fs API\r\n",
							"# List dirs:\r\n",
							"jobId <- mssparkutils.env.getJobId()\r\n",
							"directory <- sprintf(\"synfs:/%s/testR\", jobId)\r\n",
							"mssparkutils.fs.ls(directory)"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Read file content:\r\n",
							"jobId <- mssparkutils.env.getJobId()\r\n",
							"file <- sprintf(\"synfs:/%s/testR/myFile.txt\", jobId)\r\n",
							"mssparkutils.fs.head(file)"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create directory:\r\n",
							"jobId <- mssparkutils.env.getJobId()\r\n",
							"directory <- sprintf(\"synfs:/%s/testR/newdir\", jobId)\r\n",
							"mssparkutils.fs.mkdirs(directory)"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# How to unmount the mount point\r\n",
							"mssparkutils.fs.unmount(\"/testR\")"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Notebook utilities"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.notebook.help()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Reference a notebook\r\n",
							"mssparkutils.notebook.run(\"SparkSQLCTE\", 360)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Exit a notebook\r\n",
							"#mssparkutils.notebook.exit(\"value string\")"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Credentials utilities"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.credentials.help()"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get token\r\n",
							"mssparkutils.credentials.getToken(\"Synapse\")"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Validate token\r\n",
							"token <- mssparkutils.credentials.getToken(\"Synapse\")\r\n",
							"mssparkutils.credentials.isValidToken(token)"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get connection string or credentials for linked service\r\n",
							"mssparkutils.credentials.getConnectionStringOrCreds(\"nbutilsmssparkutils\")"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get secret using workspace identity\r\n",
							"mssparkutils.credentials.getSecret(\"nbutilskv\",\"mytestSecret\",\"nbutilskvlinkedservice\")"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get secret using user credentials\r\n",
							"mssparkutils.credentials.getSecret(\"nbutilskv\",\"mytestSecret\")"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Put secret using workspace identity\r\n",
							"mssparkutils.credentials.putSecret(\"nbutilskv\", \"mytestSecret\",\"secretvaluewithmsi\",\"nbutilskvlinkedservice\")"
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Put secret using user credentials\r\n",
							"mssparkutils.credentials.putSecret(\"nbutilskv\", \"mytestSecret\",\"secretvaluewithusercred\")"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Environment utilities"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.env.help()"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get user name\r\n",
							"mssparkutils.env.getUserName()"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get user ID\r\n",
							"mssparkutils.env.getUserId()"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get job ID\r\n",
							"mssparkutils.env.getJobId()"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get workspace name\r\n",
							"mssparkutils.env.getWorkspaceName()"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get pool name\r\n",
							"mssparkutils.env.getPoolName()"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Get cluster ID\r\n",
							"mssparkutils.env.getClusterId()"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Runtime Context"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ctx <- mssparkutils.runtime.context()\r\n",
							"for (key in ls(ctx)) {\r\n",
							"    writeLines(paste(key, ctx[[key]], sep = \"\\t\"))\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Visualization"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df <- read.df('abfss://users@contosolake.dfs.core.windows.net/NYCTripSmall.parquet', format='parquet')\r\n",
							"display(limit(df, 10))"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"displayHTML('<!DOCTYPE html>\r\n",
							"<meta charset=\"utf-8\">\r\n",
							"\r\n",
							"<!-- Load d3.js -->\r\n",
							"<script src=\"https://d3js.org/d3.v4.js\"></script>\r\n",
							"\r\n",
							"<!-- Create a div where the graph will take place -->\r\n",
							"<div id=\"my_dataviz\"></div>\r\n",
							"<script>\r\n",
							"\r\n",
							"// set the dimensions and margins of the graph\r\n",
							"var margin = {top: 10, right: 30, bottom: 30, left: 40},\r\n",
							"  width = 400 - margin.left - margin.right,\r\n",
							"  height = 400 - margin.top - margin.bottom;\r\n",
							"\r\n",
							"// append the svg object to the body of the page\r\n",
							"var svg = d3.select(\"#my_dataviz\")\r\n",
							".append(\"svg\")\r\n",
							"  .attr(\"width\", width + margin.left + margin.right)\r\n",
							"  .attr(\"height\", height + margin.top + margin.bottom)\r\n",
							".append(\"g\")\r\n",
							"  .attr(\"transform\",\r\n",
							"        \"translate(\" + margin.left + \",\" + margin.top + \")\");\r\n",
							"\r\n",
							"// Create Data\r\n",
							"var data = [12,19,11,13,12,22,13,4,15,16,18,19,20,12,11,9]\r\n",
							"\r\n",
							"// Compute summary statistics used for the box:\r\n",
							"var data_sorted = data.sort(d3.ascending)\r\n",
							"var q1 = d3.quantile(data_sorted, .25)\r\n",
							"var median = d3.quantile(data_sorted, .5)\r\n",
							"var q3 = d3.quantile(data_sorted, .75)\r\n",
							"var interQuantileRange = q3 - q1\r\n",
							"var min = q1 - 1.5 * interQuantileRange\r\n",
							"var max = q1 + 1.5 * interQuantileRange\r\n",
							"\r\n",
							"// Show the Y scale\r\n",
							"var y = d3.scaleLinear()\r\n",
							"  .domain([0,24])\r\n",
							"  .range([height, 0]);\r\n",
							"svg.call(d3.axisLeft(y))\r\n",
							"\r\n",
							"// a few features for the box\r\n",
							"var center = 200\r\n",
							"var width = 100\r\n",
							"\r\n",
							"// Show the main vertical line\r\n",
							"svg\r\n",
							".append(\"line\")\r\n",
							"  .attr(\"x1\", center)\r\n",
							"  .attr(\"x2\", center)\r\n",
							"  .attr(\"y1\", y(min) )\r\n",
							"  .attr(\"y2\", y(max) )\r\n",
							"  .attr(\"stroke\", \"black\")\r\n",
							"\r\n",
							"// Show the box\r\n",
							"svg\r\n",
							".append(\"rect\")\r\n",
							"  .attr(\"x\", center - width/2)\r\n",
							"  .attr(\"y\", y(q3) )\r\n",
							"  .attr(\"height\", (y(q1)-y(q3)) )\r\n",
							"  .attr(\"width\", width )\r\n",
							"  .attr(\"stroke\", \"black\")\r\n",
							"  .style(\"fill\", \"#69b3a2\")\r\n",
							"\r\n",
							"// show median, min and max horizontal \r\n",
							"svg\r\n",
							".selectAll(\"toto\")\r\n",
							".data([min, median, max])\r\n",
							".enter()\r\n",
							".append(\"line\")\r\n",
							"  .attr(\"x1\", center-width/2)\r\n",
							"  .attr(\"x2\", center+width/2)\r\n",
							"  .attr(\"y1\", function(d){ return(y(d))} )\r\n",
							"  .attr(\"y2\", function(d){ return(y(d))} )\r\n",
							"  .attr(\"stroke\", \"black\")\r\n",
							"</script>')"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Magic command"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"%%sparkr\r\n",
							"df <- read.df('abfss://users@contosolake.dfs.core.windows.net/NYCTripSmall.parquet', format='parquet')\r\n",
							"display(limit(df, 10))"
						],
						"outputs": [],
						"execution_count": 47
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sklearn-unpickle')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "_adhoc"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "threetwo",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "73a2317a-416e-4c77-be2b-783e79515ae7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4e06275-58d1-4081-8f1b-be12462eb701/resourceGroups/wplushiramsynapse/providers/Microsoft.Synapse/workspaces/wplushiramsynapse/bigDataPools/threetwo",
						"name": "threetwo",
						"type": "Spark",
						"endpoint": "https://wplushiramsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/threetwo",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import pickle\r\n",
							"\r\n",
							"numbers_list = [1, 2, 3, 4, 5]\r\n",
							"list_pickle_path = 'list_pickle.pkl'\r\n",
							"\r\n",
							"list_pickle = open(list_pickle_path, 'wb')\r\n",
							"pickle.dump(numbers_list, list_pickle)\r\n",
							"list_pickle.close()"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"list_pickle_path = 'list_pickle.pkl'\r\n",
							"list_unpickle = open(list_pickle_path, 'rb')\r\n",
							"\r\n",
							"numbers_list = pickle.load(list_unpickle)\r\n",
							"\r\n",
							"print(numbers_list)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"loaded_model = pickle.load(open(list_pickle_path, 'rb'))\r\n",
							"loaded_model"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/wplussynapsedw')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/threetwo')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 5,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		}
	]
}